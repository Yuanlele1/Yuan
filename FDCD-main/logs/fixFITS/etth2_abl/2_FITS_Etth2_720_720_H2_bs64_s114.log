Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=72, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=72, out_features=144, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9289728.0
params:  10512.0
Trainable parameters:  10512
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.473029375076294
Epoch: 1, Steps: 56 | Train Loss: 0.8722784 Vali Loss: 0.8521692 Test Loss: 0.4955268
Validation loss decreased (inf --> 0.852169).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.720067262649536
Epoch: 2, Steps: 56 | Train Loss: 0.7279443 Vali Loss: 0.7936168 Test Loss: 0.4552792
Validation loss decreased (0.852169 --> 0.793617).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.2853798866271973
Epoch: 3, Steps: 56 | Train Loss: 0.6465033 Vali Loss: 0.7635835 Test Loss: 0.4334747
Validation loss decreased (0.793617 --> 0.763584).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.847046136856079
Epoch: 4, Steps: 56 | Train Loss: 0.5973279 Vali Loss: 0.7402472 Test Loss: 0.4210654
Validation loss decreased (0.763584 --> 0.740247).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.481564521789551
Epoch: 5, Steps: 56 | Train Loss: 0.5661072 Vali Loss: 0.7236938 Test Loss: 0.4139753
Validation loss decreased (0.740247 --> 0.723694).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.880790948867798
Epoch: 6, Steps: 56 | Train Loss: 0.5451526 Vali Loss: 0.7158443 Test Loss: 0.4094530
Validation loss decreased (0.723694 --> 0.715844).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.8395895957946777
Epoch: 7, Steps: 56 | Train Loss: 0.5300263 Vali Loss: 0.7102195 Test Loss: 0.4064264
Validation loss decreased (0.715844 --> 0.710220).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.538545846939087
Epoch: 8, Steps: 56 | Train Loss: 0.5180390 Vali Loss: 0.7098418 Test Loss: 0.4041852
Validation loss decreased (0.710220 --> 0.709842).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.9529917240142822
Epoch: 9, Steps: 56 | Train Loss: 0.5080798 Vali Loss: 0.7012019 Test Loss: 0.4024168
Validation loss decreased (0.709842 --> 0.701202).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.3316776752471924
Epoch: 10, Steps: 56 | Train Loss: 0.4998928 Vali Loss: 0.7000099 Test Loss: 0.4009861
Validation loss decreased (0.701202 --> 0.700010).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.4850780963897705
Epoch: 11, Steps: 56 | Train Loss: 0.4943637 Vali Loss: 0.6966379 Test Loss: 0.3996928
Validation loss decreased (0.700010 --> 0.696638).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.036449432373047
Epoch: 12, Steps: 56 | Train Loss: 0.4873085 Vali Loss: 0.6926316 Test Loss: 0.3986123
Validation loss decreased (0.696638 --> 0.692632).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.9288785457611084
Epoch: 13, Steps: 56 | Train Loss: 0.4828327 Vali Loss: 0.6906154 Test Loss: 0.3976574
Validation loss decreased (0.692632 --> 0.690615).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.5726938247680664
Epoch: 14, Steps: 56 | Train Loss: 0.4783915 Vali Loss: 0.6865436 Test Loss: 0.3968110
Validation loss decreased (0.690615 --> 0.686544).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.7671709060668945
Epoch: 15, Steps: 56 | Train Loss: 0.4737570 Vali Loss: 0.6856336 Test Loss: 0.3960365
Validation loss decreased (0.686544 --> 0.685634).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.749552011489868
Epoch: 16, Steps: 56 | Train Loss: 0.4705301 Vali Loss: 0.6851737 Test Loss: 0.3953336
Validation loss decreased (0.685634 --> 0.685174).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.9714412689208984
Epoch: 17, Steps: 56 | Train Loss: 0.4673005 Vali Loss: 0.6802096 Test Loss: 0.3947359
Validation loss decreased (0.685174 --> 0.680210).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.7074668407440186
Epoch: 18, Steps: 56 | Train Loss: 0.4653855 Vali Loss: 0.6781709 Test Loss: 0.3941563
Validation loss decreased (0.680210 --> 0.678171).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.461225748062134
Epoch: 19, Steps: 56 | Train Loss: 0.4621836 Vali Loss: 0.6800675 Test Loss: 0.3935956
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.23726749420166
Epoch: 20, Steps: 56 | Train Loss: 0.4598743 Vali Loss: 0.6811723 Test Loss: 0.3931396
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.2130987644195557
Epoch: 21, Steps: 56 | Train Loss: 0.4569081 Vali Loss: 0.6780235 Test Loss: 0.3926706
Validation loss decreased (0.678171 --> 0.678024).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.881859540939331
Epoch: 22, Steps: 56 | Train Loss: 0.4562028 Vali Loss: 0.6754903 Test Loss: 0.3922412
Validation loss decreased (0.678024 --> 0.675490).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.495990037918091
Epoch: 23, Steps: 56 | Train Loss: 0.4543904 Vali Loss: 0.6758227 Test Loss: 0.3919053
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.8749759197235107
Epoch: 24, Steps: 56 | Train Loss: 0.4527053 Vali Loss: 0.6735640 Test Loss: 0.3915614
Validation loss decreased (0.675490 --> 0.673564).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.508254051208496
Epoch: 25, Steps: 56 | Train Loss: 0.4515076 Vali Loss: 0.6760364 Test Loss: 0.3912459
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.924381971359253
Epoch: 26, Steps: 56 | Train Loss: 0.4496929 Vali Loss: 0.6704558 Test Loss: 0.3909224
Validation loss decreased (0.673564 --> 0.670456).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.7243239879608154
Epoch: 27, Steps: 56 | Train Loss: 0.4492792 Vali Loss: 0.6719986 Test Loss: 0.3906575
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.4042301177978516
Epoch: 28, Steps: 56 | Train Loss: 0.4473312 Vali Loss: 0.6674554 Test Loss: 0.3904074
Validation loss decreased (0.670456 --> 0.667455).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.829526662826538
Epoch: 29, Steps: 56 | Train Loss: 0.4474186 Vali Loss: 0.6717766 Test Loss: 0.3901845
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.784013509750366
Epoch: 30, Steps: 56 | Train Loss: 0.4451352 Vali Loss: 0.6726582 Test Loss: 0.3899654
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.5637829303741455
Epoch: 31, Steps: 56 | Train Loss: 0.4453390 Vali Loss: 0.6714332 Test Loss: 0.3897834
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.1593074798583984
Epoch: 32, Steps: 56 | Train Loss: 0.4442452 Vali Loss: 0.6683305 Test Loss: 0.3895716
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.773444890975952
Epoch: 33, Steps: 56 | Train Loss: 0.4438981 Vali Loss: 0.6703509 Test Loss: 0.3894198
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.4018025398254395
Epoch: 34, Steps: 56 | Train Loss: 0.4430008 Vali Loss: 0.6687711 Test Loss: 0.3892493
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.534839391708374
Epoch: 35, Steps: 56 | Train Loss: 0.4422739 Vali Loss: 0.6669192 Test Loss: 0.3890965
Validation loss decreased (0.667455 --> 0.666919).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.9492483139038086
Epoch: 36, Steps: 56 | Train Loss: 0.4413873 Vali Loss: 0.6693860 Test Loss: 0.3889635
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.89548397064209
Epoch: 37, Steps: 56 | Train Loss: 0.4411659 Vali Loss: 0.6669066 Test Loss: 0.3888379
Validation loss decreased (0.666919 --> 0.666907).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.688714027404785
Epoch: 38, Steps: 56 | Train Loss: 0.4412804 Vali Loss: 0.6687889 Test Loss: 0.3886933
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.3310394287109375
Epoch: 39, Steps: 56 | Train Loss: 0.4406317 Vali Loss: 0.6667485 Test Loss: 0.3885906
Validation loss decreased (0.666907 --> 0.666748).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.4870388507843018
Epoch: 40, Steps: 56 | Train Loss: 0.4393930 Vali Loss: 0.6638151 Test Loss: 0.3884764
Validation loss decreased (0.666748 --> 0.663815).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.6574130058288574
Epoch: 41, Steps: 56 | Train Loss: 0.4390212 Vali Loss: 0.6645577 Test Loss: 0.3883769
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.807459831237793
Epoch: 42, Steps: 56 | Train Loss: 0.4392671 Vali Loss: 0.6653216 Test Loss: 0.3882777
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.9095585346221924
Epoch: 43, Steps: 56 | Train Loss: 0.4385604 Vali Loss: 0.6655478 Test Loss: 0.3881857
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.8881654739379883
Epoch: 44, Steps: 56 | Train Loss: 0.4386583 Vali Loss: 0.6666601 Test Loss: 0.3881170
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 3.0332541465759277
Epoch: 45, Steps: 56 | Train Loss: 0.4382756 Vali Loss: 0.6650771 Test Loss: 0.3880445
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.7012979984283447
Epoch: 46, Steps: 56 | Train Loss: 0.4381149 Vali Loss: 0.6615345 Test Loss: 0.3879669
Validation loss decreased (0.663815 --> 0.661535).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 3.1571428775787354
Epoch: 47, Steps: 56 | Train Loss: 0.4374058 Vali Loss: 0.6635923 Test Loss: 0.3878987
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.993135452270508
Epoch: 48, Steps: 56 | Train Loss: 0.4371704 Vali Loss: 0.6642269 Test Loss: 0.3878374
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.653304100036621
Epoch: 49, Steps: 56 | Train Loss: 0.4366777 Vali Loss: 0.6663033 Test Loss: 0.3877852
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.937842845916748
Epoch: 50, Steps: 56 | Train Loss: 0.4369084 Vali Loss: 0.6648235 Test Loss: 0.3877403
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.418437957763672
Epoch: 51, Steps: 56 | Train Loss: 0.4365947 Vali Loss: 0.6647909 Test Loss: 0.3876848
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 3.177736282348633
Epoch: 52, Steps: 56 | Train Loss: 0.4365706 Vali Loss: 0.6630073 Test Loss: 0.3876282
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.940467596054077
Epoch: 53, Steps: 56 | Train Loss: 0.4360650 Vali Loss: 0.6648846 Test Loss: 0.3875915
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 3.0938007831573486
Epoch: 54, Steps: 56 | Train Loss: 0.4367639 Vali Loss: 0.6638827 Test Loss: 0.3875535
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.834134101867676
Epoch: 55, Steps: 56 | Train Loss: 0.4359648 Vali Loss: 0.6622297 Test Loss: 0.3875050
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.9052629470825195
Epoch: 56, Steps: 56 | Train Loss: 0.4366355 Vali Loss: 0.6663634 Test Loss: 0.3874667
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.7665977478027344
Epoch: 57, Steps: 56 | Train Loss: 0.4358463 Vali Loss: 0.6631818 Test Loss: 0.3874432
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.5528128147125244
Epoch: 58, Steps: 56 | Train Loss: 0.4347893 Vali Loss: 0.6652920 Test Loss: 0.3873910
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.6673777103424072
Epoch: 59, Steps: 56 | Train Loss: 0.4348893 Vali Loss: 0.6631846 Test Loss: 0.3873670
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.7862188816070557
Epoch: 60, Steps: 56 | Train Loss: 0.4354225 Vali Loss: 0.6645665 Test Loss: 0.3873336
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 3.627631425857544
Epoch: 61, Steps: 56 | Train Loss: 0.4344541 Vali Loss: 0.6626762 Test Loss: 0.3873072
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.5089545249938965
Epoch: 62, Steps: 56 | Train Loss: 0.4349830 Vali Loss: 0.6634768 Test Loss: 0.3872918
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 3.1096949577331543
Epoch: 63, Steps: 56 | Train Loss: 0.4344898 Vali Loss: 0.6656476 Test Loss: 0.3872598
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.922762632369995
Epoch: 64, Steps: 56 | Train Loss: 0.4351472 Vali Loss: 0.6633037 Test Loss: 0.3872335
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.795032262802124
Epoch: 65, Steps: 56 | Train Loss: 0.4350086 Vali Loss: 0.6627287 Test Loss: 0.3872161
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.772526264190674
Epoch: 66, Steps: 56 | Train Loss: 0.4347567 Vali Loss: 0.6629263 Test Loss: 0.3871931
EarlyStopping counter: 20 out of 20
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=72, out_features=144, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9289728.0
params:  10512.0
Trainable parameters:  10512
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.9995298385620117
Epoch: 1, Steps: 56 | Train Loss: 0.8165149 Vali Loss: 0.6544449 Test Loss: 0.3852853
Validation loss decreased (inf --> 0.654445).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.4681780338287354
Epoch: 2, Steps: 56 | Train Loss: 0.8115316 Vali Loss: 0.6551449 Test Loss: 0.3838204
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.976228952407837
Epoch: 3, Steps: 56 | Train Loss: 0.8083612 Vali Loss: 0.6553940 Test Loss: 0.3829827
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.7888312339782715
Epoch: 4, Steps: 56 | Train Loss: 0.8062486 Vali Loss: 0.6510216 Test Loss: 0.3825214
Validation loss decreased (0.654445 --> 0.651022).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.9878218173980713
Epoch: 5, Steps: 56 | Train Loss: 0.8065861 Vali Loss: 0.6510758 Test Loss: 0.3821853
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.8359673023223877
Epoch: 6, Steps: 56 | Train Loss: 0.8058423 Vali Loss: 0.6433364 Test Loss: 0.3820494
Validation loss decreased (0.651022 --> 0.643336).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.282397747039795
Epoch: 7, Steps: 56 | Train Loss: 0.8043714 Vali Loss: 0.6456323 Test Loss: 0.3819408
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.7389791011810303
Epoch: 8, Steps: 56 | Train Loss: 0.8057484 Vali Loss: 0.6461120 Test Loss: 0.3818689
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.005230665206909
Epoch: 9, Steps: 56 | Train Loss: 0.8023877 Vali Loss: 0.6481700 Test Loss: 0.3816890
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.6741857528686523
Epoch: 10, Steps: 56 | Train Loss: 0.8034420 Vali Loss: 0.6443884 Test Loss: 0.3817371
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.6161091327667236
Epoch: 11, Steps: 56 | Train Loss: 0.8039312 Vali Loss: 0.6509297 Test Loss: 0.3816483
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.64467716217041
Epoch: 12, Steps: 56 | Train Loss: 0.8030705 Vali Loss: 0.6483622 Test Loss: 0.3816395
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.010706901550293
Epoch: 13, Steps: 56 | Train Loss: 0.8039920 Vali Loss: 0.6451232 Test Loss: 0.3817490
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.968442916870117
Epoch: 14, Steps: 56 | Train Loss: 0.8037731 Vali Loss: 0.6439981 Test Loss: 0.3816809
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.0393340587615967
Epoch: 15, Steps: 56 | Train Loss: 0.8023652 Vali Loss: 0.6431199 Test Loss: 0.3816389
Validation loss decreased (0.643336 --> 0.643120).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.788193941116333
Epoch: 16, Steps: 56 | Train Loss: 0.8032645 Vali Loss: 0.6479111 Test Loss: 0.3816808
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.973926067352295
Epoch: 17, Steps: 56 | Train Loss: 0.8029951 Vali Loss: 0.6450286 Test Loss: 0.3817402
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.523432493209839
Epoch: 18, Steps: 56 | Train Loss: 0.8028069 Vali Loss: 0.6433606 Test Loss: 0.3817236
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.5368497371673584
Epoch: 19, Steps: 56 | Train Loss: 0.8015507 Vali Loss: 0.6419145 Test Loss: 0.3817234
Validation loss decreased (0.643120 --> 0.641915).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.779489517211914
Epoch: 20, Steps: 56 | Train Loss: 0.8021798 Vali Loss: 0.6425810 Test Loss: 0.3817320
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.822073459625244
Epoch: 21, Steps: 56 | Train Loss: 0.8012756 Vali Loss: 0.6431305 Test Loss: 0.3817217
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.195321798324585
Epoch: 22, Steps: 56 | Train Loss: 0.8025713 Vali Loss: 0.6467999 Test Loss: 0.3817482
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.255563259124756
Epoch: 23, Steps: 56 | Train Loss: 0.8021280 Vali Loss: 0.6436694 Test Loss: 0.3817413
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.7676289081573486
Epoch: 24, Steps: 56 | Train Loss: 0.8022818 Vali Loss: 0.6461146 Test Loss: 0.3817396
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.709141254425049
Epoch: 25, Steps: 56 | Train Loss: 0.8022002 Vali Loss: 0.6441268 Test Loss: 0.3817542
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.68890643119812
Epoch: 26, Steps: 56 | Train Loss: 0.8030878 Vali Loss: 0.6442295 Test Loss: 0.3817446
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.5210156440734863
Epoch: 27, Steps: 56 | Train Loss: 0.8025010 Vali Loss: 0.6459591 Test Loss: 0.3817600
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.724257230758667
Epoch: 28, Steps: 56 | Train Loss: 0.7998145 Vali Loss: 0.6444317 Test Loss: 0.3817475
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.8223154544830322
Epoch: 29, Steps: 56 | Train Loss: 0.8017137 Vali Loss: 0.6459833 Test Loss: 0.3817774
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.6401829719543457
Epoch: 30, Steps: 56 | Train Loss: 0.8019602 Vali Loss: 0.6439858 Test Loss: 0.3817301
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.025923013687134
Epoch: 31, Steps: 56 | Train Loss: 0.8025287 Vali Loss: 0.6434790 Test Loss: 0.3817762
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.6183505058288574
Epoch: 32, Steps: 56 | Train Loss: 0.8027609 Vali Loss: 0.6422907 Test Loss: 0.3817677
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.922255754470825
Epoch: 33, Steps: 56 | Train Loss: 0.8020501 Vali Loss: 0.6442610 Test Loss: 0.3817489
EarlyStopping counter: 14 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.2590274810791016
Epoch: 34, Steps: 56 | Train Loss: 0.8021049 Vali Loss: 0.6394276 Test Loss: 0.3817788
Validation loss decreased (0.641915 --> 0.639428).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.674224853515625
Epoch: 35, Steps: 56 | Train Loss: 0.8003493 Vali Loss: 0.6464589 Test Loss: 0.3817673
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.6225879192352295
Epoch: 36, Steps: 56 | Train Loss: 0.8030466 Vali Loss: 0.6435924 Test Loss: 0.3817807
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.8247666358947754
Epoch: 37, Steps: 56 | Train Loss: 0.8028204 Vali Loss: 0.6423559 Test Loss: 0.3817492
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.9644217491149902
Epoch: 38, Steps: 56 | Train Loss: 0.8013461 Vali Loss: 0.6406337 Test Loss: 0.3817896
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.842078924179077
Epoch: 39, Steps: 56 | Train Loss: 0.8033100 Vali Loss: 0.6446317 Test Loss: 0.3817743
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.257378578186035
Epoch: 40, Steps: 56 | Train Loss: 0.8019310 Vali Loss: 0.6467506 Test Loss: 0.3817925
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.9830117225646973
Epoch: 41, Steps: 56 | Train Loss: 0.8024034 Vali Loss: 0.6446076 Test Loss: 0.3817871
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.8756768703460693
Epoch: 42, Steps: 56 | Train Loss: 0.8004113 Vali Loss: 0.6443096 Test Loss: 0.3817912
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.2233657836914062
Epoch: 43, Steps: 56 | Train Loss: 0.8005293 Vali Loss: 0.6450185 Test Loss: 0.3817574
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.677049160003662
Epoch: 44, Steps: 56 | Train Loss: 0.8006610 Vali Loss: 0.6429160 Test Loss: 0.3817714
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.7110612392425537
Epoch: 45, Steps: 56 | Train Loss: 0.8003491 Vali Loss: 0.6423104 Test Loss: 0.3817750
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.990450143814087
Epoch: 46, Steps: 56 | Train Loss: 0.8017768 Vali Loss: 0.6414590 Test Loss: 0.3817932
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.9061338901519775
Epoch: 47, Steps: 56 | Train Loss: 0.8005608 Vali Loss: 0.6421328 Test Loss: 0.3817801
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.9601056575775146
Epoch: 48, Steps: 56 | Train Loss: 0.8019036 Vali Loss: 0.6402026 Test Loss: 0.3817888
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.593696355819702
Epoch: 49, Steps: 56 | Train Loss: 0.8020159 Vali Loss: 0.6486778 Test Loss: 0.3817892
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.698209762573242
Epoch: 50, Steps: 56 | Train Loss: 0.8032563 Vali Loss: 0.6398510 Test Loss: 0.3818077
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 3.0405185222625732
Epoch: 51, Steps: 56 | Train Loss: 0.8020821 Vali Loss: 0.6420938 Test Loss: 0.3817873
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 3.030574321746826
Epoch: 52, Steps: 56 | Train Loss: 0.7996747 Vali Loss: 0.6424478 Test Loss: 0.3818119
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.94348406791687
Epoch: 53, Steps: 56 | Train Loss: 0.8024587 Vali Loss: 0.6441798 Test Loss: 0.3817871
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 3.0724129676818848
Epoch: 54, Steps: 56 | Train Loss: 0.8031396 Vali Loss: 0.6444951 Test Loss: 0.3818001
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.3803667426109314, mae:0.42387697100639343, rse:0.49295467138290405, corr:[ 2.17802867e-01  2.18459770e-01  2.19261602e-01  2.19235048e-01
  2.18229115e-01  2.16846347e-01  2.15544701e-01  2.14334100e-01
  2.13314876e-01  2.12255105e-01  2.11295754e-01  2.10308820e-01
  2.09363252e-01  2.08453730e-01  2.07526132e-01  2.06677511e-01
  2.05818176e-01  2.04951078e-01  2.04070911e-01  2.03172162e-01
  2.02203155e-01  2.01085716e-01  1.99891523e-01  1.98522374e-01
  1.97198421e-01  1.96011633e-01  1.95055991e-01  1.94169477e-01
  1.93327919e-01  1.92406341e-01  1.91484958e-01  1.90547392e-01
  1.89584479e-01  1.88681498e-01  1.87877461e-01  1.87109306e-01
  1.86341450e-01  1.85583189e-01  1.84976742e-01  1.84410200e-01
  1.83896989e-01  1.83355466e-01  1.82711795e-01  1.81969851e-01
  1.81111068e-01  1.80101588e-01  1.79045275e-01  1.77691013e-01
  1.76295862e-01  1.75040439e-01  1.74040005e-01  1.73128933e-01
  1.72280669e-01  1.71475500e-01  1.70519903e-01  1.69579729e-01
  1.68815032e-01  1.68227956e-01  1.67939737e-01  1.67925701e-01
  1.68135107e-01  1.68232247e-01  1.68228239e-01  1.68240637e-01
  1.68076918e-01  1.67949006e-01  1.67912498e-01  1.67921022e-01
  1.67877287e-01  1.67750582e-01  1.67564541e-01  1.67275444e-01
  1.66982502e-01  1.66615427e-01  1.66271448e-01  1.65778637e-01
  1.65322229e-01  1.65022373e-01  1.64820224e-01  1.64691105e-01
  1.64756358e-01  1.65016234e-01  1.65272474e-01  1.65454388e-01
  1.65520743e-01  1.65464073e-01  1.65276811e-01  1.65013030e-01
  1.64785877e-01  1.64755493e-01  1.64949641e-01  1.65169433e-01
  1.65553674e-01  1.65824816e-01  1.65946677e-01  1.65804341e-01
  1.65411636e-01  1.64921314e-01  1.64398864e-01  1.63986757e-01
  1.63882628e-01  1.63846552e-01  1.64005309e-01  1.64162993e-01
  1.64342731e-01  1.64330646e-01  1.64195091e-01  1.64035946e-01
  1.63745269e-01  1.63392514e-01  1.63140699e-01  1.63108155e-01
  1.63167551e-01  1.63266346e-01  1.63274109e-01  1.63124591e-01
  1.62786216e-01  1.62095964e-01  1.61267340e-01  1.60311893e-01
  1.59350902e-01  1.58446983e-01  1.57675698e-01  1.57120332e-01
  1.56566396e-01  1.56042635e-01  1.55486673e-01  1.54959247e-01
  1.54530853e-01  1.54009312e-01  1.53615773e-01  1.53130814e-01
  1.52667671e-01  1.52107105e-01  1.51533559e-01  1.51087776e-01
  1.50623471e-01  1.50181383e-01  1.49804011e-01  1.49421453e-01
  1.49092197e-01  1.48588508e-01  1.47871450e-01  1.46963298e-01
  1.45819187e-01  1.44843116e-01  1.44160137e-01  1.43709734e-01
  1.43248796e-01  1.42704114e-01  1.42164215e-01  1.41613945e-01
  1.41183823e-01  1.40771568e-01  1.40380725e-01  1.39832929e-01
  1.39271870e-01  1.38792574e-01  1.38471261e-01  1.38314351e-01
  1.38174906e-01  1.38185829e-01  1.38338417e-01  1.38516426e-01
  1.38722971e-01  1.38731942e-01  1.38551295e-01  1.38144672e-01
  1.37627780e-01  1.37058482e-01  1.36561632e-01  1.36087999e-01
  1.35636181e-01  1.35067523e-01  1.34319782e-01  1.33526012e-01
  1.32855445e-01  1.32336348e-01  1.31842330e-01  1.31374821e-01
  1.30989298e-01  1.30708069e-01  1.30626604e-01  1.30654648e-01
  1.30673856e-01  1.30824432e-01  1.31087974e-01  1.31369546e-01
  1.31694719e-01  1.32052332e-01  1.32507041e-01  1.32901177e-01
  1.33143604e-01  1.33310303e-01  1.33395642e-01  1.33472115e-01
  1.33566946e-01  1.33485734e-01  1.33425876e-01  1.33257151e-01
  1.33012488e-01  1.32664815e-01  1.32459849e-01  1.32183850e-01
  1.31965548e-01  1.31985456e-01  1.32155761e-01  1.32446527e-01
  1.32809699e-01  1.33247808e-01  1.33579835e-01  1.33825764e-01
  1.34100094e-01  1.34330228e-01  1.34485036e-01  1.34378538e-01
  1.34057522e-01  1.33583263e-01  1.33042067e-01  1.32743999e-01
  1.32528841e-01  1.32645369e-01  1.32779822e-01  1.33113697e-01
  1.33436844e-01  1.33632511e-01  1.33772507e-01  1.33770972e-01
  1.33840531e-01  1.33952245e-01  1.34233445e-01  1.34733871e-01
  1.35356352e-01  1.36037126e-01  1.36751309e-01  1.37443244e-01
  1.38154119e-01  1.38755322e-01  1.39263719e-01  1.39718950e-01
  1.40103176e-01  1.40280068e-01  1.40347704e-01  1.40512854e-01
  1.40753165e-01  1.40991211e-01  1.41369492e-01  1.41737148e-01
  1.42233506e-01  1.42684013e-01  1.43247783e-01  1.43847600e-01
  1.44499332e-01  1.45291463e-01  1.46069109e-01  1.46947846e-01
  1.47702694e-01  1.48386165e-01  1.48932233e-01  1.49517372e-01
  1.50228977e-01  1.50926217e-01  1.51731625e-01  1.52417555e-01
  1.52978033e-01  1.53333217e-01  1.53576717e-01  1.53899938e-01
  1.54091835e-01  1.54307306e-01  1.54544696e-01  1.54786512e-01
  1.55087814e-01  1.55351952e-01  1.55735672e-01  1.56191587e-01
  1.56605020e-01  1.57204345e-01  1.57772973e-01  1.58279791e-01
  1.58772811e-01  1.59260094e-01  1.59693271e-01  1.60142869e-01
  1.60564244e-01  1.60917237e-01  1.61348984e-01  1.61669731e-01
  1.61838263e-01  1.61788836e-01  1.61548749e-01  1.61457509e-01
  1.61405236e-01  1.61355153e-01  1.61313355e-01  1.61428005e-01
  1.61619768e-01  1.61783621e-01  1.61958352e-01  1.62180826e-01
  1.62239432e-01  1.62455082e-01  1.62838802e-01  1.63291723e-01
  1.63561895e-01  1.63826331e-01  1.63939148e-01  1.64133146e-01
  1.64377078e-01  1.64665163e-01  1.64928496e-01  1.65064976e-01
  1.65154889e-01  1.65052742e-01  1.64971828e-01  1.64751887e-01
  1.64462909e-01  1.64172098e-01  1.63924307e-01  1.63775936e-01
  1.63610533e-01  1.63551539e-01  1.63624302e-01  1.63861543e-01
  1.64105430e-01  1.64234877e-01  1.64557308e-01  1.64844140e-01
  1.65095434e-01  1.65303856e-01  1.65485933e-01  1.65890068e-01
  1.66538373e-01  1.67247623e-01  1.68053627e-01  1.68765977e-01
  1.69112176e-01  1.69243425e-01  1.69393331e-01  1.69461057e-01
  1.69563711e-01  1.69567913e-01  1.69687375e-01  1.69799209e-01
  1.69830292e-01  1.69887647e-01  1.69857532e-01  1.70068875e-01
  1.70433760e-01  1.70932785e-01  1.71563759e-01  1.72344476e-01
  1.73110485e-01  1.73877388e-01  1.74528152e-01  1.75206766e-01
  1.75844431e-01  1.76414222e-01  1.76923037e-01  1.77233711e-01
  1.77374288e-01  1.77414358e-01  1.77582607e-01  1.77825615e-01
  1.78320348e-01  1.78910404e-01  1.79486439e-01  1.79981366e-01
  1.80211619e-01  1.80435345e-01  1.80539623e-01  1.80625185e-01
  1.80584967e-01  1.80543020e-01  1.80557191e-01  1.80597365e-01
  1.80694878e-01  1.80816606e-01  1.80845737e-01  1.80924729e-01
  1.81077152e-01  1.81277230e-01  1.81354240e-01  1.81406260e-01
  1.81436896e-01  1.81417421e-01  1.81503505e-01  1.81743100e-01
  1.82151809e-01  1.82612255e-01  1.83031172e-01  1.83397233e-01
  1.83669060e-01  1.83803454e-01  1.83893621e-01  1.83853582e-01
  1.83830619e-01  1.83642894e-01  1.83477297e-01  1.83428600e-01
  1.83381081e-01  1.83277071e-01  1.83033079e-01  1.82746083e-01
  1.82477668e-01  1.82199910e-01  1.81921750e-01  1.81745321e-01
  1.81599811e-01  1.81489125e-01  1.81412905e-01  1.81279898e-01
  1.81154996e-01  1.81042552e-01  1.80931196e-01  1.80926427e-01
  1.80960789e-01  1.80986688e-01  1.80812195e-01  1.80506647e-01
  1.79937720e-01  1.79210335e-01  1.78341001e-01  1.77373141e-01
  1.76325142e-01  1.75168738e-01  1.74159482e-01  1.73328206e-01
  1.72675058e-01  1.72201872e-01  1.71698034e-01  1.71243697e-01
  1.70725688e-01  1.70114547e-01  1.69607490e-01  1.69120863e-01
  1.68674886e-01  1.68265164e-01  1.67894736e-01  1.67555764e-01
  1.67047307e-01  1.66551039e-01  1.65994674e-01  1.65491670e-01
  1.64890483e-01  1.64454401e-01  1.64035693e-01  1.63754627e-01
  1.63474977e-01  1.63174346e-01  1.62805825e-01  1.62595913e-01
  1.62331328e-01  1.62060112e-01  1.61801264e-01  1.61562294e-01
  1.61402851e-01  1.61237195e-01  1.61003143e-01  1.60868168e-01
  1.60741121e-01  1.60454810e-01  1.60029143e-01  1.59642041e-01
  1.59101307e-01  1.58630103e-01  1.58265978e-01  1.57837927e-01
  1.57480925e-01  1.57064036e-01  1.56754389e-01  1.56401739e-01
  1.55966923e-01  1.55427381e-01  1.54978991e-01  1.54649779e-01
  1.54383391e-01  1.54227585e-01  1.54055566e-01  1.53805181e-01
  1.53334677e-01  1.52664870e-01  1.52004823e-01  1.51088014e-01
  1.50065556e-01  1.49087891e-01  1.48300141e-01  1.47580311e-01
  1.47024274e-01  1.46538347e-01  1.46163702e-01  1.45615354e-01
  1.44840091e-01  1.43917099e-01  1.43121243e-01  1.42533571e-01
  1.41958058e-01  1.41518146e-01  1.41199484e-01  1.40938818e-01
  1.40611246e-01  1.40048355e-01  1.39359266e-01  1.38364896e-01
  1.37095451e-01  1.35919183e-01  1.35107547e-01  1.34596393e-01
  1.34283006e-01  1.34168088e-01  1.33930430e-01  1.33567333e-01
  1.33004174e-01  1.32207155e-01  1.31311536e-01  1.30220100e-01
  1.29174948e-01  1.28269121e-01  1.27675593e-01  1.27238169e-01
  1.26772016e-01  1.26336366e-01  1.25790864e-01  1.25121072e-01
  1.24303430e-01  1.23493887e-01  1.22610249e-01  1.21569961e-01
  1.20256364e-01  1.18884139e-01  1.17717817e-01  1.16484381e-01
  1.15297057e-01  1.14170924e-01  1.13156736e-01  1.12312876e-01
  1.11519590e-01  1.10747695e-01  1.10004269e-01  1.09149486e-01
  1.08141579e-01  1.06970079e-01  1.05806679e-01  1.04728326e-01
  1.03686228e-01  1.02864116e-01  1.02053344e-01  1.01321161e-01
  1.00714475e-01  9.99707207e-02  9.92207304e-02  9.82525423e-02
  9.72086862e-02  9.60983410e-02  9.49778929e-02  9.38155651e-02
  9.26479101e-02  9.15720537e-02  9.06716809e-02  8.97811875e-02
  8.92129391e-02  8.87251571e-02  8.84015635e-02  8.78943354e-02
  8.72471407e-02  8.65212902e-02  8.58760998e-02  8.52207616e-02
  8.47964659e-02  8.44627470e-02  8.39729458e-02  8.32323655e-02
  8.21620524e-02  8.09814259e-02  7.97006860e-02  7.84127563e-02
  7.70135224e-02  7.57741854e-02  7.46707022e-02  7.36303926e-02
  7.26759657e-02  7.17430040e-02  7.08319768e-02  6.99173361e-02
  6.90321028e-02  6.82160854e-02  6.73897415e-02  6.67029917e-02
  6.60159886e-02  6.53709471e-02  6.48710206e-02  6.43788502e-02
  6.38782755e-02  6.33366928e-02  6.26305118e-02  6.18834011e-02
  6.11203425e-02  6.04302287e-02  5.96516803e-02  5.87870553e-02
  5.78649566e-02  5.69949560e-02  5.58989942e-02  5.47440611e-02
  5.33490740e-02  5.19622788e-02  5.05921990e-02  4.93452288e-02
  4.81505916e-02  4.72235866e-02  4.67226058e-02  4.62147519e-02
  4.56275977e-02  4.49604839e-02  4.44029234e-02  4.39792015e-02
  4.36435752e-02  4.36309315e-02  4.37443256e-02  4.39735502e-02
  4.40571383e-02  4.40240577e-02  4.35834788e-02  4.30776253e-02
  4.23381925e-02  4.13849279e-02  4.02872041e-02  3.91599238e-02
  3.81400213e-02  3.74473855e-02  3.70225720e-02  3.67187858e-02
  3.65299247e-02  3.64666358e-02  3.62943299e-02  3.59062366e-02
  3.51406746e-02  3.41447406e-02  3.32898647e-02  3.29334773e-02
  3.26545909e-02  3.25539261e-02  3.24966535e-02  3.22407298e-02
  3.19302753e-02  3.16766240e-02  3.12400479e-02  3.05570718e-02
  2.96882596e-02  2.89115645e-02  2.83222031e-02  2.76527014e-02
  2.70151515e-02  2.63270177e-02  2.56361887e-02  2.51355134e-02
  2.48589702e-02  2.47150455e-02  2.47005522e-02  2.46730614e-02
  2.44129598e-02  2.40776222e-02  2.38977075e-02  2.37069372e-02
  2.35423446e-02  2.33912114e-02  2.34218035e-02  2.37149764e-02
  2.40374338e-02  2.43485831e-02  2.44290326e-02  2.41826884e-02
  2.35421900e-02  2.27187946e-02  2.19665691e-02  2.10157763e-02
  2.02536713e-02  1.95885431e-02  1.93618946e-02  1.90523602e-02
  1.88026540e-02  1.84954144e-02  1.79290418e-02  1.70226935e-02
  1.62068196e-02  1.54615017e-02  1.52704697e-02  1.55125884e-02
  1.59814637e-02  1.65016148e-02  1.68384332e-02  1.64315589e-02
  1.54533042e-02  1.40532469e-02  1.26063842e-02  1.11561771e-02
  9.85007640e-03  8.73477478e-03  8.03675223e-03  7.35886861e-03
  6.69678021e-03  5.66346571e-03  4.46230173e-03  2.83672870e-03
  1.19055063e-03 -1.17078504e-04 -1.07455661e-03 -1.65864942e-03
 -2.09335587e-03 -2.65871570e-03 -2.95767351e-03 -3.01961089e-03
 -2.69944337e-03 -1.79517188e-03 -6.67070679e-04  3.60772276e-04
  9.47862340e-04  8.98410217e-04 -5.65455295e-04 -3.73986387e-03]
