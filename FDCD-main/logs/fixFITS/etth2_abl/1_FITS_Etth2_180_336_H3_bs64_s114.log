Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_180_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_180_336_FITS_ETTh2_ftM_sl180_ll48_pl336_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8125
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=34, out_features=97, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2955008.0
params:  3395.0
Trainable parameters:  3395
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.8122944831848145
Epoch: 1, Steps: 63 | Train Loss: 0.8403279 Vali Loss: 0.4597252 Test Loss: 0.4859516
Validation loss decreased (inf --> 0.459725).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.627770185470581
Epoch: 2, Steps: 63 | Train Loss: 0.7296875 Vali Loss: 0.4182937 Test Loss: 0.4422765
Validation loss decreased (0.459725 --> 0.418294).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.4388065338134766
Epoch: 3, Steps: 63 | Train Loss: 0.6855931 Vali Loss: 0.4012239 Test Loss: 0.4226910
Validation loss decreased (0.418294 --> 0.401224).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.9178223609924316
Epoch: 4, Steps: 63 | Train Loss: 0.6635632 Vali Loss: 0.3918536 Test Loss: 0.4131136
Validation loss decreased (0.401224 --> 0.391854).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.490360736846924
Epoch: 5, Steps: 63 | Train Loss: 0.6500724 Vali Loss: 0.3873236 Test Loss: 0.4078240
Validation loss decreased (0.391854 --> 0.387324).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.0404300689697266
Epoch: 6, Steps: 63 | Train Loss: 0.6461600 Vali Loss: 0.3802448 Test Loss: 0.4045939
Validation loss decreased (0.387324 --> 0.380245).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.764270544052124
Epoch: 7, Steps: 63 | Train Loss: 0.6411442 Vali Loss: 0.3796856 Test Loss: 0.4027893
Validation loss decreased (0.380245 --> 0.379686).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.7002980709075928
Epoch: 8, Steps: 63 | Train Loss: 0.6372648 Vali Loss: 0.3780158 Test Loss: 0.4013632
Validation loss decreased (0.379686 --> 0.378016).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.026498556137085
Epoch: 9, Steps: 63 | Train Loss: 0.6366846 Vali Loss: 0.3761395 Test Loss: 0.4003050
Validation loss decreased (0.378016 --> 0.376139).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.927968978881836
Epoch: 10, Steps: 63 | Train Loss: 0.6356389 Vali Loss: 0.3767218 Test Loss: 0.3996858
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.194626569747925
Epoch: 11, Steps: 63 | Train Loss: 0.6336868 Vali Loss: 0.3720728 Test Loss: 0.3989450
Validation loss decreased (0.376139 --> 0.372073).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.237555503845215
Epoch: 12, Steps: 63 | Train Loss: 0.6324989 Vali Loss: 0.3743292 Test Loss: 0.3984608
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.8204920291900635
Epoch: 13, Steps: 63 | Train Loss: 0.6301529 Vali Loss: 0.3743714 Test Loss: 0.3979768
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.117656946182251
Epoch: 14, Steps: 63 | Train Loss: 0.6294621 Vali Loss: 0.3718341 Test Loss: 0.3975605
Validation loss decreased (0.372073 --> 0.371834).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.8286364078521729
Epoch: 15, Steps: 63 | Train Loss: 0.6302120 Vali Loss: 0.3706802 Test Loss: 0.3972847
Validation loss decreased (0.371834 --> 0.370680).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.5456595420837402
Epoch: 16, Steps: 63 | Train Loss: 0.6262405 Vali Loss: 0.3737052 Test Loss: 0.3970514
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 5.0512144565582275
Epoch: 17, Steps: 63 | Train Loss: 0.6268386 Vali Loss: 0.3704690 Test Loss: 0.3967732
Validation loss decreased (0.370680 --> 0.370469).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.026110887527466
Epoch: 18, Steps: 63 | Train Loss: 0.6259835 Vali Loss: 0.3682891 Test Loss: 0.3965662
Validation loss decreased (0.370469 --> 0.368289).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.474743366241455
Epoch: 19, Steps: 63 | Train Loss: 0.6282428 Vali Loss: 0.3722578 Test Loss: 0.3963436
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.03857684135437
Epoch: 20, Steps: 63 | Train Loss: 0.6267602 Vali Loss: 0.3711996 Test Loss: 0.3961149
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.07584285736084
Epoch: 21, Steps: 63 | Train Loss: 0.6262809 Vali Loss: 0.3700215 Test Loss: 0.3960089
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.8335778713226318
Epoch: 22, Steps: 63 | Train Loss: 0.6263548 Vali Loss: 0.3682598 Test Loss: 0.3958253
Validation loss decreased (0.368289 --> 0.368260).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.944702386856079
Epoch: 23, Steps: 63 | Train Loss: 0.6261656 Vali Loss: 0.3696494 Test Loss: 0.3956417
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.179230213165283
Epoch: 24, Steps: 63 | Train Loss: 0.6249354 Vali Loss: 0.3687721 Test Loss: 0.3955421
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.8604154586791992
Epoch: 25, Steps: 63 | Train Loss: 0.6243641 Vali Loss: 0.3664268 Test Loss: 0.3954040
Validation loss decreased (0.368260 --> 0.366427).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.6618125438690186
Epoch: 26, Steps: 63 | Train Loss: 0.6253300 Vali Loss: 0.3684569 Test Loss: 0.3953477
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.8917481899261475
Epoch: 27, Steps: 63 | Train Loss: 0.6245829 Vali Loss: 0.3675920 Test Loss: 0.3952566
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.066873550415039
Epoch: 28, Steps: 63 | Train Loss: 0.6233674 Vali Loss: 0.3704066 Test Loss: 0.3951587
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.0554540157318115
Epoch: 29, Steps: 63 | Train Loss: 0.6254113 Vali Loss: 0.3691179 Test Loss: 0.3950289
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.9744129180908203
Epoch: 30, Steps: 63 | Train Loss: 0.6235147 Vali Loss: 0.3675438 Test Loss: 0.3949576
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.528303384780884
Epoch: 31, Steps: 63 | Train Loss: 0.6225770 Vali Loss: 0.3692145 Test Loss: 0.3949153
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.9118077754974365
Epoch: 32, Steps: 63 | Train Loss: 0.6230609 Vali Loss: 0.3700366 Test Loss: 0.3948286
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.9815289974212646
Epoch: 33, Steps: 63 | Train Loss: 0.6218921 Vali Loss: 0.3672074 Test Loss: 0.3948033
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.9097769260406494
Epoch: 34, Steps: 63 | Train Loss: 0.6236479 Vali Loss: 0.3691214 Test Loss: 0.3947092
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.3178203105926514
Epoch: 35, Steps: 63 | Train Loss: 0.6228352 Vali Loss: 0.3654168 Test Loss: 0.3946953
Validation loss decreased (0.366427 --> 0.365417).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.2640678882598877
Epoch: 36, Steps: 63 | Train Loss: 0.6219612 Vali Loss: 0.3685574 Test Loss: 0.3946219
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.299468755722046
Epoch: 37, Steps: 63 | Train Loss: 0.6217872 Vali Loss: 0.3658215 Test Loss: 0.3945660
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.7646596431732178
Epoch: 38, Steps: 63 | Train Loss: 0.6222524 Vali Loss: 0.3681748 Test Loss: 0.3945344
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.170926570892334
Epoch: 39, Steps: 63 | Train Loss: 0.6220148 Vali Loss: 0.3679260 Test Loss: 0.3944944
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 8.314313411712646
Epoch: 40, Steps: 63 | Train Loss: 0.6211571 Vali Loss: 0.3670762 Test Loss: 0.3944707
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.9875140190124512
Epoch: 41, Steps: 63 | Train Loss: 0.6213489 Vali Loss: 0.3646078 Test Loss: 0.3944608
Validation loss decreased (0.365417 --> 0.364608).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.3086655139923096
Epoch: 42, Steps: 63 | Train Loss: 0.6216515 Vali Loss: 0.3634419 Test Loss: 0.3943999
Validation loss decreased (0.364608 --> 0.363442).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.0579757690429688
Epoch: 43, Steps: 63 | Train Loss: 0.6206355 Vali Loss: 0.3668945 Test Loss: 0.3943858
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.308161497116089
Epoch: 44, Steps: 63 | Train Loss: 0.6230952 Vali Loss: 0.3648942 Test Loss: 0.3943117
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.0296404361724854
Epoch: 45, Steps: 63 | Train Loss: 0.6232826 Vali Loss: 0.3660497 Test Loss: 0.3943325
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.822862148284912
Epoch: 46, Steps: 63 | Train Loss: 0.6224462 Vali Loss: 0.3668134 Test Loss: 0.3943008
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.3926475048065186
Epoch: 47, Steps: 63 | Train Loss: 0.6218026 Vali Loss: 0.3644154 Test Loss: 0.3942805
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.9738788604736328
Epoch: 48, Steps: 63 | Train Loss: 0.6230886 Vali Loss: 0.3665456 Test Loss: 0.3942532
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.1700339317321777
Epoch: 49, Steps: 63 | Train Loss: 0.6198859 Vali Loss: 0.3660037 Test Loss: 0.3942477
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.3617007732391357
Epoch: 50, Steps: 63 | Train Loss: 0.6214518 Vali Loss: 0.3681734 Test Loss: 0.3942328
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.136441946029663
Epoch: 51, Steps: 63 | Train Loss: 0.6217454 Vali Loss: 0.3661349 Test Loss: 0.3941915
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.9150676727294922
Epoch: 52, Steps: 63 | Train Loss: 0.6207204 Vali Loss: 0.3658517 Test Loss: 0.3941762
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.8061587810516357
Epoch: 53, Steps: 63 | Train Loss: 0.6229330 Vali Loss: 0.3692809 Test Loss: 0.3941558
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.082378625869751
Epoch: 54, Steps: 63 | Train Loss: 0.6207948 Vali Loss: 0.3676448 Test Loss: 0.3941588
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.8428332805633545
Epoch: 55, Steps: 63 | Train Loss: 0.6216195 Vali Loss: 0.3657639 Test Loss: 0.3941318
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.043356418609619
Epoch: 56, Steps: 63 | Train Loss: 0.6205947 Vali Loss: 0.3676512 Test Loss: 0.3941316
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.8876683712005615
Epoch: 57, Steps: 63 | Train Loss: 0.6223682 Vali Loss: 0.3678833 Test Loss: 0.3941167
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.0162675380706787
Epoch: 58, Steps: 63 | Train Loss: 0.6226925 Vali Loss: 0.3649990 Test Loss: 0.3941135
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.5281245708465576
Epoch: 59, Steps: 63 | Train Loss: 0.6218329 Vali Loss: 0.3686666 Test Loss: 0.3941091
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.9817605018615723
Epoch: 60, Steps: 63 | Train Loss: 0.6215929 Vali Loss: 0.3673790 Test Loss: 0.3940850
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.7105929851531982
Epoch: 61, Steps: 63 | Train Loss: 0.6209587 Vali Loss: 0.3661782 Test Loss: 0.3940639
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.2872660160064697
Epoch: 62, Steps: 63 | Train Loss: 0.6211032 Vali Loss: 0.3682553 Test Loss: 0.3940588
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_180_336_FITS_ETTh2_ftM_sl180_ll48_pl336_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.389739990234375, mae:0.41357481479644775, rse:0.49914535880088806, corr:[0.2617244  0.2666149  0.26512074 0.26235178 0.26158616 0.26156458
 0.26067206 0.2588916  0.25737196 0.25630867 0.25526533 0.2537356
 0.252345   0.25120237 0.25049984 0.24985716 0.24897875 0.24789996
 0.2469466  0.24616763 0.24531266 0.24416608 0.24262752 0.24084732
 0.23885244 0.23703106 0.23534729 0.23396505 0.23263247 0.23119399
 0.22945079 0.22749583 0.22577786 0.22459367 0.2237005  0.222612
 0.22113855 0.21958682 0.21859847 0.2180103  0.21739827 0.21628253
 0.21487914 0.21371551 0.21291094 0.2118643  0.20991369 0.20708582
 0.20399566 0.20161717 0.199727   0.19793622 0.19589002 0.19388452
 0.19195631 0.19015408 0.18868886 0.18739185 0.1864793  0.18567006
 0.18500802 0.18417372 0.18356654 0.18313596 0.18289275 0.1825302
 0.18171045 0.18065298 0.17983575 0.17947602 0.1788296  0.17755176
 0.17546663 0.17350523 0.17207715 0.17111151 0.17014404 0.16913164
 0.168146   0.1673638  0.16711654 0.16695316 0.16671191 0.16634846
 0.16616373 0.16594246 0.16578622 0.16544124 0.16493785 0.16434827
 0.16407135 0.16403948 0.1642465  0.16422163 0.16359998 0.16256982
 0.16119769 0.15997805 0.15878496 0.15776534 0.15680538 0.15605316
 0.15565893 0.15536174 0.1552787  0.15513022 0.15521236 0.15523507
 0.15525144 0.15476364 0.15409856 0.1535134  0.15319663 0.15306704
 0.15286241 0.15237801 0.15183306 0.15128003 0.1505021  0.14921483
 0.14727561 0.14534377 0.14372866 0.1426467  0.14160271 0.14053248
 0.13949704 0.13853692 0.1376761  0.13687624 0.13633397 0.1359169
 0.13565217 0.13514079 0.13464978 0.1342259  0.13389519 0.1334298
 0.13296093 0.1326937  0.13274355 0.13258855 0.13163386 0.12992638
 0.12755527 0.1256978  0.12406921 0.12290283 0.12194612 0.12114915
 0.12053116 0.11990775 0.11960871 0.11953713 0.11968195 0.11963274
 0.11997008 0.12010946 0.12020692 0.12021608 0.12025545 0.12006821
 0.11976024 0.11961173 0.11965949 0.1196817  0.11918185 0.11799482
 0.11615058 0.11483771 0.1136567  0.11266007 0.11157037 0.11047729
 0.10990004 0.10956824 0.10948449 0.10914437 0.10911468 0.1092765
 0.10968927 0.10975108 0.10963025 0.1095809  0.10967747 0.10970131
 0.10979922 0.10990079 0.11012095 0.11040143 0.11032514 0.10980339
 0.10886505 0.10835215 0.10793017 0.10778419 0.10738503 0.10690473
 0.10673776 0.10701093 0.10766272 0.107949   0.10808332 0.10780992
 0.10784845 0.1080023  0.10823298 0.1082451  0.10801224 0.10789483
 0.10797164 0.10831048 0.10869262 0.10883717 0.10865578 0.10807014
 0.10703626 0.10592709 0.10491799 0.10430592 0.10377405 0.10379315
 0.1039478  0.10426591 0.10441968 0.10454826 0.10494936 0.10525667
 0.10540731 0.10504709 0.10489371 0.10518467 0.10580138 0.10622706
 0.10633636 0.10654805 0.10713364 0.10789008 0.10805199 0.107416
 0.10596624 0.10500341 0.10449028 0.10447615 0.10408606 0.10334829
 0.10281613 0.10330607 0.10450462 0.1053596  0.10593694 0.10619321
 0.10703979 0.10812518 0.10887066 0.10908964 0.10891603 0.10902902
 0.10932381 0.10986312 0.11057164 0.11112671 0.1114326  0.11153746
 0.11127653 0.11090358 0.11027385 0.1098946  0.10960703 0.10997399
 0.11053879 0.11082581 0.11096186 0.11110274 0.11189485 0.11264671
 0.11330384 0.11315521 0.11296136 0.11297318 0.11338555 0.1138457
 0.11417079 0.11431723 0.11463156 0.11512675 0.1154009  0.11550722
 0.11500525 0.11463331 0.114233   0.11406031 0.11337557 0.11263236
 0.11240748 0.1128897  0.11420397 0.11487553 0.11505469 0.11519577
 0.11625092 0.11731955 0.11808345 0.11820317 0.11800718 0.11820519
 0.11878099 0.11957366 0.12016955 0.12043951 0.12056276 0.12048896
 0.11974236 0.11883529 0.11757328 0.11674897 0.11616106 0.1160852
 0.11562661 0.11526947 0.11543138 0.1163004  0.11786677 0.11880834
 0.11914326 0.11836621 0.11839013 0.11919943 0.11982862 0.11907699
 0.11773168 0.11703359 0.11745123 0.1178626  0.11563775 0.1098914 ]
