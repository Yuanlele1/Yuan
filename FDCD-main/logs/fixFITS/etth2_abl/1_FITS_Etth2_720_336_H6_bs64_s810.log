Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=810, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  50401792.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.516258716583252
Epoch: 1, Steps: 59 | Train Loss: 0.8316544 Vali Loss: 0.5044382 Test Loss: 0.3983521
Validation loss decreased (inf --> 0.504438).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.4525063037872314
Epoch: 2, Steps: 59 | Train Loss: 0.6980794 Vali Loss: 0.4524349 Test Loss: 0.3777526
Validation loss decreased (0.504438 --> 0.452435).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.5354933738708496
Epoch: 3, Steps: 59 | Train Loss: 0.6675408 Vali Loss: 0.4330877 Test Loss: 0.3701314
Validation loss decreased (0.452435 --> 0.433088).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.4127922058105469
Epoch: 4, Steps: 59 | Train Loss: 0.6519767 Vali Loss: 0.4181768 Test Loss: 0.3664273
Validation loss decreased (0.433088 --> 0.418177).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.4176008701324463
Epoch: 5, Steps: 59 | Train Loss: 0.6449035 Vali Loss: 0.4134804 Test Loss: 0.3639165
Validation loss decreased (0.418177 --> 0.413480).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.3944237232208252
Epoch: 6, Steps: 59 | Train Loss: 0.6383780 Vali Loss: 0.4059277 Test Loss: 0.3624352
Validation loss decreased (0.413480 --> 0.405928).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.4889473915100098
Epoch: 7, Steps: 59 | Train Loss: 0.6314883 Vali Loss: 0.4037263 Test Loss: 0.3612711
Validation loss decreased (0.405928 --> 0.403726).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.493884801864624
Epoch: 8, Steps: 59 | Train Loss: 0.6312725 Vali Loss: 0.3992951 Test Loss: 0.3605204
Validation loss decreased (0.403726 --> 0.399295).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.452113151550293
Epoch: 9, Steps: 59 | Train Loss: 0.6278132 Vali Loss: 0.3980018 Test Loss: 0.3602916
Validation loss decreased (0.399295 --> 0.398002).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.6350796222686768
Epoch: 10, Steps: 59 | Train Loss: 0.6273740 Vali Loss: 0.3968430 Test Loss: 0.3596544
Validation loss decreased (0.398002 --> 0.396843).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.882169485092163
Epoch: 11, Steps: 59 | Train Loss: 0.6254523 Vali Loss: 0.3949872 Test Loss: 0.3593009
Validation loss decreased (0.396843 --> 0.394987).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.5023515224456787
Epoch: 12, Steps: 59 | Train Loss: 0.6233958 Vali Loss: 0.3920888 Test Loss: 0.3592756
Validation loss decreased (0.394987 --> 0.392089).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.4878132343292236
Epoch: 13, Steps: 59 | Train Loss: 0.6217631 Vali Loss: 0.3907261 Test Loss: 0.3590880
Validation loss decreased (0.392089 --> 0.390726).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.5092120170593262
Epoch: 14, Steps: 59 | Train Loss: 0.6205745 Vali Loss: 0.3884229 Test Loss: 0.3590362
Validation loss decreased (0.390726 --> 0.388423).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.804196834564209
Epoch: 15, Steps: 59 | Train Loss: 0.6193208 Vali Loss: 0.3895708 Test Loss: 0.3589264
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.4494984149932861
Epoch: 16, Steps: 59 | Train Loss: 0.6191190 Vali Loss: 0.3879963 Test Loss: 0.3589107
Validation loss decreased (0.388423 --> 0.387996).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.4864976406097412
Epoch: 17, Steps: 59 | Train Loss: 0.6188335 Vali Loss: 0.3880877 Test Loss: 0.3587028
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.4240903854370117
Epoch: 18, Steps: 59 | Train Loss: 0.6195830 Vali Loss: 0.3859937 Test Loss: 0.3586361
Validation loss decreased (0.387996 --> 0.385994).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.3985037803649902
Epoch: 19, Steps: 59 | Train Loss: 0.6183480 Vali Loss: 0.3857807 Test Loss: 0.3585473
Validation loss decreased (0.385994 --> 0.385781).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.4157543182373047
Epoch: 20, Steps: 59 | Train Loss: 0.6174373 Vali Loss: 0.3853831 Test Loss: 0.3586062
Validation loss decreased (0.385781 --> 0.385383).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.4801044464111328
Epoch: 21, Steps: 59 | Train Loss: 0.6176771 Vali Loss: 0.3881711 Test Loss: 0.3584571
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.517655611038208
Epoch: 22, Steps: 59 | Train Loss: 0.6150853 Vali Loss: 0.3841264 Test Loss: 0.3585151
Validation loss decreased (0.385383 --> 0.384126).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.6697397232055664
Epoch: 23, Steps: 59 | Train Loss: 0.6167144 Vali Loss: 0.3849443 Test Loss: 0.3584197
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.5275425910949707
Epoch: 24, Steps: 59 | Train Loss: 0.6167800 Vali Loss: 0.3858064 Test Loss: 0.3585010
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.3796765804290771
Epoch: 25, Steps: 59 | Train Loss: 0.6163189 Vali Loss: 0.3831931 Test Loss: 0.3583255
Validation loss decreased (0.384126 --> 0.383193).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.434239387512207
Epoch: 26, Steps: 59 | Train Loss: 0.6160538 Vali Loss: 0.3850650 Test Loss: 0.3584471
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.5024573802947998
Epoch: 27, Steps: 59 | Train Loss: 0.6149553 Vali Loss: 0.3832201 Test Loss: 0.3584219
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.5153567790985107
Epoch: 28, Steps: 59 | Train Loss: 0.6155693 Vali Loss: 0.3843677 Test Loss: 0.3583485
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.9304463863372803
Epoch: 29, Steps: 59 | Train Loss: 0.6154486 Vali Loss: 0.3837304 Test Loss: 0.3583462
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.548412561416626
Epoch: 30, Steps: 59 | Train Loss: 0.6138914 Vali Loss: 0.3843384 Test Loss: 0.3583608
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.473093032836914
Epoch: 31, Steps: 59 | Train Loss: 0.6147867 Vali Loss: 0.3813298 Test Loss: 0.3583423
Validation loss decreased (0.383193 --> 0.381330).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.4878599643707275
Epoch: 32, Steps: 59 | Train Loss: 0.6142777 Vali Loss: 0.3838025 Test Loss: 0.3583615
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.4290218353271484
Epoch: 33, Steps: 59 | Train Loss: 0.6139573 Vali Loss: 0.3832076 Test Loss: 0.3583372
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.389495849609375
Epoch: 34, Steps: 59 | Train Loss: 0.6138187 Vali Loss: 0.3832617 Test Loss: 0.3583478
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.45676589012146
Epoch: 35, Steps: 59 | Train Loss: 0.6149973 Vali Loss: 0.3785686 Test Loss: 0.3582911
Validation loss decreased (0.381330 --> 0.378569).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.4555535316467285
Epoch: 36, Steps: 59 | Train Loss: 0.6135867 Vali Loss: 0.3827248 Test Loss: 0.3582773
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.4189033508300781
Epoch: 37, Steps: 59 | Train Loss: 0.6120838 Vali Loss: 0.3814094 Test Loss: 0.3582504
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.120593547821045
Epoch: 38, Steps: 59 | Train Loss: 0.6121343 Vali Loss: 0.3808446 Test Loss: 0.3582985
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.3944547176361084
Epoch: 39, Steps: 59 | Train Loss: 0.6135057 Vali Loss: 0.3823490 Test Loss: 0.3583272
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.388747215270996
Epoch: 40, Steps: 59 | Train Loss: 0.6139077 Vali Loss: 0.3831581 Test Loss: 0.3582473
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.404341697692871
Epoch: 41, Steps: 59 | Train Loss: 0.6140569 Vali Loss: 0.3833152 Test Loss: 0.3582803
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.4197189807891846
Epoch: 42, Steps: 59 | Train Loss: 0.6125559 Vali Loss: 0.3809877 Test Loss: 0.3582256
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.4141371250152588
Epoch: 43, Steps: 59 | Train Loss: 0.6127156 Vali Loss: 0.3827063 Test Loss: 0.3582529
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.445958137512207
Epoch: 44, Steps: 59 | Train Loss: 0.6125955 Vali Loss: 0.3803751 Test Loss: 0.3582873
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.5915379524230957
Epoch: 45, Steps: 59 | Train Loss: 0.6126253 Vali Loss: 0.3819183 Test Loss: 0.3582895
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.5212523937225342
Epoch: 46, Steps: 59 | Train Loss: 0.6112108 Vali Loss: 0.3805905 Test Loss: 0.3582799
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.5482687950134277
Epoch: 47, Steps: 59 | Train Loss: 0.6123137 Vali Loss: 0.3817476 Test Loss: 0.3582821
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.382209062576294
Epoch: 48, Steps: 59 | Train Loss: 0.6139260 Vali Loss: 0.3820533 Test Loss: 0.3583111
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.8298850059509277
Epoch: 49, Steps: 59 | Train Loss: 0.6117485 Vali Loss: 0.3815692 Test Loss: 0.3582655
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.4473013877868652
Epoch: 50, Steps: 59 | Train Loss: 0.6132316 Vali Loss: 0.3805019 Test Loss: 0.3582570
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.4698827266693115
Epoch: 51, Steps: 59 | Train Loss: 0.6129546 Vali Loss: 0.3801270 Test Loss: 0.3582590
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.402360439300537
Epoch: 52, Steps: 59 | Train Loss: 0.6127263 Vali Loss: 0.3813118 Test Loss: 0.3582554
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.6901638507843018
Epoch: 53, Steps: 59 | Train Loss: 0.6112265 Vali Loss: 0.3814589 Test Loss: 0.3582744
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.5327425003051758
Epoch: 54, Steps: 59 | Train Loss: 0.6125599 Vali Loss: 0.3794879 Test Loss: 0.3582618
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.6959328651428223
Epoch: 55, Steps: 59 | Train Loss: 0.6134294 Vali Loss: 0.3797573 Test Loss: 0.3582678
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.354046493768692, mae:0.395670086145401, rse:0.4757400155067444, corr:[0.25880772 0.26374245 0.26108065 0.26298976 0.26267108 0.26058
 0.26095355 0.26112506 0.25908425 0.25775084 0.2573849  0.2562247
 0.2548768  0.25411177 0.25334355 0.25229737 0.25180608 0.2513698
 0.25032392 0.24895859 0.24812557 0.2475617  0.24636735 0.24494539
 0.24351135 0.2421689  0.24089128 0.23995148 0.23915248 0.23822705
 0.23731384 0.23637493 0.2354989  0.23460506 0.2337097  0.2327725
 0.23209533 0.23164628 0.23079842 0.22967958 0.22891378 0.22846062
 0.22780012 0.22679836 0.22580822 0.22513671 0.22399203 0.22202621
 0.22017199 0.218914   0.21773683 0.21642153 0.214867   0.21290664
 0.21120909 0.21027088 0.2090668  0.20720652 0.20570596 0.2048109
 0.20388775 0.2032809  0.2035376  0.20369218 0.20293073 0.20235658
 0.20220041 0.20188837 0.20116685 0.20050363 0.19982654 0.19897035
 0.19818695 0.19767703 0.19694601 0.19566843 0.19466381 0.19401293
 0.19313341 0.1923076  0.19228685 0.19214809 0.1913109  0.19065191
 0.1907071  0.19062443 0.18999614 0.18967311 0.18983808 0.18957393
 0.18871886 0.18831961 0.18879528 0.18902352 0.18871425 0.18834713
 0.18797411 0.18721806 0.18650506 0.18619506 0.18588993 0.18499036
 0.18418363 0.18386881 0.18378662 0.1833063  0.18305612 0.18313357
 0.18279004 0.18194683 0.18116088 0.1809211  0.18064901 0.18013757
 0.17943032 0.17893039 0.17871578 0.17821418 0.17730652 0.17605007
 0.17485069 0.17379215 0.17267554 0.17148314 0.17072617 0.17042825
 0.16975228 0.16866441 0.16799593 0.16779834 0.1672813  0.16611436
 0.16507468 0.16441551 0.16409907 0.16400269 0.16384242 0.16319978
 0.16203067 0.16120456 0.16106378 0.16086473 0.16008589 0.1587647
 0.15710983 0.15549766 0.1542498  0.15345536 0.15258555 0.15142705
 0.15045792 0.15005217 0.14997919 0.14933787 0.14828031 0.14736046
 0.146712   0.1462021  0.14599548 0.14596108 0.14568117 0.14538911
 0.1449367  0.14441122 0.14425205 0.14443588 0.14422457 0.14296435
 0.14134508 0.1400046  0.13909182 0.13811915 0.13698496 0.13566998
 0.13475938 0.13396294 0.13320716 0.13219476 0.13088751 0.13022901
 0.13015725 0.13017756 0.13015464 0.13024198 0.12980208 0.12906101
 0.12844826 0.1283581  0.12857307 0.12852518 0.12865174 0.12870748
 0.12845874 0.12790355 0.12728071 0.12689354 0.12632506 0.12552796
 0.12491346 0.12437912 0.12396406 0.12370261 0.12376539 0.12367928
 0.12311013 0.12269102 0.12257075 0.12274729 0.1228533  0.12304116
 0.12319391 0.1231855  0.12296883 0.1226479  0.12240151 0.12212792
 0.12176655 0.12094704 0.11967589 0.11879113 0.11846007 0.11842752
 0.11791646 0.11784976 0.11825189 0.11851451 0.118222   0.11763292
 0.11696335 0.11631767 0.11590009 0.11636895 0.11683103 0.11692282
 0.11708125 0.11771895 0.11811908 0.1179937  0.11792783 0.11804174
 0.11759598 0.11667947 0.11631162 0.11655212 0.11608145 0.11501783
 0.11473408 0.11490578 0.11494563 0.11491503 0.11523347 0.11571169
 0.11576201 0.11626986 0.11706956 0.11822411 0.11891357 0.1197045
 0.12033262 0.12097655 0.12154726 0.12156197 0.12170657 0.12222455
 0.12317649 0.12330785 0.12278611 0.12261911 0.12298942 0.12318008
 0.12275285 0.12243855 0.12252983 0.1222318  0.12250336 0.12289876
 0.12294418 0.12263857 0.12284873 0.12342674 0.12334304 0.12299338
 0.12350526 0.12421339 0.12361786 0.12294237 0.12353578 0.12407006
 0.12335205 0.12259694 0.12244804 0.1218975  0.12081587 0.12027496
 0.12026741 0.11933127 0.11838283 0.11863311 0.1191053  0.11818957
 0.11713143 0.11758631 0.11784555 0.1177929  0.11825995 0.11965553
 0.11934565 0.11887977 0.11977368 0.1206658  0.12011041 0.11960667
 0.11990654 0.11873155 0.11707477 0.11687273 0.11694657 0.11529952
 0.11425275 0.1158731  0.11662294 0.11601531 0.11645319 0.11834503
 0.1179923  0.11712146 0.11887834 0.12047689 0.11983518 0.12017085
 0.12164652 0.12087005 0.12119257 0.12446557 0.12400597 0.11717384]
