Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_360_720_FITS_ETTh2_ftM_sl360_ll48_pl720_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=42, out_features=126, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4741632.0
params:  5418.0
Trainable parameters:  5418
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.000030755996704
Epoch: 1, Steps: 59 | Train Loss: 1.1078462 Vali Loss: 0.8020306 Test Loss: 0.5079831
Validation loss decreased (inf --> 0.802031).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.5877659320831299
Epoch: 2, Steps: 59 | Train Loss: 0.9562834 Vali Loss: 0.7426363 Test Loss: 0.4560580
Validation loss decreased (0.802031 --> 0.742636).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.6199874877929688
Epoch: 3, Steps: 59 | Train Loss: 0.8886200 Vali Loss: 0.7102066 Test Loss: 0.4314605
Validation loss decreased (0.742636 --> 0.710207).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.1178653240203857
Epoch: 4, Steps: 59 | Train Loss: 0.8584550 Vali Loss: 0.6953472 Test Loss: 0.4188998
Validation loss decreased (0.710207 --> 0.695347).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.512317180633545
Epoch: 5, Steps: 59 | Train Loss: 0.8416800 Vali Loss: 0.6870332 Test Loss: 0.4115920
Validation loss decreased (0.695347 --> 0.687033).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.040095567703247
Epoch: 6, Steps: 59 | Train Loss: 0.8324751 Vali Loss: 0.6776097 Test Loss: 0.4070067
Validation loss decreased (0.687033 --> 0.677610).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.9857149124145508
Epoch: 7, Steps: 59 | Train Loss: 0.8257421 Vali Loss: 0.6754907 Test Loss: 0.4038732
Validation loss decreased (0.677610 --> 0.675491).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.112654685974121
Epoch: 8, Steps: 59 | Train Loss: 0.8223026 Vali Loss: 0.6694422 Test Loss: 0.4014547
Validation loss decreased (0.675491 --> 0.669442).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.7078697681427002
Epoch: 9, Steps: 59 | Train Loss: 0.8199507 Vali Loss: 0.6683611 Test Loss: 0.3996063
Validation loss decreased (0.669442 --> 0.668361).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.9830882549285889
Epoch: 10, Steps: 59 | Train Loss: 0.8172727 Vali Loss: 0.6629990 Test Loss: 0.3981113
Validation loss decreased (0.668361 --> 0.662999).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.06807541847229
Epoch: 11, Steps: 59 | Train Loss: 0.8148964 Vali Loss: 0.6573437 Test Loss: 0.3968835
Validation loss decreased (0.662999 --> 0.657344).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.9722836017608643
Epoch: 12, Steps: 59 | Train Loss: 0.8137706 Vali Loss: 0.6590358 Test Loss: 0.3958305
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.0125834941864014
Epoch: 13, Steps: 59 | Train Loss: 0.8118410 Vali Loss: 0.6581002 Test Loss: 0.3949934
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.057619094848633
Epoch: 14, Steps: 59 | Train Loss: 0.8100785 Vali Loss: 0.6548741 Test Loss: 0.3943298
Validation loss decreased (0.657344 --> 0.654874).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.438185453414917
Epoch: 15, Steps: 59 | Train Loss: 0.8104169 Vali Loss: 0.6600001 Test Loss: 0.3936608
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.8073797225952148
Epoch: 16, Steps: 59 | Train Loss: 0.8093354 Vali Loss: 0.6549962 Test Loss: 0.3931748
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.5638432502746582
Epoch: 17, Steps: 59 | Train Loss: 0.8087001 Vali Loss: 0.6550611 Test Loss: 0.3927267
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.1670665740966797
Epoch: 18, Steps: 59 | Train Loss: 0.8073217 Vali Loss: 0.6545334 Test Loss: 0.3923500
Validation loss decreased (0.654874 --> 0.654533).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.8936421871185303
Epoch: 19, Steps: 59 | Train Loss: 0.8070554 Vali Loss: 0.6524937 Test Loss: 0.3920134
Validation loss decreased (0.654533 --> 0.652494).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.4896080493927
Epoch: 20, Steps: 59 | Train Loss: 0.8068214 Vali Loss: 0.6503667 Test Loss: 0.3917404
Validation loss decreased (0.652494 --> 0.650367).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.4237313270568848
Epoch: 21, Steps: 59 | Train Loss: 0.8058766 Vali Loss: 0.6529535 Test Loss: 0.3914998
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.929762363433838
Epoch: 22, Steps: 59 | Train Loss: 0.8060913 Vali Loss: 0.6511364 Test Loss: 0.3912703
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.1788992881774902
Epoch: 23, Steps: 59 | Train Loss: 0.8047990 Vali Loss: 0.6514416 Test Loss: 0.3910927
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.0141379833221436
Epoch: 24, Steps: 59 | Train Loss: 0.8047885 Vali Loss: 0.6491703 Test Loss: 0.3909094
Validation loss decreased (0.650367 --> 0.649170).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.9017555713653564
Epoch: 25, Steps: 59 | Train Loss: 0.8050068 Vali Loss: 0.6507140 Test Loss: 0.3907712
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.7581429481506348
Epoch: 26, Steps: 59 | Train Loss: 0.8046828 Vali Loss: 0.6479698 Test Loss: 0.3906146
Validation loss decreased (0.649170 --> 0.647970).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.1306354999542236
Epoch: 27, Steps: 59 | Train Loss: 0.8045771 Vali Loss: 0.6433555 Test Loss: 0.3904868
Validation loss decreased (0.647970 --> 0.643356).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.6662876605987549
Epoch: 28, Steps: 59 | Train Loss: 0.8043217 Vali Loss: 0.6445521 Test Loss: 0.3903835
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.6919562816619873
Epoch: 29, Steps: 59 | Train Loss: 0.8038567 Vali Loss: 0.6468294 Test Loss: 0.3902857
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.3421947956085205
Epoch: 30, Steps: 59 | Train Loss: 0.8033843 Vali Loss: 0.6511763 Test Loss: 0.3902209
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.2201151847839355
Epoch: 31, Steps: 59 | Train Loss: 0.8023052 Vali Loss: 0.6467314 Test Loss: 0.3901226
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.3755393028259277
Epoch: 32, Steps: 59 | Train Loss: 0.8029214 Vali Loss: 0.6497635 Test Loss: 0.3900532
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.1045281887054443
Epoch: 33, Steps: 59 | Train Loss: 0.8028845 Vali Loss: 0.6485124 Test Loss: 0.3899868
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.191279172897339
Epoch: 34, Steps: 59 | Train Loss: 0.8032253 Vali Loss: 0.6452906 Test Loss: 0.3899099
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.21690034866333
Epoch: 35, Steps: 59 | Train Loss: 0.8028910 Vali Loss: 0.6455150 Test Loss: 0.3898694
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.804387092590332
Epoch: 36, Steps: 59 | Train Loss: 0.8024217 Vali Loss: 0.6454585 Test Loss: 0.3897991
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.214214324951172
Epoch: 37, Steps: 59 | Train Loss: 0.8028956 Vali Loss: 0.6452509 Test Loss: 0.3897722
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.8816328048706055
Epoch: 38, Steps: 59 | Train Loss: 0.8028686 Vali Loss: 0.6478119 Test Loss: 0.3897558
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.9245805740356445
Epoch: 39, Steps: 59 | Train Loss: 0.8025958 Vali Loss: 0.6493310 Test Loss: 0.3896917
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.0615930557250977
Epoch: 40, Steps: 59 | Train Loss: 0.8021842 Vali Loss: 0.6484249 Test Loss: 0.3896556
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.3063180446624756
Epoch: 41, Steps: 59 | Train Loss: 0.8016133 Vali Loss: 0.6418155 Test Loss: 0.3896254
Validation loss decreased (0.643356 --> 0.641816).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.4789681434631348
Epoch: 42, Steps: 59 | Train Loss: 0.8025403 Vali Loss: 0.6454094 Test Loss: 0.3896060
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.009105920791626
Epoch: 43, Steps: 59 | Train Loss: 0.8022015 Vali Loss: 0.6468860 Test Loss: 0.3895572
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.2438125610351562
Epoch: 44, Steps: 59 | Train Loss: 0.8020215 Vali Loss: 0.6473910 Test Loss: 0.3895444
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.7463905811309814
Epoch: 45, Steps: 59 | Train Loss: 0.8010916 Vali Loss: 0.6426355 Test Loss: 0.3895150
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.0235037803649902
Epoch: 46, Steps: 59 | Train Loss: 0.8007797 Vali Loss: 0.6471496 Test Loss: 0.3894933
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.0649831295013428
Epoch: 47, Steps: 59 | Train Loss: 0.8023396 Vali Loss: 0.6504971 Test Loss: 0.3894768
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.7853689193725586
Epoch: 48, Steps: 59 | Train Loss: 0.8019446 Vali Loss: 0.6477870 Test Loss: 0.3894590
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.9071767330169678
Epoch: 49, Steps: 59 | Train Loss: 0.8020121 Vali Loss: 0.6497274 Test Loss: 0.3894337
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.6780402660369873
Epoch: 50, Steps: 59 | Train Loss: 0.8011112 Vali Loss: 0.6462365 Test Loss: 0.3894131
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.1865105628967285
Epoch: 51, Steps: 59 | Train Loss: 0.8017719 Vali Loss: 0.6464256 Test Loss: 0.3893929
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.8008506298065186
Epoch: 52, Steps: 59 | Train Loss: 0.8012254 Vali Loss: 0.6513775 Test Loss: 0.3893771
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.3339743614196777
Epoch: 53, Steps: 59 | Train Loss: 0.8018135 Vali Loss: 0.6442081 Test Loss: 0.3893774
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.095921754837036
Epoch: 54, Steps: 59 | Train Loss: 0.8018485 Vali Loss: 0.6467711 Test Loss: 0.3893555
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.81834077835083
Epoch: 55, Steps: 59 | Train Loss: 0.8014890 Vali Loss: 0.6471582 Test Loss: 0.3893467
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.6108646392822266
Epoch: 56, Steps: 59 | Train Loss: 0.8019640 Vali Loss: 0.6430869 Test Loss: 0.3893394
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.144796133041382
Epoch: 57, Steps: 59 | Train Loss: 0.8014060 Vali Loss: 0.6451539 Test Loss: 0.3893354
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.1530683040618896
Epoch: 58, Steps: 59 | Train Loss: 0.8012261 Vali Loss: 0.6476373 Test Loss: 0.3893244
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.4673871994018555
Epoch: 59, Steps: 59 | Train Loss: 0.8013775 Vali Loss: 0.6461283 Test Loss: 0.3893073
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.9256937503814697
Epoch: 60, Steps: 59 | Train Loss: 0.8009802 Vali Loss: 0.6451581 Test Loss: 0.3893026
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.3927102088928223
Epoch: 61, Steps: 59 | Train Loss: 0.8015220 Vali Loss: 0.6471797 Test Loss: 0.3892856
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_360_720_FITS_ETTh2_ftM_sl360_ll48_pl720_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.3875916004180908, mae:0.42463570833206177, rse:0.4976143538951874, corr:[ 0.21708941  0.21846792  0.21905658  0.21824513  0.21658002  0.21512331
  0.21425618  0.21372253  0.21313292  0.21207729  0.21066417  0.2090372
  0.20761895  0.20654291  0.20577078  0.20522372  0.20469788  0.20395766
  0.20294231  0.2017276   0.2004149   0.19915734  0.19796525  0.19684076
  0.19575489  0.19464971  0.19346377  0.19216566  0.1909366   0.18984236
  0.1889351   0.18811586  0.18723379  0.18613344  0.1848765   0.18356694
  0.18230541  0.18119031  0.18040495  0.17984739  0.1793549   0.17873842
  0.17787623  0.17686084  0.17566712  0.17454462  0.1734654   0.1722672
  0.17109683  0.17009014  0.16912457  0.168242    0.1672954   0.1663634
  0.16546229  0.16467077  0.1640459   0.16354854  0.16308971  0.16278104
  0.1627099   0.16271913  0.16280802  0.16288644  0.16292341  0.16288765
  0.16275355  0.16257617  0.16231775  0.16211182  0.16195215  0.16183284
  0.16168636  0.16144902  0.16114593  0.16070624  0.16021514  0.15978746
  0.15944712  0.15927689  0.15926933  0.15935278  0.1594326   0.15941173
  0.15931712  0.15914923  0.15898725  0.15888333  0.15885533  0.15894766
  0.1590891   0.15921308  0.15933089  0.15930665  0.15920451  0.158971
  0.15873653  0.15852682  0.15822701  0.15783057  0.15744181  0.15695459
  0.15656649  0.15628971  0.15623032  0.15626462  0.15633102  0.15641424
  0.15630308  0.15597837  0.15551321  0.15508628  0.15473375  0.15451798
  0.15442033  0.15438189  0.15427503  0.1538688   0.15322548  0.15237525
  0.15144359  0.15062994  0.14995873  0.14950638  0.1491746   0.14896485
  0.14866948  0.14827962  0.14781277  0.14721698  0.1467314   0.14633036
  0.14598191  0.14553209  0.14497593  0.14441866  0.1437979   0.14328657
  0.14283714  0.1426048   0.14251031  0.1423215   0.14177948  0.14096701
  0.13988198  0.13881297  0.1379179   0.13736662  0.13711429  0.1370273
  0.13709792  0.13715315  0.13706656  0.13673273  0.1362121   0.13548967
  0.13486739  0.13454482  0.13442056  0.13451315  0.13470457  0.13486174
  0.13487819  0.13475053  0.13456872  0.13435689  0.13421778  0.13407846
  0.13392855  0.13369262  0.13323212  0.13265894  0.13206917  0.13156042
  0.13114855  0.13101332  0.13109335  0.13122132  0.13114916  0.13088767
  0.13052186  0.13016142  0.12991291  0.12989825  0.13023354  0.13079588
  0.13146785  0.13197711  0.13220488  0.13224047  0.13213263  0.13201006
  0.13203476  0.13224696  0.13262516  0.13305974  0.13335541  0.13336903
  0.13325925  0.13310598  0.13294974  0.13293429  0.1331772   0.13338906
  0.13356961  0.13368967  0.13355799  0.1333829   0.1333415   0.13361534
  0.1340822   0.13472359  0.13531348  0.1356151   0.1355028   0.13497382
  0.1343376   0.13381086  0.13362217  0.13382088  0.13425492  0.13478927
  0.13521333  0.13548963  0.13557109  0.13549638  0.13553897  0.13569435
  0.13603202  0.13644381  0.13684775  0.13719922  0.13746935  0.1377469
  0.13813871  0.13872457  0.13941453  0.14004038  0.14052309  0.14084023
  0.14101109  0.1410731   0.14121962  0.14145838  0.14182793  0.14214401
  0.14240468  0.14267278  0.14289048  0.14315666  0.1435673   0.14409742
  0.14480719  0.14572391  0.14649458  0.14713052  0.14753036  0.14780112
  0.14806981  0.14853425  0.1492787   0.1501594   0.15106569  0.15168019
  0.15199943  0.1520447   0.15194985  0.15197349  0.15212099  0.15260397
  0.1533155   0.15418805  0.15496162  0.15558766  0.15610856  0.15649417
  0.15675326  0.15732393  0.158036    0.1587788   0.15946944  0.16006605
  0.16041261  0.16061468  0.1606666   0.16072643  0.16101095  0.16147609
  0.16184111  0.16206619  0.16211963  0.16210859  0.16193223  0.1617726
  0.16169195  0.16181193  0.16222201  0.16281877  0.16343774  0.16404982
  0.16430736  0.16434787  0.16426115  0.16421723  0.16423883  0.16455097
  0.16493998  0.16538827  0.16573955  0.16579136  0.1657721   0.1656036
  0.165438    0.16529651  0.16529413  0.1654016   0.16542354  0.16533121
  0.16511698  0.1647942   0.16447505  0.16426708  0.16438638  0.16473392
  0.16518816  0.16548449  0.16578525  0.16584559  0.16589856  0.16601992
  0.16631643  0.1669241   0.16770904  0.16835453  0.1688027   0.16889708
  0.16872135  0.16835934  0.16805623  0.16793503  0.16809942  0.16840553
  0.16886997  0.16932997  0.16951373  0.16949102  0.16929647  0.16922225
  0.16945794  0.16990416  0.17051554  0.17114587  0.17165592  0.17204763
  0.17238493  0.1728037   0.17313763  0.17343089  0.17363401  0.17371874
  0.17369612  0.17359467  0.17357534  0.1736165   0.17382023  0.17417367
  0.17457235  0.17489612  0.17500158  0.1751131   0.17503135  0.17486753
  0.17458966  0.17430055  0.1740951   0.17399082  0.17397502  0.174034
  0.1740904   0.17419136  0.17423551  0.17416997  0.17391409  0.17359197
  0.17330703  0.173112    0.1730878   0.17317049  0.17327529  0.17332666
  0.17337336  0.17348674  0.17374417  0.17415942  0.17465611  0.1749416
  0.17494869  0.17445096  0.17383665  0.17323542  0.1727133   0.17238092
  0.17223181  0.17217092  0.17212759  0.17194259  0.17154154  0.17103426
  0.17041509  0.16990644  0.16957557  0.16942123  0.16939573  0.16934316
  0.16909601  0.16878195  0.16838177  0.16790491  0.16746631  0.16703771
  0.1664909   0.16581723  0.16498293  0.16395572  0.16271906  0.16140483
  0.16026594  0.15950355  0.1590115   0.15871114  0.15826432  0.15755242
  0.15655802  0.15541296  0.15430056  0.15332776  0.15263519  0.15223125
  0.15208173  0.15179825  0.15119515  0.15018588  0.14898263  0.14782354
  0.14689173  0.14657149  0.14647941  0.14649157  0.14627707  0.14579841
  0.14500524  0.14428455  0.14362215  0.14323159  0.14310923  0.14310393
  0.14303231  0.1427284   0.1421241   0.14141242  0.14066829  0.14004983
  0.13974088  0.13972214  0.13969591  0.13953225  0.13912629  0.13838656
  0.13738985  0.13635233  0.13568911  0.13543175  0.13550733  0.13563108
  0.13564298  0.13527817  0.13454047  0.13359314  0.13254331  0.1315628
  0.13085355  0.1304234   0.13019581  0.12970957  0.12885028  0.1276901
  0.12652843  0.12550908  0.12491496  0.12473311  0.12482602  0.12459481
  0.12402376  0.12290684  0.12164871  0.12047585  0.11955149  0.11906042
  0.11887299  0.11873489  0.1183343   0.1174232   0.11607645  0.11435547
  0.11250065  0.11100926  0.11004563  0.10949677  0.10907074  0.10868396
  0.10806882  0.10735105  0.10661001  0.10593557  0.10539247  0.1047736
  0.10424185  0.1036187   0.10289125  0.10193539  0.10071286  0.09957232
  0.09871104  0.09818888  0.09774075  0.0972032   0.09628265  0.09486368
  0.09304171  0.09110094  0.08948011  0.08819479  0.0873442   0.0867997
  0.08635991  0.0858108   0.08497708  0.08383825  0.08256052  0.08138075
  0.08048481  0.07990529  0.07959095  0.07923423  0.07853289  0.07772381
  0.07668917  0.07565092  0.0747329   0.07380681  0.07296077  0.07190835
  0.07054232  0.06900067  0.06729905  0.06583142  0.06474521  0.06406184
  0.06372223  0.06357427  0.06357356  0.06327963  0.06273949  0.06187966
  0.06100619  0.06043838  0.06020268  0.06005959  0.06001662  0.05982404
  0.05919212  0.05815093  0.05677326  0.05542438  0.05412066  0.05293068
  0.05177604  0.05076492  0.04961823  0.04828309  0.04695913  0.04579919
  0.04495762  0.04434362  0.0439821   0.04381146  0.04354212  0.04327163
  0.042742    0.0420804   0.04138843  0.04086301  0.04046012  0.04024807
  0.04013227  0.03979853  0.03905994  0.0380994   0.03695473  0.03573311
  0.03462673  0.03367323  0.03270966  0.03187335  0.03091414  0.02985403
  0.0288286   0.02787966  0.02713104  0.02691011  0.02707839  0.02736373
  0.02743898  0.02722311  0.02695582  0.02666393  0.02635913  0.02623667
  0.0262606   0.02645501  0.02652398  0.02640328  0.02601792  0.02535571
  0.02454779  0.02371042  0.02308027  0.02279468  0.02276031  0.02287073
  0.02292568  0.02273134  0.02248853  0.02218373  0.02212941  0.02232432
  0.02263412  0.02292652  0.02292223  0.02289953  0.02244795  0.02179368
  0.0210037   0.02029026  0.01989081  0.01965476  0.01935999  0.01873833
  0.01784966  0.01682336  0.01558298  0.01444926  0.0137844   0.01344567
  0.01360471  0.01400325  0.01430844  0.01412394  0.01365428  0.0128666
  0.01208397  0.01194278  0.01238273  0.01323814  0.01395641  0.01426444
  0.01407172  0.01341475  0.01236089  0.01152751  0.01080569  0.01031683
  0.01010202  0.00985907  0.00934469  0.00831428  0.00727561  0.0065033
  0.00642775  0.00703082  0.00803548  0.00926571  0.00989268  0.00979481
  0.00893341  0.00778154  0.00708376  0.0070512   0.00784552  0.00913994
  0.01042542  0.01109053  0.01082295  0.00981816  0.00821438  0.00648313
  0.00471861  0.00366028  0.00325041  0.00297815  0.00263     0.00206951
  0.00165299  0.00138782  0.00190442  0.00289768  0.00379587  0.00437383
  0.00383425  0.00219145 -0.00023326 -0.00215837 -0.00258012 -0.00074667
  0.00184892  0.003461    0.00236959 -0.00294613 -0.01156004 -0.02176489]
