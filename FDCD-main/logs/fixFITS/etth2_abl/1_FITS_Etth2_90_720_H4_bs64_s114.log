Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=26, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_90_720_FITS_ETTh2_ftM_sl90_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=26, out_features=234, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5451264.0
params:  6318.0
Trainable parameters:  6318
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.534830093383789
Epoch: 1, Steps: 61 | Train Loss: 1.3549221 Vali Loss: 0.8501296 Test Loss: 0.6812632
Validation loss decreased (inf --> 0.850130).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.9917354583740234
Epoch: 2, Steps: 61 | Train Loss: 1.1414129 Vali Loss: 0.7617282 Test Loss: 0.5811312
Validation loss decreased (0.850130 --> 0.761728).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.0175106525421143
Epoch: 3, Steps: 61 | Train Loss: 1.0322069 Vali Loss: 0.7219208 Test Loss: 0.5250030
Validation loss decreased (0.761728 --> 0.721921).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.638371229171753
Epoch: 4, Steps: 61 | Train Loss: 0.9696504 Vali Loss: 0.6885530 Test Loss: 0.4911925
Validation loss decreased (0.721921 --> 0.688553).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.7428529262542725
Epoch: 5, Steps: 61 | Train Loss: 0.9336571 Vali Loss: 0.6718910 Test Loss: 0.4697306
Validation loss decreased (0.688553 --> 0.671891).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.809088945388794
Epoch: 6, Steps: 61 | Train Loss: 0.9076479 Vali Loss: 0.6621870 Test Loss: 0.4559199
Validation loss decreased (0.671891 --> 0.662187).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.1993582248687744
Epoch: 7, Steps: 61 | Train Loss: 0.8941215 Vali Loss: 0.6523544 Test Loss: 0.4466613
Validation loss decreased (0.662187 --> 0.652354).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.8771274089813232
Epoch: 8, Steps: 61 | Train Loss: 0.8837193 Vali Loss: 0.6451899 Test Loss: 0.4404128
Validation loss decreased (0.652354 --> 0.645190).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.047727346420288
Epoch: 9, Steps: 61 | Train Loss: 0.8770364 Vali Loss: 0.6433219 Test Loss: 0.4360523
Validation loss decreased (0.645190 --> 0.643322).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.2078707218170166
Epoch: 10, Steps: 61 | Train Loss: 0.8708282 Vali Loss: 0.6411114 Test Loss: 0.4329446
Validation loss decreased (0.643322 --> 0.641111).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.5475671291351318
Epoch: 11, Steps: 61 | Train Loss: 0.8678187 Vali Loss: 0.6394652 Test Loss: 0.4307398
Validation loss decreased (0.641111 --> 0.639465).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.9831976890563965
Epoch: 12, Steps: 61 | Train Loss: 0.8652781 Vali Loss: 0.6354159 Test Loss: 0.4290754
Validation loss decreased (0.639465 --> 0.635416).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.438190221786499
Epoch: 13, Steps: 61 | Train Loss: 0.8631249 Vali Loss: 0.6314120 Test Loss: 0.4278138
Validation loss decreased (0.635416 --> 0.631412).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.0324602127075195
Epoch: 14, Steps: 61 | Train Loss: 0.8624701 Vali Loss: 0.6346617 Test Loss: 0.4268545
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.041243076324463
Epoch: 15, Steps: 61 | Train Loss: 0.8603316 Vali Loss: 0.6357418 Test Loss: 0.4261163
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.6707634925842285
Epoch: 16, Steps: 61 | Train Loss: 0.8608643 Vali Loss: 0.6351004 Test Loss: 0.4254936
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.6445930004119873
Epoch: 17, Steps: 61 | Train Loss: 0.8588429 Vali Loss: 0.6380236 Test Loss: 0.4250088
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.0236992835998535
Epoch: 18, Steps: 61 | Train Loss: 0.8595429 Vali Loss: 0.6370791 Test Loss: 0.4245816
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.849179744720459
Epoch: 19, Steps: 61 | Train Loss: 0.8583373 Vali Loss: 0.6331949 Test Loss: 0.4242457
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.842210054397583
Epoch: 20, Steps: 61 | Train Loss: 0.8574468 Vali Loss: 0.6281343 Test Loss: 0.4239406
Validation loss decreased (0.631412 --> 0.628134).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.310790777206421
Epoch: 21, Steps: 61 | Train Loss: 0.8545649 Vali Loss: 0.6329183 Test Loss: 0.4237035
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.2316832542419434
Epoch: 22, Steps: 61 | Train Loss: 0.8563387 Vali Loss: 0.6344290 Test Loss: 0.4234680
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.7777791023254395
Epoch: 23, Steps: 61 | Train Loss: 0.8562893 Vali Loss: 0.6311164 Test Loss: 0.4232727
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.8916423320770264
Epoch: 24, Steps: 61 | Train Loss: 0.8548128 Vali Loss: 0.6305511 Test Loss: 0.4230921
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.50675630569458
Epoch: 25, Steps: 61 | Train Loss: 0.8544318 Vali Loss: 0.6283922 Test Loss: 0.4229363
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.1463801860809326
Epoch: 26, Steps: 61 | Train Loss: 0.8532669 Vali Loss: 0.6302253 Test Loss: 0.4228016
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.480962038040161
Epoch: 27, Steps: 61 | Train Loss: 0.8532991 Vali Loss: 0.6325576 Test Loss: 0.4226743
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.3305702209472656
Epoch: 28, Steps: 61 | Train Loss: 0.8535212 Vali Loss: 0.6352437 Test Loss: 0.4225752
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.454392433166504
Epoch: 29, Steps: 61 | Train Loss: 0.8543130 Vali Loss: 0.6304332 Test Loss: 0.4224575
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.2513062953948975
Epoch: 30, Steps: 61 | Train Loss: 0.8545524 Vali Loss: 0.6279037 Test Loss: 0.4223613
Validation loss decreased (0.628134 --> 0.627904).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.7529611587524414
Epoch: 31, Steps: 61 | Train Loss: 0.8541834 Vali Loss: 0.6346152 Test Loss: 0.4222684
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.0853350162506104
Epoch: 32, Steps: 61 | Train Loss: 0.8535993 Vali Loss: 0.6284556 Test Loss: 0.4221990
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.8814969062805176
Epoch: 33, Steps: 61 | Train Loss: 0.8522786 Vali Loss: 0.6338419 Test Loss: 0.4221273
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.8596577644348145
Epoch: 34, Steps: 61 | Train Loss: 0.8526638 Vali Loss: 0.6215925 Test Loss: 0.4220538
Validation loss decreased (0.627904 --> 0.621593).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.921633243560791
Epoch: 35, Steps: 61 | Train Loss: 0.8541340 Vali Loss: 0.6307505 Test Loss: 0.4219961
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.6282265186309814
Epoch: 36, Steps: 61 | Train Loss: 0.8517102 Vali Loss: 0.6299231 Test Loss: 0.4219298
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.5117502212524414
Epoch: 37, Steps: 61 | Train Loss: 0.8528535 Vali Loss: 0.6285427 Test Loss: 0.4218754
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.6272392272949219
Epoch: 38, Steps: 61 | Train Loss: 0.8532792 Vali Loss: 0.6256131 Test Loss: 0.4218262
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.5671353340148926
Epoch: 39, Steps: 61 | Train Loss: 0.8528586 Vali Loss: 0.6270666 Test Loss: 0.4217773
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.8800554275512695
Epoch: 40, Steps: 61 | Train Loss: 0.8530104 Vali Loss: 0.6248195 Test Loss: 0.4217337
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.866880178451538
Epoch: 41, Steps: 61 | Train Loss: 0.8519763 Vali Loss: 0.6260411 Test Loss: 0.4216890
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.8712413311004639
Epoch: 42, Steps: 61 | Train Loss: 0.8515881 Vali Loss: 0.6264546 Test Loss: 0.4216549
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.8083374500274658
Epoch: 43, Steps: 61 | Train Loss: 0.8523388 Vali Loss: 0.6323721 Test Loss: 0.4216137
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.2196614742279053
Epoch: 44, Steps: 61 | Train Loss: 0.8522925 Vali Loss: 0.6265307 Test Loss: 0.4215803
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.8121154308319092
Epoch: 45, Steps: 61 | Train Loss: 0.8524021 Vali Loss: 0.6296289 Test Loss: 0.4215504
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.519439697265625
Epoch: 46, Steps: 61 | Train Loss: 0.8533391 Vali Loss: 0.6283293 Test Loss: 0.4215213
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.0609474182128906
Epoch: 47, Steps: 61 | Train Loss: 0.8513422 Vali Loss: 0.6277119 Test Loss: 0.4214948
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.8774514198303223
Epoch: 48, Steps: 61 | Train Loss: 0.8509364 Vali Loss: 0.6291816 Test Loss: 0.4214627
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.754958152770996
Epoch: 49, Steps: 61 | Train Loss: 0.8503875 Vali Loss: 0.6308020 Test Loss: 0.4214458
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.3454391956329346
Epoch: 50, Steps: 61 | Train Loss: 0.8513772 Vali Loss: 0.6269734 Test Loss: 0.4214191
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.245830535888672
Epoch: 51, Steps: 61 | Train Loss: 0.8523239 Vali Loss: 0.6312681 Test Loss: 0.4213961
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.7756202220916748
Epoch: 52, Steps: 61 | Train Loss: 0.8508991 Vali Loss: 0.6234410 Test Loss: 0.4213744
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.074195623397827
Epoch: 53, Steps: 61 | Train Loss: 0.8525580 Vali Loss: 0.6301122 Test Loss: 0.4213530
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.5360674858093262
Epoch: 54, Steps: 61 | Train Loss: 0.8518089 Vali Loss: 0.6287842 Test Loss: 0.4213379
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_90_720_FITS_ETTh2_ftM_sl90_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4204035997390747, mae:0.4374527037143707, rse:0.5182495713233948, corr:[ 0.21756223  0.22078633  0.21690153  0.21778461  0.2167565   0.21422513
  0.214155    0.21377668  0.21181946  0.2109942   0.21067552  0.20880419
  0.20726307  0.20684838  0.20588489  0.2048525   0.20470567  0.20407528
  0.20249853  0.20169832  0.20140895  0.20029037  0.1989742   0.19768927
  0.19507961  0.1924496   0.1912383   0.19055845  0.18916996  0.18847442
  0.18883915  0.18800288  0.18670362  0.18626331  0.18597026  0.18492053
  0.18413772  0.1838029   0.18306474  0.18249777  0.18238176  0.18195869
  0.18130098  0.18093662  0.18057509  0.17985754  0.17920113  0.17770693
  0.1746355   0.17194082  0.17021993  0.16942495  0.1685045   0.16774657
  0.16769277  0.16691357  0.16655147  0.16627286  0.16634454  0.1661466
  0.16559741  0.16528153  0.16517232  0.1654954   0.16582765  0.1657131
  0.16551413  0.16521195  0.16485815  0.1645204   0.16401437  0.16282776
  0.16078044  0.15982729  0.1593377   0.15875074  0.15818101  0.15839307
  0.1588211   0.1582191   0.15801409  0.1582991   0.15853162  0.1583279
  0.157947    0.15772836  0.15757631  0.15758191  0.15769127  0.1575535
  0.15730336  0.15700057  0.1567692   0.1563393   0.1558269   0.1549266
  0.15313555  0.15185872  0.15089048  0.15020058  0.14946838  0.14896458
  0.14920537  0.14868492  0.14866418  0.14876656  0.14876314  0.14852159
  0.14844495  0.14846045  0.1479644   0.14781852  0.14803049  0.14763168
  0.14690454  0.1464467   0.14627348  0.14547125  0.14421195  0.14285322
  0.140774    0.13907342  0.13771547  0.13702397  0.13615823  0.13563341
  0.13572294  0.13535728  0.13509521  0.1349729   0.13496526  0.13449961
  0.13404259  0.13378522  0.13318892  0.1325238   0.13228469  0.13188939
  0.13105889  0.13026723  0.1300144   0.12969716  0.12863952  0.12680425
  0.1236797   0.12126063  0.1196601   0.11898629  0.11835358  0.11775438
  0.11788877  0.11752903  0.11741326  0.11753285  0.11776044  0.11733767
  0.11681384  0.11693842  0.11691129  0.11666027  0.11657007  0.1165712
  0.11638356  0.1159584   0.11594872  0.1156334   0.11455907  0.11279901
  0.11026145  0.10857297  0.10733742  0.10678183  0.10644338  0.10635561
  0.10684195  0.10707414  0.10751595  0.10769762  0.10795363  0.10782054
  0.10762085  0.10755189  0.10742083  0.10753991  0.10768841  0.10779475
  0.10774123  0.10763735  0.10779279  0.10808285  0.10770922  0.10694379
  0.10577441  0.10513636  0.10455285  0.10472312  0.10548152  0.1060859
  0.10727887  0.10843039  0.10927983  0.10951173  0.10995485  0.11042071
  0.11049925  0.11033486  0.1106103   0.11106201  0.11111886  0.11130164
  0.11161096  0.11161172  0.11142861  0.11135369  0.11115483  0.10994622
  0.10809057  0.10689406  0.10572945  0.10520381  0.10522329  0.10606157
  0.10747863  0.10897064  0.11029299  0.11084494  0.11110207  0.11140965
  0.11178841  0.11188021  0.11215667  0.11288927  0.113463    0.11369308
  0.11403906  0.11441866  0.11456801  0.11466531  0.11463674  0.11418306
  0.11286927  0.11215889  0.11152854  0.1116138   0.11221337  0.11321684
  0.11481913  0.11554456  0.1165336   0.11728635  0.11792596  0.11811946
  0.11829684  0.11878352  0.11891043  0.11918025  0.1194718   0.11992292
  0.12016713  0.12025293  0.12027821  0.12019648  0.12006254  0.11981245
  0.1187666   0.11824254  0.11781456  0.11805322  0.11866765  0.12001513
  0.12181003  0.12262825  0.12327383  0.12359717  0.12420731  0.12430694
  0.12437209  0.12442948  0.12460911  0.12465529  0.1246385   0.12472314
  0.12486287  0.1245835   0.1241613   0.12397186  0.12394515  0.12356466
  0.12270386  0.12220908  0.12168573  0.12139262  0.12155195  0.12202122
  0.12271316  0.12318778  0.12385198  0.12424047  0.12462188  0.12487886
  0.12481616  0.12457198  0.12457471  0.12509511  0.12540327  0.12538418
  0.12528579  0.12505005  0.12480536  0.12436048  0.12406351  0.12374943
  0.12251019  0.12184433  0.12176068  0.12195136  0.12197683  0.1224766
  0.12365812  0.12435699  0.12513642  0.1255973   0.12597956  0.12630676
  0.12677743  0.12705088  0.12713537  0.12749352  0.1278805   0.12795395
  0.12812692  0.12857184  0.12867463  0.12834743  0.1282776   0.12855314
  0.12758683  0.12681732  0.1266663   0.12707862  0.12796044  0.12934004
  0.13156305  0.13268927  0.13309653  0.13342541  0.13403909  0.13464211
  0.13504906  0.13528663  0.13525774  0.1353757   0.13589227  0.13624132
  0.13641462  0.1367754   0.13720389  0.1373622   0.13724579  0.1373714
  0.137193    0.1370971   0.1370317   0.13746189  0.13830568  0.13991018
  0.14264993  0.14474697  0.1458636   0.14653763  0.14740826  0.14817731
  0.14896198  0.14991195  0.15059125  0.15102181  0.15163739  0.15239975
  0.15276125  0.15312007  0.15374684  0.15408655  0.15413044  0.15424874
  0.15432668  0.1546228   0.15512992  0.15591109  0.15696214  0.15840074
  0.16070946  0.16238226  0.1634607   0.1641794   0.16492209  0.16533217
  0.16569173  0.16608056  0.16634774  0.1665619   0.16679785  0.1668534
  0.1667548   0.16683033  0.16713627  0.16710195  0.16678539  0.16673008
  0.1663435   0.16609702  0.16597944  0.16644447  0.16702633  0.16760296
  0.16871268  0.16973783  0.17034216  0.1706152   0.17081487  0.17081515
  0.17070352  0.17079675  0.17093776  0.1707625   0.17049411  0.17048478
  0.17049484  0.17039898  0.1704005   0.17046998  0.17020845  0.1696545
  0.16910373  0.16875216  0.16832204  0.16815053  0.16830213  0.16860382
  0.16949499  0.16988188  0.17019136  0.17019641  0.17008905  0.17000736
  0.17016548  0.17036049  0.17023897  0.17006485  0.16993478  0.16991276
  0.16980717  0.16964623  0.16947562  0.16927491  0.1690661   0.16877651
  0.16819362  0.16767703  0.16698533  0.16676733  0.16673346  0.16672394
  0.16702186  0.16718218  0.16740939  0.16752349  0.16746816  0.1671193
  0.16691378  0.16671136  0.16629076  0.16612327  0.16619398  0.16601071
  0.16552806  0.16509768  0.16487013  0.16443296  0.16364247  0.1626387
  0.16117415  0.15996978  0.15900004  0.15808064  0.1571922   0.15641268
  0.15626158  0.15583625  0.15537043  0.1550125   0.15484495  0.15438327
  0.15394558  0.1539307   0.15379475  0.15333281  0.15287235  0.15259096
  0.15209532  0.15148583  0.15122434  0.15092385  0.14999212  0.14821199
  0.14584008  0.14379102  0.14211647  0.14087135  0.13995002  0.13946685
  0.13933511  0.13911356  0.13894624  0.13892922  0.13888834  0.13829722
  0.1377357   0.13767453  0.137518    0.13694245  0.13649817  0.13628218
  0.1356225   0.1345944   0.13385558  0.13344999  0.13230091  0.13023613
  0.1276008   0.12543832  0.12333094  0.12118026  0.11927824  0.11760665
  0.11690903  0.1161904   0.1159943   0.11595416  0.11593271  0.11548126
  0.1149696   0.11485326  0.11428517  0.11337357  0.1126328   0.11218089
  0.11127684  0.11018971  0.10929595  0.10827052  0.10658854  0.10414948
  0.10093531  0.09855796  0.09598012  0.09406633  0.0928048   0.09207652
  0.09205078  0.09152754  0.09136549  0.09128428  0.09148867  0.09105615
  0.09033775  0.09031562  0.09024795  0.08940286  0.08855954  0.08803579
  0.08718171  0.08585404  0.08472066  0.08360036  0.08184578  0.07940236
  0.07632671  0.07395629  0.07190046  0.07008377  0.06859266  0.06739471
  0.06723751  0.06732214  0.06741665  0.0671651   0.06706265  0.06682549
  0.06631146  0.06614593  0.06611346  0.06562099  0.06484862  0.06444582
  0.06435315  0.06391373  0.06308549  0.06190885  0.06018384  0.05801373
  0.05535496  0.05338611  0.05148719  0.04991174  0.04838831  0.04722707
  0.04704046  0.04705385  0.047128    0.04698949  0.04741574  0.04779096
  0.04771563  0.04764847  0.04769092  0.047551    0.04692224  0.04636055
  0.04606336  0.04565269  0.04515271  0.04448609  0.04280939  0.04069027
  0.03805235  0.03603029  0.03403309  0.03257112  0.03134041  0.03027183
  0.03011512  0.02994162  0.0300819   0.03000739  0.03026585  0.03052136
  0.03006578  0.02953059  0.02933021  0.02890516  0.0277623   0.02669511
  0.0259186   0.02494798  0.02436223  0.02367095  0.02212508  0.01978713
  0.01731589  0.01513982  0.01274066  0.01111714  0.00984397  0.00830125
  0.00759319  0.00789324  0.0081393   0.00760423  0.00797021  0.00857174
  0.00806406  0.00754191  0.008014    0.00833162  0.00748026  0.00663902
  0.00629814  0.00542741  0.00484982  0.00498754  0.00381989  0.00110204
 -0.00151483 -0.00324004 -0.00556709 -0.00805936 -0.00981633 -0.01107338
 -0.01154837 -0.01108475 -0.00981411 -0.00965618 -0.00943126 -0.00845235
 -0.00831283 -0.00925567 -0.00937334 -0.00879211 -0.00935096 -0.01061857
 -0.01111984 -0.01171679 -0.01225512 -0.01261873 -0.01408271 -0.01629361
 -0.01934916 -0.02169809 -0.0234022  -0.02427624 -0.02580255 -0.0274415
 -0.02675266 -0.02494828 -0.02329852 -0.02345102 -0.02232215 -0.02012649
 -0.01993637 -0.02202346 -0.022444   -0.02131176 -0.02290526 -0.02635199
 -0.02734818 -0.02586822 -0.02598054 -0.02987536 -0.02791988 -0.01566527]
