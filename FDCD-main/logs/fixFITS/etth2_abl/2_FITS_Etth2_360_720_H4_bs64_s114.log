Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_360_720_FITS_ETTh2_ftM_sl360_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=74, out_features=222, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14719488.0
params:  16650.0
Trainable parameters:  16650
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.9243948459625244
Epoch: 1, Steps: 59 | Train Loss: 0.9442099 Vali Loss: 0.8175176 Test Loss: 0.5222819
Validation loss decreased (inf --> 0.817518).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.1155550479888916
Epoch: 2, Steps: 59 | Train Loss: 0.7793638 Vali Loss: 0.7575517 Test Loss: 0.4734703
Validation loss decreased (0.817518 --> 0.757552).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.8611788749694824
Epoch: 3, Steps: 59 | Train Loss: 0.6952057 Vali Loss: 0.7277421 Test Loss: 0.4483064
Validation loss decreased (0.757552 --> 0.727742).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.6268856525421143
Epoch: 4, Steps: 59 | Train Loss: 0.6518065 Vali Loss: 0.7096291 Test Loss: 0.4350923
Validation loss decreased (0.727742 --> 0.709629).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.3567075729370117
Epoch: 5, Steps: 59 | Train Loss: 0.6257232 Vali Loss: 0.6977810 Test Loss: 0.4274393
Validation loss decreased (0.709629 --> 0.697781).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.20505690574646
Epoch: 6, Steps: 59 | Train Loss: 0.6101545 Vali Loss: 0.6916792 Test Loss: 0.4226568
Validation loss decreased (0.697781 --> 0.691679).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.9636385440826416
Epoch: 7, Steps: 59 | Train Loss: 0.5996936 Vali Loss: 0.6876813 Test Loss: 0.4192196
Validation loss decreased (0.691679 --> 0.687681).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.1450605392456055
Epoch: 8, Steps: 59 | Train Loss: 0.5916368 Vali Loss: 0.6756411 Test Loss: 0.4166112
Validation loss decreased (0.687681 --> 0.675641).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.5222322940826416
Epoch: 9, Steps: 59 | Train Loss: 0.5859404 Vali Loss: 0.6747283 Test Loss: 0.4144925
Validation loss decreased (0.675641 --> 0.674728).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.328393936157227
Epoch: 10, Steps: 59 | Train Loss: 0.5810941 Vali Loss: 0.6739222 Test Loss: 0.4126542
Validation loss decreased (0.674728 --> 0.673922).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.549198865890503
Epoch: 11, Steps: 59 | Train Loss: 0.5773979 Vali Loss: 0.6710500 Test Loss: 0.4110696
Validation loss decreased (0.673922 --> 0.671050).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.2357301712036133
Epoch: 12, Steps: 59 | Train Loss: 0.5736123 Vali Loss: 0.6654762 Test Loss: 0.4096271
Validation loss decreased (0.671050 --> 0.665476).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.73146915435791
Epoch: 13, Steps: 59 | Train Loss: 0.5712719 Vali Loss: 0.6684014 Test Loss: 0.4083369
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.154059886932373
Epoch: 14, Steps: 59 | Train Loss: 0.5688876 Vali Loss: 0.6682127 Test Loss: 0.4071919
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.5900611877441406
Epoch: 15, Steps: 59 | Train Loss: 0.5667153 Vali Loss: 0.6599640 Test Loss: 0.4061663
Validation loss decreased (0.665476 --> 0.659964).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.4827287197113037
Epoch: 16, Steps: 59 | Train Loss: 0.5649360 Vali Loss: 0.6659784 Test Loss: 0.4052570
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.800912857055664
Epoch: 17, Steps: 59 | Train Loss: 0.5636440 Vali Loss: 0.6623671 Test Loss: 0.4043720
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.1077768802642822
Epoch: 18, Steps: 59 | Train Loss: 0.5621255 Vali Loss: 0.6565998 Test Loss: 0.4035612
Validation loss decreased (0.659964 --> 0.656600).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.45729660987854
Epoch: 19, Steps: 59 | Train Loss: 0.5608782 Vali Loss: 0.6596498 Test Loss: 0.4028324
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.2602827548980713
Epoch: 20, Steps: 59 | Train Loss: 0.5599910 Vali Loss: 0.6519580 Test Loss: 0.4021943
Validation loss decreased (0.656600 --> 0.651958).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.555640697479248
Epoch: 21, Steps: 59 | Train Loss: 0.5587479 Vali Loss: 0.6573603 Test Loss: 0.4015511
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.3363258838653564
Epoch: 22, Steps: 59 | Train Loss: 0.5577161 Vali Loss: 0.6566404 Test Loss: 0.4010110
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.3037965297698975
Epoch: 23, Steps: 59 | Train Loss: 0.5567970 Vali Loss: 0.6536583 Test Loss: 0.4004812
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.739481449127197
Epoch: 24, Steps: 59 | Train Loss: 0.5561250 Vali Loss: 0.6540544 Test Loss: 0.4000182
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.53183650970459
Epoch: 25, Steps: 59 | Train Loss: 0.5557186 Vali Loss: 0.6526536 Test Loss: 0.3996021
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.5620296001434326
Epoch: 26, Steps: 59 | Train Loss: 0.5553760 Vali Loss: 0.6519656 Test Loss: 0.3991840
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.025669813156128
Epoch: 27, Steps: 59 | Train Loss: 0.5547717 Vali Loss: 0.6497350 Test Loss: 0.3988054
Validation loss decreased (0.651958 --> 0.649735).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.075753927230835
Epoch: 28, Steps: 59 | Train Loss: 0.5539388 Vali Loss: 0.6510947 Test Loss: 0.3984386
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.8685226440429688
Epoch: 29, Steps: 59 | Train Loss: 0.5535148 Vali Loss: 0.6495393 Test Loss: 0.3981265
Validation loss decreased (0.649735 --> 0.649539).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.585352659225464
Epoch: 30, Steps: 59 | Train Loss: 0.5532552 Vali Loss: 0.6554200 Test Loss: 0.3978338
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.5544068813323975
Epoch: 31, Steps: 59 | Train Loss: 0.5527325 Vali Loss: 0.6537636 Test Loss: 0.3975353
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.7513926029205322
Epoch: 32, Steps: 59 | Train Loss: 0.5522938 Vali Loss: 0.6537722 Test Loss: 0.3972917
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.1473302841186523
Epoch: 33, Steps: 59 | Train Loss: 0.5520928 Vali Loss: 0.6517920 Test Loss: 0.3970394
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.5230014324188232
Epoch: 34, Steps: 59 | Train Loss: 0.5514015 Vali Loss: 0.6516469 Test Loss: 0.3968103
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.5877938270568848
Epoch: 35, Steps: 59 | Train Loss: 0.5513825 Vali Loss: 0.6507438 Test Loss: 0.3966060
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 4.494910478591919
Epoch: 36, Steps: 59 | Train Loss: 0.5513024 Vali Loss: 0.6505203 Test Loss: 0.3963955
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.5496928691864014
Epoch: 37, Steps: 59 | Train Loss: 0.5507851 Vali Loss: 0.6531646 Test Loss: 0.3962142
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.626472234725952
Epoch: 38, Steps: 59 | Train Loss: 0.5506453 Vali Loss: 0.6495718 Test Loss: 0.3960401
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.6127593517303467
Epoch: 39, Steps: 59 | Train Loss: 0.5505366 Vali Loss: 0.6512997 Test Loss: 0.3958667
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.5981829166412354
Epoch: 40, Steps: 59 | Train Loss: 0.5502665 Vali Loss: 0.6497466 Test Loss: 0.3957328
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.154433250427246
Epoch: 41, Steps: 59 | Train Loss: 0.5503696 Vali Loss: 0.6487471 Test Loss: 0.3955742
Validation loss decreased (0.649539 --> 0.648747).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.4396796226501465
Epoch: 42, Steps: 59 | Train Loss: 0.5495209 Vali Loss: 0.6483601 Test Loss: 0.3954415
Validation loss decreased (0.648747 --> 0.648360).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.2683143615722656
Epoch: 43, Steps: 59 | Train Loss: 0.5498992 Vali Loss: 0.6489304 Test Loss: 0.3953085
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.913055181503296
Epoch: 44, Steps: 59 | Train Loss: 0.5492306 Vali Loss: 0.6526200 Test Loss: 0.3951876
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 3.3481101989746094
Epoch: 45, Steps: 59 | Train Loss: 0.5494056 Vali Loss: 0.6518884 Test Loss: 0.3950816
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.6025173664093018
Epoch: 46, Steps: 59 | Train Loss: 0.5493665 Vali Loss: 0.6484421 Test Loss: 0.3949673
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.2069895267486572
Epoch: 47, Steps: 59 | Train Loss: 0.5486669 Vali Loss: 0.6501817 Test Loss: 0.3948787
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.358381748199463
Epoch: 48, Steps: 59 | Train Loss: 0.5486523 Vali Loss: 0.6498722 Test Loss: 0.3947703
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.0368268489837646
Epoch: 49, Steps: 59 | Train Loss: 0.5487424 Vali Loss: 0.6489881 Test Loss: 0.3946846
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 3.316769599914551
Epoch: 50, Steps: 59 | Train Loss: 0.5484418 Vali Loss: 0.6486838 Test Loss: 0.3946100
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 3.841702461242676
Epoch: 51, Steps: 59 | Train Loss: 0.5486485 Vali Loss: 0.6498944 Test Loss: 0.3945257
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.2635881900787354
Epoch: 52, Steps: 59 | Train Loss: 0.5484934 Vali Loss: 0.6507466 Test Loss: 0.3944501
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 3.6855056285858154
Epoch: 53, Steps: 59 | Train Loss: 0.5485395 Vali Loss: 0.6483225 Test Loss: 0.3943782
Validation loss decreased (0.648360 --> 0.648322).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 3.951323986053467
Epoch: 54, Steps: 59 | Train Loss: 0.5483084 Vali Loss: 0.6476325 Test Loss: 0.3943100
Validation loss decreased (0.648322 --> 0.647632).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 3.2083816528320312
Epoch: 55, Steps: 59 | Train Loss: 0.5483710 Vali Loss: 0.6422666 Test Loss: 0.3942415
Validation loss decreased (0.647632 --> 0.642267).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.6364009380340576
Epoch: 56, Steps: 59 | Train Loss: 0.5482622 Vali Loss: 0.6483678 Test Loss: 0.3941861
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.24928879737854
Epoch: 57, Steps: 59 | Train Loss: 0.5484498 Vali Loss: 0.6472862 Test Loss: 0.3941291
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.6453471183776855
Epoch: 58, Steps: 59 | Train Loss: 0.5474583 Vali Loss: 0.6486195 Test Loss: 0.3940716
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 3.040776252746582
Epoch: 59, Steps: 59 | Train Loss: 0.5475276 Vali Loss: 0.6454841 Test Loss: 0.3940194
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.461934804916382
Epoch: 60, Steps: 59 | Train Loss: 0.5480939 Vali Loss: 0.6478139 Test Loss: 0.3939701
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.3189585208892822
Epoch: 61, Steps: 59 | Train Loss: 0.5477021 Vali Loss: 0.6483569 Test Loss: 0.3939308
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 3.0772902965545654
Epoch: 62, Steps: 59 | Train Loss: 0.5476400 Vali Loss: 0.6499427 Test Loss: 0.3938814
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 3.338268756866455
Epoch: 63, Steps: 59 | Train Loss: 0.5474980 Vali Loss: 0.6450887 Test Loss: 0.3938419
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.661402940750122
Epoch: 64, Steps: 59 | Train Loss: 0.5475182 Vali Loss: 0.6463079 Test Loss: 0.3938053
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 3.044987916946411
Epoch: 65, Steps: 59 | Train Loss: 0.5471046 Vali Loss: 0.6513658 Test Loss: 0.3937720
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.5892105102539062
Epoch: 66, Steps: 59 | Train Loss: 0.5478505 Vali Loss: 0.6438239 Test Loss: 0.3937326
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.8677921295166016
Epoch: 67, Steps: 59 | Train Loss: 0.5474901 Vali Loss: 0.6473191 Test Loss: 0.3936946
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.6211798191070557
Epoch: 68, Steps: 59 | Train Loss: 0.5478461 Vali Loss: 0.6475909 Test Loss: 0.3936656
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.1149139404296875
Epoch: 69, Steps: 59 | Train Loss: 0.5477485 Vali Loss: 0.6470911 Test Loss: 0.3936355
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 3.389819860458374
Epoch: 70, Steps: 59 | Train Loss: 0.5473888 Vali Loss: 0.6486756 Test Loss: 0.3936073
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 4.042716979980469
Epoch: 71, Steps: 59 | Train Loss: 0.5473612 Vali Loss: 0.6469883 Test Loss: 0.3935826
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.2799875736236572
Epoch: 72, Steps: 59 | Train Loss: 0.5475145 Vali Loss: 0.6466643 Test Loss: 0.3935587
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.3890128135681152
Epoch: 73, Steps: 59 | Train Loss: 0.5471976 Vali Loss: 0.6475412 Test Loss: 0.3935327
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.4920928478240967
Epoch: 74, Steps: 59 | Train Loss: 0.5471607 Vali Loss: 0.6503221 Test Loss: 0.3935115
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 3.0014524459838867
Epoch: 75, Steps: 59 | Train Loss: 0.5473815 Vali Loss: 0.6489677 Test Loss: 0.3934887
EarlyStopping counter: 20 out of 20
Early stopping
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=74, out_features=222, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14719488.0
params:  16650.0
Trainable parameters:  16650
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.750161647796631
Epoch: 1, Steps: 59 | Train Loss: 0.8042471 Vali Loss: 0.6443115 Test Loss: 0.3911992
Validation loss decreased (inf --> 0.644312).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.8843250274658203
Epoch: 2, Steps: 59 | Train Loss: 0.8013514 Vali Loss: 0.6432241 Test Loss: 0.3895089
Validation loss decreased (0.644312 --> 0.643224).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.4731011390686035
Epoch: 3, Steps: 59 | Train Loss: 0.7998563 Vali Loss: 0.6422737 Test Loss: 0.3885373
Validation loss decreased (0.643224 --> 0.642274).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.37255859375
Epoch: 4, Steps: 59 | Train Loss: 0.7983270 Vali Loss: 0.6413842 Test Loss: 0.3879395
Validation loss decreased (0.642274 --> 0.641384).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.280916929244995
Epoch: 5, Steps: 59 | Train Loss: 0.7978931 Vali Loss: 0.6373011 Test Loss: 0.3875128
Validation loss decreased (0.641384 --> 0.637301).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.5921266078948975
Epoch: 6, Steps: 59 | Train Loss: 0.7967548 Vali Loss: 0.6400433 Test Loss: 0.3871929
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.6769444942474365
Epoch: 7, Steps: 59 | Train Loss: 0.7966795 Vali Loss: 0.6397511 Test Loss: 0.3870941
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.387317657470703
Epoch: 8, Steps: 59 | Train Loss: 0.7965874 Vali Loss: 0.6391587 Test Loss: 0.3869855
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.3399860858917236
Epoch: 9, Steps: 59 | Train Loss: 0.7958663 Vali Loss: 0.6389065 Test Loss: 0.3868308
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.4255940914154053
Epoch: 10, Steps: 59 | Train Loss: 0.7960345 Vali Loss: 0.6394013 Test Loss: 0.3867748
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.9053006172180176
Epoch: 11, Steps: 59 | Train Loss: 0.7952859 Vali Loss: 0.6379504 Test Loss: 0.3867295
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.844468593597412
Epoch: 12, Steps: 59 | Train Loss: 0.7953294 Vali Loss: 0.6356922 Test Loss: 0.3868028
Validation loss decreased (0.637301 --> 0.635692).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.9488954544067383
Epoch: 13, Steps: 59 | Train Loss: 0.7958336 Vali Loss: 0.6376398 Test Loss: 0.3867475
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 5.757559061050415
Epoch: 14, Steps: 59 | Train Loss: 0.7947410 Vali Loss: 0.6365288 Test Loss: 0.3866661
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.9512269496917725
Epoch: 15, Steps: 59 | Train Loss: 0.7956729 Vali Loss: 0.6329399 Test Loss: 0.3867399
Validation loss decreased (0.635692 --> 0.632940).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.5960464477539062
Epoch: 16, Steps: 59 | Train Loss: 0.7955889 Vali Loss: 0.6349140 Test Loss: 0.3867317
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.9384284019470215
Epoch: 17, Steps: 59 | Train Loss: 0.7953974 Vali Loss: 0.6349183 Test Loss: 0.3867519
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 5.527835130691528
Epoch: 18, Steps: 59 | Train Loss: 0.7952348 Vali Loss: 0.6365918 Test Loss: 0.3867222
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.606563091278076
Epoch: 19, Steps: 59 | Train Loss: 0.7956102 Vali Loss: 0.6343558 Test Loss: 0.3866806
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.4823052883148193
Epoch: 20, Steps: 59 | Train Loss: 0.7951035 Vali Loss: 0.6370496 Test Loss: 0.3866036
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.2910420894622803
Epoch: 21, Steps: 59 | Train Loss: 0.7951588 Vali Loss: 0.6399968 Test Loss: 0.3866566
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.459930896759033
Epoch: 22, Steps: 59 | Train Loss: 0.7954316 Vali Loss: 0.6357294 Test Loss: 0.3866701
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.0839486122131348
Epoch: 23, Steps: 59 | Train Loss: 0.7945721 Vali Loss: 0.6378807 Test Loss: 0.3866241
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.379889726638794
Epoch: 24, Steps: 59 | Train Loss: 0.7954535 Vali Loss: 0.6336733 Test Loss: 0.3866469
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.8878026008605957
Epoch: 25, Steps: 59 | Train Loss: 0.7942725 Vali Loss: 0.6377141 Test Loss: 0.3867110
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.1480367183685303
Epoch: 26, Steps: 59 | Train Loss: 0.7945096 Vali Loss: 0.6351455 Test Loss: 0.3866308
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.304793357849121
Epoch: 27, Steps: 59 | Train Loss: 0.7948726 Vali Loss: 0.6377361 Test Loss: 0.3866231
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.4629695415496826
Epoch: 28, Steps: 59 | Train Loss: 0.7942207 Vali Loss: 0.6385741 Test Loss: 0.3866439
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 4.07173228263855
Epoch: 29, Steps: 59 | Train Loss: 0.7938167 Vali Loss: 0.6368027 Test Loss: 0.3866425
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.4686684608459473
Epoch: 30, Steps: 59 | Train Loss: 0.7949751 Vali Loss: 0.6338520 Test Loss: 0.3866260
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.591592788696289
Epoch: 31, Steps: 59 | Train Loss: 0.7952006 Vali Loss: 0.6363064 Test Loss: 0.3866220
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.21563720703125
Epoch: 32, Steps: 59 | Train Loss: 0.7939448 Vali Loss: 0.6376420 Test Loss: 0.3866624
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 4.009385347366333
Epoch: 33, Steps: 59 | Train Loss: 0.7946867 Vali Loss: 0.6340907 Test Loss: 0.3866403
EarlyStopping counter: 18 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.15049409866333
Epoch: 34, Steps: 59 | Train Loss: 0.7945725 Vali Loss: 0.6341149 Test Loss: 0.3866296
EarlyStopping counter: 19 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.734154224395752
Epoch: 35, Steps: 59 | Train Loss: 0.7945990 Vali Loss: 0.6365451 Test Loss: 0.3866155
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_360_720_FITS_ETTh2_ftM_sl360_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.3848279118537903, mae:0.42258039116859436, rse:0.49583709239959717, corr:[ 2.20187664e-01  2.22204551e-01  2.20454901e-01  2.18766913e-01
  2.18347445e-01  2.18267143e-01  2.17408806e-01  2.16010273e-01
  2.15082392e-01  2.14296296e-01  2.13324577e-01  2.11852044e-01
  2.10406005e-01  2.09327593e-01  2.08575681e-01  2.08084375e-01
  2.07528666e-01  2.06816688e-01  2.05827519e-01  2.04789385e-01
  2.03996405e-01  2.03434005e-01  2.02654585e-01  2.01220274e-01
  1.99389249e-01  1.97736099e-01  1.96619809e-01  1.95790127e-01
  1.95071653e-01  1.94243148e-01  1.93345487e-01  1.92393616e-01
  1.91332296e-01  1.90245092e-01  1.89283714e-01  1.88436940e-01
  1.87467948e-01  1.86361864e-01  1.85387090e-01  1.84650898e-01
  1.84072316e-01  1.83532462e-01  1.82847723e-01  1.82078019e-01
  1.81426138e-01  1.80823818e-01  1.79949477e-01  1.78444728e-01
  1.76744014e-01  1.75424770e-01  1.74387753e-01  1.73541978e-01
  1.72609597e-01  1.71656743e-01  1.70708477e-01  1.69849411e-01
  1.69251978e-01  1.68752491e-01  1.68200940e-01  1.67781085e-01
  1.67568848e-01  1.67544648e-01  1.67851686e-01  1.68194324e-01
  1.68243021e-01  1.67998999e-01  1.67689830e-01  1.67660445e-01
  1.67807475e-01  1.67961136e-01  1.67776063e-01  1.67289943e-01
  1.66709691e-01  1.66315272e-01  1.66095972e-01  1.65768713e-01
  1.65438786e-01  1.65224418e-01  1.65049240e-01  1.64807469e-01
  1.64591640e-01  1.64454654e-01  1.64469168e-01  1.64533481e-01
  1.64637640e-01  1.64723948e-01  1.64854392e-01  1.65111303e-01
  1.65255755e-01  1.65210381e-01  1.64854929e-01  1.64439738e-01
  1.64462641e-01  1.64711028e-01  1.64864704e-01  1.64668530e-01
  1.64406642e-01  1.64277777e-01  1.64151341e-01  1.63863882e-01
  1.63497731e-01  1.63103029e-01  1.63023189e-01  1.62968248e-01
  1.62994057e-01  1.62917823e-01  1.62909359e-01  1.63172916e-01
  1.63284257e-01  1.63145542e-01  1.62784979e-01  1.62607417e-01
  1.62526280e-01  1.62396505e-01  1.62111655e-01  1.61693782e-01
  1.61515206e-01  1.61382630e-01  1.61125124e-01  1.60400659e-01
  1.59334481e-01  1.58388942e-01  1.57856941e-01  1.57601088e-01
  1.57091454e-01  1.56391010e-01  1.55802414e-01  1.55570805e-01
  1.55520171e-01  1.55101076e-01  1.54420242e-01  1.53681576e-01
  1.53138191e-01  1.52679294e-01  1.52157202e-01  1.51704967e-01
  1.51211113e-01  1.50780499e-01  1.50225535e-01  1.49868757e-01
  1.49686784e-01  1.49341807e-01  1.48583427e-01  1.47470772e-01
  1.45993188e-01  1.44802317e-01  1.44223407e-01  1.44221768e-01
  1.44125834e-01  1.43544704e-01  1.42924607e-01  1.42570421e-01
  1.42707869e-01  1.42813876e-01  1.42366946e-01  1.41438037e-01
  1.40670702e-01  1.40596107e-01  1.40685007e-01  1.40835941e-01
  1.40861720e-01  1.40853167e-01  1.40800610e-01  1.40667215e-01
  1.40648440e-01  1.40748143e-01  1.40941173e-01  1.40729651e-01
  1.40148014e-01  1.39336690e-01  1.38677046e-01  1.38541415e-01
  1.38622984e-01  1.38320595e-01  1.37378484e-01  1.36439666e-01
  1.35978624e-01  1.35887906e-01  1.35802224e-01  1.35534927e-01
  1.35371178e-01  1.35385722e-01  1.35381550e-01  1.35147706e-01
  1.34709477e-01  1.34345904e-01  1.34446546e-01  1.34952784e-01
  1.35514915e-01  1.36163965e-01  1.36676848e-01  1.36845872e-01
  1.36638284e-01  1.36191532e-01  1.36071414e-01  1.36662304e-01
  1.37593448e-01  1.38055712e-01  1.37738094e-01  1.36895269e-01
  1.36259839e-01  1.36395514e-01  1.37177885e-01  1.37575522e-01
  1.37351438e-01  1.36946008e-01  1.36862308e-01  1.37324050e-01
  1.37947947e-01  1.38349384e-01  1.38488665e-01  1.38622180e-01
  1.38843596e-01  1.38970181e-01  1.39051065e-01  1.38945326e-01
  1.38818100e-01  1.38590574e-01  1.38417155e-01  1.38450831e-01
  1.38604194e-01  1.38857052e-01  1.38996840e-01  1.39103517e-01
  1.39169902e-01  1.39219806e-01  1.39406785e-01  1.39533103e-01
  1.39619961e-01  1.39706269e-01  1.39793068e-01  1.39955655e-01
  1.40161276e-01  1.40652612e-01  1.41531497e-01  1.42577797e-01
  1.43151924e-01  1.43069237e-01  1.42967224e-01  1.43372655e-01
  1.44200519e-01  1.44907475e-01  1.45240188e-01  1.45147800e-01
  1.44987106e-01  1.44702524e-01  1.44458830e-01  1.44384995e-01
  1.44594431e-01  1.45085379e-01  1.45965338e-01  1.46636888e-01
  1.46799937e-01  1.46783918e-01  1.47027716e-01  1.48127586e-01
  1.49430320e-01  1.50216922e-01  1.50318891e-01  1.50283232e-01
  1.50725961e-01  1.51548177e-01  1.52492523e-01  1.52977541e-01
  1.53134838e-01  1.53254718e-01  1.53622225e-01  1.54261842e-01
  1.54733315e-01  1.55103922e-01  1.55273721e-01  1.55595765e-01
  1.56027153e-01  1.56597674e-01  1.57433704e-01  1.57922670e-01
  1.58179551e-01  1.58668578e-01  1.59369811e-01  1.60194963e-01
  1.60817996e-01  1.61214694e-01  1.61291391e-01  1.61507905e-01
  1.61881924e-01  1.62512571e-01  1.63288325e-01  1.63839504e-01
  1.63859576e-01  1.63680911e-01  1.63510650e-01  1.63603574e-01
  1.63558766e-01  1.63446039e-01  1.63257882e-01  1.63308382e-01
  1.63810924e-01  1.64343208e-01  1.64640158e-01  1.64631635e-01
  1.64314985e-01  1.64089069e-01  1.64345488e-01  1.65078536e-01
  1.65572330e-01  1.65700510e-01  1.65372044e-01  1.65419444e-01
  1.65972799e-01  1.66495651e-01  1.66788876e-01  1.66510552e-01
  1.66143566e-01  1.65925264e-01  1.66143239e-01  1.66457787e-01
  1.66386962e-01  1.66099757e-01  1.65844053e-01  1.65669337e-01
  1.65529042e-01  1.65381089e-01  1.65327579e-01  1.65224433e-01
  1.65019035e-01  1.64965957e-01  1.65505096e-01  1.66277856e-01
  1.66924551e-01  1.66999951e-01  1.66596577e-01  1.66619375e-01
  1.67086527e-01  1.67827278e-01  1.68590352e-01  1.68854862e-01
  1.68673649e-01  1.68149844e-01  1.67938471e-01  1.68101460e-01
  1.68580487e-01  1.68923989e-01  1.69100732e-01  1.69022709e-01
  1.68829665e-01  1.68841451e-01  1.68921530e-01  1.68931782e-01
  1.68819517e-01  1.68829575e-01  1.69342682e-01  1.70247078e-01
  1.70948848e-01  1.71211272e-01  1.71200335e-01  1.71413839e-01
  1.71796963e-01  1.72405601e-01  1.72803640e-01  1.72713295e-01
  1.72337770e-01  1.71918660e-01  1.71841815e-01  1.71991155e-01
  1.72287256e-01  1.72781795e-01  1.73348188e-01  1.73855036e-01
  1.73932135e-01  1.73887074e-01  1.73738196e-01  1.73666686e-01
  1.73552394e-01  1.73388198e-01  1.73276961e-01  1.73282951e-01
  1.73380941e-01  1.73485115e-01  1.73297301e-01  1.73045456e-01
  1.72932476e-01  1.73170447e-01  1.73602238e-01  1.73918471e-01
  1.73790336e-01  1.73207626e-01  1.72518462e-01  1.72152102e-01
  1.72278553e-01  1.72710165e-01  1.73245296e-01  1.73742235e-01
  1.74189538e-01  1.74678460e-01  1.75138637e-01  1.75301701e-01
  1.75213084e-01  1.74815968e-01  1.74554363e-01  1.74444214e-01
  1.74266979e-01  1.73943594e-01  1.73397765e-01  1.72838077e-01
  1.72386512e-01  1.72164530e-01  1.72174200e-01  1.72232822e-01
  1.72050834e-01  1.71723798e-01  1.71364784e-01  1.71095863e-01
  1.70944169e-01  1.70796692e-01  1.70540810e-01  1.70436203e-01
  1.70322299e-01  1.69992417e-01  1.69440284e-01  1.68784201e-01
  1.68131679e-01  1.67425632e-01  1.66589394e-01  1.65672228e-01
  1.64720878e-01  1.64012775e-01  1.63598910e-01  1.63287669e-01
  1.62772804e-01  1.62061900e-01  1.61151007e-01  1.60315648e-01
  1.59615412e-01  1.59017175e-01  1.58532336e-01  1.58082575e-01
  1.57685235e-01  1.57151237e-01  1.56483889e-01  1.55615121e-01
  1.54690161e-01  1.53834134e-01  1.53094307e-01  1.52567148e-01
  1.52303934e-01  1.52406156e-01  1.52263850e-01  1.51700571e-01
  1.50731817e-01  1.49877474e-01  1.49427280e-01  1.49352387e-01
  1.48962721e-01  1.48298085e-01  1.47874013e-01  1.48048788e-01
  1.48433998e-01  1.48358867e-01  1.47617847e-01  1.46863118e-01
  1.46531507e-01  1.46555334e-01  1.46493867e-01  1.46111414e-01
  1.45434484e-01  1.44823521e-01  1.44277051e-01  1.43443659e-01
  1.42504320e-01  1.42014012e-01  1.42307863e-01  1.42756715e-01
  1.42630592e-01  1.41830310e-01  1.41097173e-01  1.40989542e-01
  1.41120523e-01  1.40768453e-01  1.39609665e-01  1.38218641e-01
  1.37517765e-01  1.37594536e-01  1.37634069e-01  1.36794895e-01
  1.35536462e-01  1.34633124e-01  1.34346947e-01  1.34020314e-01
  1.33342609e-01  1.32532939e-01  1.32032230e-01  1.31538257e-01
  1.30932152e-01  1.29888207e-01  1.28721282e-01  1.27815068e-01
  1.27228484e-01  1.26805574e-01  1.26305431e-01  1.25585735e-01
  1.24912478e-01  1.24303430e-01  1.23590238e-01  1.22511372e-01
  1.21129103e-01  1.19870596e-01  1.18766829e-01  1.17747426e-01
  1.17035672e-01  1.16908535e-01  1.16801128e-01  1.16359785e-01
  1.15395904e-01  1.14242911e-01  1.13320701e-01  1.12509944e-01
  1.11758508e-01  1.10855319e-01  1.10073313e-01  1.09458223e-01
  1.08859472e-01  1.08242668e-01  1.07532926e-01  1.07106179e-01
  1.06924213e-01  1.06804952e-01  1.06092118e-01  1.04684576e-01
  1.03015594e-01  1.01713017e-01  1.00910269e-01  9.98085067e-02
  9.82323438e-02  9.64876637e-02  9.51531455e-02  9.42835510e-02
  9.37462598e-02  9.32588652e-02  9.26653817e-02  9.21121165e-02
  9.13746059e-02  9.03940648e-02  8.94211829e-02  8.85460526e-02
  8.75096321e-02  8.65983292e-02  8.54487121e-02  8.45472440e-02
  8.40477943e-02  8.35160315e-02  8.26601684e-02  8.11028183e-02
  7.92718902e-02  7.78823420e-02  7.69547373e-02  7.63570517e-02
  7.56752342e-02  7.48771206e-02  7.41503388e-02  7.34437555e-02
  7.27580339e-02  7.16561228e-02  7.05877393e-02  6.99367821e-02
  6.97035119e-02  6.96555376e-02  6.92745894e-02  6.84890002e-02
  6.79186508e-02  6.76222965e-02  6.70670196e-02  6.61288127e-02
  6.49479926e-02  6.39904067e-02  6.30846620e-02  6.20108247e-02
  6.06217720e-02  5.93049005e-02  5.80114387e-02  5.69488518e-02
  5.62790558e-02  5.58304936e-02  5.54033108e-02  5.47153205e-02
  5.38498163e-02  5.31256199e-02  5.25257103e-02  5.22273667e-02
  5.18372841e-02  5.12452684e-02  5.03364392e-02  4.94862124e-02
  4.89009134e-02  4.88432646e-02  4.91447784e-02  4.92366776e-02
  4.88517210e-02  4.81534228e-02  4.70777154e-02  4.57713157e-02
  4.46443968e-02  4.39257100e-02  4.33093719e-02  4.27052155e-02
  4.16848809e-02  4.06181663e-02  3.99321020e-02  3.94169055e-02
  3.86827402e-02  3.76881771e-02  3.66086923e-02  3.59880663e-02
  3.58983949e-02  3.60353254e-02  3.60165127e-02  3.56620960e-02
  3.53488885e-02  3.56651545e-02  3.63098271e-02  3.66441943e-02
  3.62626463e-02  3.56483012e-02  3.53385694e-02  3.52609642e-02
  3.49837914e-02  3.40533778e-02  3.27598415e-02  3.17454562e-02
  3.12881470e-02  3.12897302e-02  3.13019603e-02  3.09497397e-02
  3.05522773e-02  3.01940273e-02  3.02330963e-02  3.05456184e-02
  3.07387318e-02  3.04676164e-02  2.97975149e-02  2.93627344e-02
  2.88422275e-02  2.83664148e-02  2.78896652e-02  2.75156517e-02
  2.74287239e-02  2.73896921e-02  2.69688834e-02  2.58969925e-02
  2.45811008e-02  2.35321298e-02  2.27306914e-02  2.22290177e-02
  2.17802469e-02  2.08639260e-02  2.00262368e-02  1.94918513e-02
  1.92604326e-02  1.89686753e-02  1.87262464e-02  1.83114801e-02
  1.78568233e-02  1.76377036e-02  1.73287503e-02  1.70305707e-02
  1.67738963e-02  1.68039966e-02  1.68725979e-02  1.66193824e-02
  1.58197600e-02  1.53692467e-02  1.50860874e-02  1.48029337e-02
  1.43689727e-02  1.37395831e-02  1.31905638e-02  1.27119059e-02
  1.23470761e-02  1.16015160e-02  1.08808028e-02  1.05616851e-02
  1.07257813e-02  1.11280018e-02  1.08716357e-02  1.04404669e-02
  1.02996416e-02  1.06144892e-02  1.10589191e-02  1.08098993e-02
  1.02463337e-02  1.01247691e-02  1.06709218e-02  1.10154711e-02
  1.05300723e-02  9.72382817e-03  9.07744002e-03  8.79188906e-03
  7.83898961e-03  6.39053015e-03  4.79654549e-03  3.61286965e-03
  3.34993098e-03  3.48973414e-03  3.51105258e-03  2.81389500e-03
  2.13827938e-03  1.36300491e-03  3.13614408e-04 -2.59099688e-05
  2.50301353e-04  7.32024084e-04  2.86742608e-04 -8.31268844e-04
 -1.46719243e-03 -2.70827150e-04  5.29539888e-04 -9.93603258e-04
 -3.93211888e-03 -5.11648459e-03 -2.47004908e-03 -1.76608213e-03]
