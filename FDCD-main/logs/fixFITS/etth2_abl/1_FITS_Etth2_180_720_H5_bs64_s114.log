Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=50, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_180_720_FITS_ETTh2_ftM_sl180_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7741
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=50, out_features=250, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  11200000.0
params:  12750.0
Trainable parameters:  12750
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.6260855197906494
Epoch: 1, Steps: 60 | Train Loss: 1.1858776 Vali Loss: 0.7839293 Test Loss: 0.5620950
Validation loss decreased (inf --> 0.783929).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.523343563079834
Epoch: 2, Steps: 60 | Train Loss: 0.9916447 Vali Loss: 0.7111140 Test Loss: 0.4884625
Validation loss decreased (0.783929 --> 0.711114).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.1258327960968018
Epoch: 3, Steps: 60 | Train Loss: 0.9111178 Vali Loss: 0.6816661 Test Loss: 0.4536128
Validation loss decreased (0.711114 --> 0.681666).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.7705671787261963
Epoch: 4, Steps: 60 | Train Loss: 0.8713946 Vali Loss: 0.6594831 Test Loss: 0.4349389
Validation loss decreased (0.681666 --> 0.659483).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.2577810287475586
Epoch: 5, Steps: 60 | Train Loss: 0.8505823 Vali Loss: 0.6554852 Test Loss: 0.4251043
Validation loss decreased (0.659483 --> 0.655485).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.6942381858825684
Epoch: 6, Steps: 60 | Train Loss: 0.8387277 Vali Loss: 0.6504716 Test Loss: 0.4192893
Validation loss decreased (0.655485 --> 0.650472).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.0519309043884277
Epoch: 7, Steps: 60 | Train Loss: 0.8323128 Vali Loss: 0.6461413 Test Loss: 0.4158012
Validation loss decreased (0.650472 --> 0.646141).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.045778751373291
Epoch: 8, Steps: 60 | Train Loss: 0.8264869 Vali Loss: 0.6437411 Test Loss: 0.4135469
Validation loss decreased (0.646141 --> 0.643741).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.888224124908447
Epoch: 9, Steps: 60 | Train Loss: 0.8265743 Vali Loss: 0.6408489 Test Loss: 0.4120519
Validation loss decreased (0.643741 --> 0.640849).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.0769834518432617
Epoch: 10, Steps: 60 | Train Loss: 0.8239846 Vali Loss: 0.6322144 Test Loss: 0.4109835
Validation loss decreased (0.640849 --> 0.632214).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.6518912315368652
Epoch: 11, Steps: 60 | Train Loss: 0.8200578 Vali Loss: 0.6365396 Test Loss: 0.4101398
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.188755512237549
Epoch: 12, Steps: 60 | Train Loss: 0.8210494 Vali Loss: 0.6346983 Test Loss: 0.4094450
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.4966301918029785
Epoch: 13, Steps: 60 | Train Loss: 0.8202735 Vali Loss: 0.6340351 Test Loss: 0.4089555
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.7244162559509277
Epoch: 14, Steps: 60 | Train Loss: 0.8209767 Vali Loss: 0.6373663 Test Loss: 0.4085051
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.9931888580322266
Epoch: 15, Steps: 60 | Train Loss: 0.8185583 Vali Loss: 0.6385880 Test Loss: 0.4081598
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.728029727935791
Epoch: 16, Steps: 60 | Train Loss: 0.8168899 Vali Loss: 0.6299485 Test Loss: 0.4078565
Validation loss decreased (0.632214 --> 0.629948).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.591791868209839
Epoch: 17, Steps: 60 | Train Loss: 0.8173735 Vali Loss: 0.6333930 Test Loss: 0.4076229
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.528643846511841
Epoch: 18, Steps: 60 | Train Loss: 0.8175459 Vali Loss: 0.6337063 Test Loss: 0.4073716
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.6128714084625244
Epoch: 19, Steps: 60 | Train Loss: 0.8159418 Vali Loss: 0.6317440 Test Loss: 0.4071996
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.6202993392944336
Epoch: 20, Steps: 60 | Train Loss: 0.8151641 Vali Loss: 0.6327927 Test Loss: 0.4070391
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.6771090030670166
Epoch: 21, Steps: 60 | Train Loss: 0.8160672 Vali Loss: 0.6277941 Test Loss: 0.4069083
Validation loss decreased (0.629948 --> 0.627794).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.033586025238037
Epoch: 22, Steps: 60 | Train Loss: 0.8153694 Vali Loss: 0.6346175 Test Loss: 0.4067513
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.569218397140503
Epoch: 23, Steps: 60 | Train Loss: 0.8172469 Vali Loss: 0.6304672 Test Loss: 0.4066405
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.8049638271331787
Epoch: 24, Steps: 60 | Train Loss: 0.8149647 Vali Loss: 0.6294600 Test Loss: 0.4065557
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.0439860820770264
Epoch: 25, Steps: 60 | Train Loss: 0.8144921 Vali Loss: 0.6331652 Test Loss: 0.4064559
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.565877914428711
Epoch: 26, Steps: 60 | Train Loss: 0.8158640 Vali Loss: 0.6331095 Test Loss: 0.4063651
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.228651285171509
Epoch: 27, Steps: 60 | Train Loss: 0.8131596 Vali Loss: 0.6267392 Test Loss: 0.4063015
Validation loss decreased (0.627794 --> 0.626739).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.0136077404022217
Epoch: 28, Steps: 60 | Train Loss: 0.8128173 Vali Loss: 0.6306883 Test Loss: 0.4062512
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.3343758583068848
Epoch: 29, Steps: 60 | Train Loss: 0.8139729 Vali Loss: 0.6298691 Test Loss: 0.4061732
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.9919893741607666
Epoch: 30, Steps: 60 | Train Loss: 0.8132676 Vali Loss: 0.6306450 Test Loss: 0.4061138
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.600311517715454
Epoch: 31, Steps: 60 | Train Loss: 0.8110561 Vali Loss: 0.6327946 Test Loss: 0.4060337
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.1465706825256348
Epoch: 32, Steps: 60 | Train Loss: 0.8127715 Vali Loss: 0.6309627 Test Loss: 0.4060114
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.8698642253875732
Epoch: 33, Steps: 60 | Train Loss: 0.8129208 Vali Loss: 0.6277344 Test Loss: 0.4059539
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.9399633407592773
Epoch: 34, Steps: 60 | Train Loss: 0.8135067 Vali Loss: 0.6306868 Test Loss: 0.4059107
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.7775442600250244
Epoch: 35, Steps: 60 | Train Loss: 0.8123338 Vali Loss: 0.6316372 Test Loss: 0.4058877
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.7205843925476074
Epoch: 36, Steps: 60 | Train Loss: 0.8143803 Vali Loss: 0.6287516 Test Loss: 0.4058354
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.8276901245117188
Epoch: 37, Steps: 60 | Train Loss: 0.8134276 Vali Loss: 0.6285686 Test Loss: 0.4058242
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.6742894649505615
Epoch: 38, Steps: 60 | Train Loss: 0.8114523 Vali Loss: 0.6311265 Test Loss: 0.4057836
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.083404779434204
Epoch: 39, Steps: 60 | Train Loss: 0.8114389 Vali Loss: 0.6282621 Test Loss: 0.4057589
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.030371904373169
Epoch: 40, Steps: 60 | Train Loss: 0.8133983 Vali Loss: 0.6289877 Test Loss: 0.4057431
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.7692954540252686
Epoch: 41, Steps: 60 | Train Loss: 0.8111956 Vali Loss: 0.6269895 Test Loss: 0.4057197
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.8057568073272705
Epoch: 42, Steps: 60 | Train Loss: 0.8122171 Vali Loss: 0.6277798 Test Loss: 0.4056939
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.5283870697021484
Epoch: 43, Steps: 60 | Train Loss: 0.8145042 Vali Loss: 0.6259536 Test Loss: 0.4056730
Validation loss decreased (0.626739 --> 0.625954).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.1489224433898926
Epoch: 44, Steps: 60 | Train Loss: 0.8123162 Vali Loss: 0.6322703 Test Loss: 0.4056501
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 3.6602649688720703
Epoch: 45, Steps: 60 | Train Loss: 0.8109980 Vali Loss: 0.6326923 Test Loss: 0.4056333
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 4.063445568084717
Epoch: 46, Steps: 60 | Train Loss: 0.8134279 Vali Loss: 0.6313667 Test Loss: 0.4056241
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 3.3269848823547363
Epoch: 47, Steps: 60 | Train Loss: 0.8107196 Vali Loss: 0.6277927 Test Loss: 0.4056022
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 3.7156877517700195
Epoch: 48, Steps: 60 | Train Loss: 0.8128534 Vali Loss: 0.6293348 Test Loss: 0.4055995
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.8969566822052
Epoch: 49, Steps: 60 | Train Loss: 0.8133915 Vali Loss: 0.6310754 Test Loss: 0.4055761
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 6.279663801193237
Epoch: 50, Steps: 60 | Train Loss: 0.8120691 Vali Loss: 0.6307204 Test Loss: 0.4055634
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 3.22819185256958
Epoch: 51, Steps: 60 | Train Loss: 0.8110389 Vali Loss: 0.6256034 Test Loss: 0.4055514
Validation loss decreased (0.625954 --> 0.625603).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 3.6616270542144775
Epoch: 52, Steps: 60 | Train Loss: 0.8105787 Vali Loss: 0.6284336 Test Loss: 0.4055344
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.7386410236358643
Epoch: 53, Steps: 60 | Train Loss: 0.8132570 Vali Loss: 0.6289103 Test Loss: 0.4055280
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.989675760269165
Epoch: 54, Steps: 60 | Train Loss: 0.8123766 Vali Loss: 0.6277336 Test Loss: 0.4055189
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 3.455811023712158
Epoch: 55, Steps: 60 | Train Loss: 0.8116046 Vali Loss: 0.6270020 Test Loss: 0.4055059
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.467386484146118
Epoch: 56, Steps: 60 | Train Loss: 0.8129197 Vali Loss: 0.6278374 Test Loss: 0.4055015
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 3.325596332550049
Epoch: 57, Steps: 60 | Train Loss: 0.8138706 Vali Loss: 0.6284385 Test Loss: 0.4054871
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 3.9887142181396484
Epoch: 58, Steps: 60 | Train Loss: 0.8111351 Vali Loss: 0.6277499 Test Loss: 0.4054821
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 3.488079309463501
Epoch: 59, Steps: 60 | Train Loss: 0.8122085 Vali Loss: 0.6286286 Test Loss: 0.4054740
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.579775333404541
Epoch: 60, Steps: 60 | Train Loss: 0.8121963 Vali Loss: 0.6273234 Test Loss: 0.4054649
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.8756024837493896
Epoch: 61, Steps: 60 | Train Loss: 0.8110660 Vali Loss: 0.6290839 Test Loss: 0.4054606
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 3.2496440410614014
Epoch: 62, Steps: 60 | Train Loss: 0.8116311 Vali Loss: 0.6315091 Test Loss: 0.4054547
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.485464572906494
Epoch: 63, Steps: 60 | Train Loss: 0.8118515 Vali Loss: 0.6244489 Test Loss: 0.4054455
Validation loss decreased (0.625603 --> 0.624449).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.7321298122406006
Epoch: 64, Steps: 60 | Train Loss: 0.8118115 Vali Loss: 0.6277682 Test Loss: 0.4054403
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 3.204415798187256
Epoch: 65, Steps: 60 | Train Loss: 0.8106205 Vali Loss: 0.6277839 Test Loss: 0.4054343
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.446444511413574
Epoch: 66, Steps: 60 | Train Loss: 0.8127536 Vali Loss: 0.6247571 Test Loss: 0.4054302
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.4158096313476562
Epoch: 67, Steps: 60 | Train Loss: 0.8117924 Vali Loss: 0.6306410 Test Loss: 0.4054235
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.5678250789642334
Epoch: 68, Steps: 60 | Train Loss: 0.8125652 Vali Loss: 0.6318727 Test Loss: 0.4054222
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.5118913650512695
Epoch: 69, Steps: 60 | Train Loss: 0.8137926 Vali Loss: 0.6279250 Test Loss: 0.4054167
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.5662968158721924
Epoch: 70, Steps: 60 | Train Loss: 0.8113002 Vali Loss: 0.6272925 Test Loss: 0.4054126
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.725747585296631
Epoch: 71, Steps: 60 | Train Loss: 0.8103954 Vali Loss: 0.6267072 Test Loss: 0.4054076
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 3.1131322383880615
Epoch: 72, Steps: 60 | Train Loss: 0.8125685 Vali Loss: 0.6256235 Test Loss: 0.4054022
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 3.514509677886963
Epoch: 73, Steps: 60 | Train Loss: 0.8129552 Vali Loss: 0.6263019 Test Loss: 0.4053994
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 3.4713058471679688
Epoch: 74, Steps: 60 | Train Loss: 0.8099325 Vali Loss: 0.6279743 Test Loss: 0.4053974
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 3.2331464290618896
Epoch: 75, Steps: 60 | Train Loss: 0.8124480 Vali Loss: 0.6283945 Test Loss: 0.4053954
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 3.6694695949554443
Epoch: 76, Steps: 60 | Train Loss: 0.8100608 Vali Loss: 0.6288064 Test Loss: 0.4053907
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 3.436706304550171
Epoch: 77, Steps: 60 | Train Loss: 0.8124131 Vali Loss: 0.6308252 Test Loss: 0.4053876
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 3.105071783065796
Epoch: 78, Steps: 60 | Train Loss: 0.8119635 Vali Loss: 0.6210331 Test Loss: 0.4053857
Validation loss decreased (0.624449 --> 0.621033).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.4267053604125977
Epoch: 79, Steps: 60 | Train Loss: 0.8111044 Vali Loss: 0.6289717 Test Loss: 0.4053819
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.7403650283813477
Epoch: 80, Steps: 60 | Train Loss: 0.8128786 Vali Loss: 0.6240011 Test Loss: 0.4053805
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 3.0573630332946777
Epoch: 81, Steps: 60 | Train Loss: 0.8113274 Vali Loss: 0.6291317 Test Loss: 0.4053797
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.9658873081207275
Epoch: 82, Steps: 60 | Train Loss: 0.8109234 Vali Loss: 0.6219586 Test Loss: 0.4053767
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 2.227172613143921
Epoch: 83, Steps: 60 | Train Loss: 0.8103463 Vali Loss: 0.6263891 Test Loss: 0.4053735
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 3.0997960567474365
Epoch: 84, Steps: 60 | Train Loss: 0.8108340 Vali Loss: 0.6267827 Test Loss: 0.4053714
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 2.612628221511841
Epoch: 85, Steps: 60 | Train Loss: 0.8095262 Vali Loss: 0.6309912 Test Loss: 0.4053710
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 3.2750704288482666
Epoch: 86, Steps: 60 | Train Loss: 0.8116469 Vali Loss: 0.6271692 Test Loss: 0.4053687
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 3.2268428802490234
Epoch: 87, Steps: 60 | Train Loss: 0.8115949 Vali Loss: 0.6261203 Test Loss: 0.4053671
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 3.661614179611206
Epoch: 88, Steps: 60 | Train Loss: 0.8130536 Vali Loss: 0.6283172 Test Loss: 0.4053649
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 4.362631559371948
Epoch: 89, Steps: 60 | Train Loss: 0.8107866 Vali Loss: 0.6297822 Test Loss: 0.4053649
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 4.261733770370483
Epoch: 90, Steps: 60 | Train Loss: 0.8123874 Vali Loss: 0.6260256 Test Loss: 0.4053614
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 5.056859731674194
Epoch: 91, Steps: 60 | Train Loss: 0.8118432 Vali Loss: 0.6256489 Test Loss: 0.4053612
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 2.265383005142212
Epoch: 92, Steps: 60 | Train Loss: 0.8127804 Vali Loss: 0.6320610 Test Loss: 0.4053597
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 2.849093198776245
Epoch: 93, Steps: 60 | Train Loss: 0.8116985 Vali Loss: 0.6268001 Test Loss: 0.4053579
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 2.7775721549987793
Epoch: 94, Steps: 60 | Train Loss: 0.8113110 Vali Loss: 0.6261189 Test Loss: 0.4053570
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 4.066734313964844
Epoch: 95, Steps: 60 | Train Loss: 0.8096159 Vali Loss: 0.6271896 Test Loss: 0.4053557
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 3.0347259044647217
Epoch: 96, Steps: 60 | Train Loss: 0.8116966 Vali Loss: 0.6234916 Test Loss: 0.4053547
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 3.4449636936187744
Epoch: 97, Steps: 60 | Train Loss: 0.8131847 Vali Loss: 0.6269314 Test Loss: 0.4053537
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 3.9565558433532715
Epoch: 98, Steps: 60 | Train Loss: 0.8115925 Vali Loss: 0.6314228 Test Loss: 0.4053531
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_180_720_FITS_ETTh2_ftM_sl180_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4035618305206299, mae:0.4301442801952362, rse:0.5077626705169678, corr:[ 2.18924597e-01  2.21571624e-01  2.19362393e-01  2.19805241e-01
  2.19214946e-01  2.17201546e-01  2.16777295e-01  2.16472983e-01
  2.14941800e-01  2.13635594e-01  2.13187799e-01  2.11962029e-01
  2.10432768e-01  2.09685951e-01  2.09210768e-01  2.08410829e-01
  2.07839176e-01  2.07392514e-01  2.06523910e-01  2.05462769e-01
  2.04859257e-01  2.04470798e-01  2.03556776e-01  2.02051058e-01
  2.00278416e-01  1.98842049e-01  1.97604313e-01  1.96446985e-01
  1.95253313e-01  1.94194898e-01  1.92996711e-01  1.91631734e-01
  1.90444231e-01  1.89518422e-01  1.88355505e-01  1.87052011e-01
  1.86185569e-01  1.85385779e-01  1.84374064e-01  1.83416992e-01
  1.82905167e-01  1.82380080e-01  1.81783646e-01  1.81297585e-01
  1.80694714e-01  1.79847687e-01  1.78942129e-01  1.77311450e-01
  1.74782082e-01  1.72583774e-01  1.71245679e-01  1.70227930e-01
  1.68636262e-01  1.67469904e-01  1.66809842e-01  1.65936530e-01
  1.65065035e-01  1.64853424e-01  1.64975569e-01  1.64265022e-01
  1.63630456e-01  1.63485155e-01  1.63734376e-01  1.63412020e-01
  1.63294852e-01  1.63438052e-01  1.63259029e-01  1.62931979e-01
  1.63307115e-01  1.63690656e-01  1.63027555e-01  1.61939546e-01
  1.60991535e-01  1.60345972e-01  1.59510717e-01  1.58873141e-01
  1.58452138e-01  1.58172205e-01  1.57930121e-01  1.57987595e-01
  1.58072829e-01  1.57774195e-01  1.57817900e-01  1.58012420e-01
  1.57944471e-01  1.57573655e-01  1.57846078e-01  1.58023193e-01
  1.57559723e-01  1.57103196e-01  1.57401770e-01  1.57499209e-01
  1.57210305e-01  1.57188550e-01  1.57377914e-01  1.56877756e-01
  1.55630484e-01  1.55183852e-01  1.54868811e-01  1.54247031e-01
  1.53582841e-01  1.53293103e-01  1.53124258e-01  1.52820915e-01
  1.53023735e-01  1.53296456e-01  1.53286189e-01  1.53037265e-01
  1.53423354e-01  1.53558969e-01  1.53173342e-01  1.52777314e-01
  1.52936548e-01  1.52851954e-01  1.52357385e-01  1.52209163e-01
  1.52491763e-01  1.52222797e-01  1.51354566e-01  1.50457323e-01
  1.49248660e-01  1.47930682e-01  1.46890491e-01  1.46447167e-01
  1.45669207e-01  1.44597873e-01  1.44042104e-01  1.43861830e-01
  1.43395856e-01  1.42677292e-01  1.42507002e-01  1.42251790e-01
  1.41612172e-01  1.41109899e-01  1.41232118e-01  1.41118169e-01
  1.40481442e-01  1.39996767e-01  1.39946997e-01  1.39530703e-01
  1.39063925e-01  1.39143199e-01  1.39058679e-01  1.37893945e-01
  1.35580495e-01  1.34338781e-01  1.33498639e-01  1.32784486e-01
  1.32155240e-01  1.31835923e-01  1.31517068e-01  1.31001815e-01
  1.30840838e-01  1.30859882e-01  1.30862668e-01  1.30546913e-01
  1.30507976e-01  1.30192101e-01  1.29990950e-01  1.30127043e-01
  1.30097583e-01  1.29513577e-01  1.29284039e-01  1.29602090e-01
  1.29603788e-01  1.29245386e-01  1.28988802e-01  1.28417984e-01
  1.26743406e-01  1.25635996e-01  1.25297770e-01  1.25172526e-01
  1.24450788e-01  1.23650528e-01  1.23396754e-01  1.23133935e-01
  1.22751489e-01  1.22418232e-01  1.22757457e-01  1.22814626e-01
  1.22637004e-01  1.22445390e-01  1.22658536e-01  1.22653335e-01
  1.22376770e-01  1.22214489e-01  1.22462049e-01  1.22567438e-01
  1.22684792e-01  1.23131238e-01  1.23397529e-01  1.23166226e-01
  1.22613110e-01  1.22521035e-01  1.22462414e-01  1.22638226e-01
  1.22876994e-01  1.22997962e-01  1.23158671e-01  1.23396941e-01
  1.23841248e-01  1.23872265e-01  1.24059074e-01  1.24169178e-01
  1.24281779e-01  1.23950623e-01  1.23756319e-01  1.23870067e-01
  1.23874672e-01  1.23902522e-01  1.24180056e-01  1.24519944e-01
  1.24639958e-01  1.24836013e-01  1.25119612e-01  1.24630496e-01
  1.23456329e-01  1.22830018e-01  1.22682281e-01  1.22425668e-01
  1.21896513e-01  1.21980675e-01  1.22236677e-01  1.22387759e-01
  1.22717120e-01  1.23359494e-01  1.23884104e-01  1.23817310e-01
  1.24012478e-01  1.24330319e-01  1.24655046e-01  1.24886952e-01
  1.25287950e-01  1.25658706e-01  1.26016706e-01  1.26431659e-01
  1.26837105e-01  1.27115920e-01  1.27325460e-01  1.27473250e-01
  1.27123922e-01  1.26866385e-01  1.26634523e-01  1.26703382e-01
  1.26659587e-01  1.26717225e-01  1.26812264e-01  1.27243519e-01
  1.27977386e-01  1.28637061e-01  1.29614100e-01  1.30185395e-01
  1.30893946e-01  1.31510645e-01  1.32001445e-01  1.32147372e-01
  1.32311478e-01  1.32780001e-01  1.33124590e-01  1.33272454e-01
  1.33838713e-01  1.34759188e-01  1.35363892e-01  1.35424703e-01
  1.35221094e-01  1.35396108e-01  1.35394216e-01  1.35855228e-01
  1.36328757e-01  1.36997506e-01  1.37418032e-01  1.37984112e-01
  1.38861239e-01  1.39473394e-01  1.40204534e-01  1.40802518e-01
  1.41550437e-01  1.41978741e-01  1.42552435e-01  1.42965928e-01
  1.43060818e-01  1.43249884e-01  1.43794701e-01  1.44190684e-01
  1.44339979e-01  1.44875690e-01  1.45726144e-01  1.45994425e-01
  1.45769626e-01  1.45995915e-01  1.46128267e-01  1.46107420e-01
  1.46070048e-01  1.46488085e-01  1.46686926e-01  1.46790847e-01
  1.47961870e-01  1.49219349e-01  1.49755552e-01  1.49974063e-01
  1.50951728e-01  1.51628420e-01  1.51910141e-01  1.52388737e-01
  1.53183803e-01  1.53523356e-01  1.53427780e-01  1.53769121e-01
  1.54298082e-01  1.54360279e-01  1.54456854e-01  1.54825449e-01
  1.54668406e-01  1.54118776e-01  1.53972656e-01  1.54455662e-01
  1.54418230e-01  1.54232368e-01  1.54301152e-01  1.54957682e-01
  1.55453861e-01  1.55730858e-01  1.56414181e-01  1.57047436e-01
  1.57677472e-01  1.57988757e-01  1.58661038e-01  1.59128174e-01
  1.59494296e-01  1.59985602e-01  1.60326019e-01  1.60371721e-01
  1.60826266e-01  1.61756888e-01  1.62170544e-01  1.61964342e-01
  1.61849126e-01  1.62185818e-01  1.62008241e-01  1.61998212e-01
  1.62669510e-01  1.63158685e-01  1.63157552e-01  1.63439333e-01
  1.64260387e-01  1.64556935e-01  1.64577857e-01  1.65195152e-01
  1.66138276e-01  1.66290522e-01  1.66422248e-01  1.67015016e-01
  1.67391062e-01  1.67361900e-01  1.67889878e-01  1.68780148e-01
  1.69093683e-01  1.69360399e-01  1.70089751e-01  1.70425355e-01
  1.70030251e-01  1.69867367e-01  1.70113221e-01  1.70070469e-01
  1.69931650e-01  1.70524999e-01  1.71466619e-01  1.72125295e-01
  1.72646269e-01  1.73443869e-01  1.74031422e-01  1.74308434e-01
  1.74962476e-01  1.75462052e-01  1.75682709e-01  1.75885499e-01
  1.76346943e-01  1.76580563e-01  1.76502392e-01  1.76716521e-01
  1.77221194e-01  1.77664116e-01  1.77880734e-01  1.78174391e-01
  1.78179130e-01  1.77892819e-01  1.77520633e-01  1.77803680e-01
  1.78062707e-01  1.78090721e-01  1.78409114e-01  1.79201797e-01
  1.80028126e-01  1.80358082e-01  1.80915788e-01  1.81295082e-01
  1.81587905e-01  1.81401610e-01  1.81364402e-01  1.81374699e-01
  1.81250781e-01  1.81071565e-01  1.80974171e-01  1.80911481e-01
  1.80817962e-01  1.80749178e-01  1.80668995e-01  1.80447116e-01
  1.80082589e-01  1.80036277e-01  1.79971099e-01  1.79912448e-01
  1.79811910e-01  1.79728583e-01  1.79467440e-01  1.79350644e-01
  1.79545358e-01  1.79595217e-01  1.79373056e-01  1.78954601e-01
  1.78615168e-01  1.78126156e-01  1.77626282e-01  1.76980391e-01
  1.76316202e-01  1.75654173e-01  1.75121054e-01  1.74801722e-01
  1.74714938e-01  1.74715236e-01  1.74555242e-01  1.74081743e-01
  1.73286647e-01  1.72600478e-01  1.72142386e-01  1.71852350e-01
  1.71195328e-01  1.70271114e-01  1.69736832e-01  1.69542655e-01
  1.69134110e-01  1.68278769e-01  1.67843476e-01  1.67481020e-01
  1.66908681e-01  1.66164756e-01  1.65972859e-01  1.65934756e-01
  1.65491000e-01  1.65105984e-01  1.65160760e-01  1.65153056e-01
  1.64783955e-01  1.64821282e-01  1.65084645e-01  1.64869547e-01
  1.64294615e-01  1.64108574e-01  1.63592741e-01  1.62927374e-01
  1.62492007e-01  1.62280858e-01  1.61597863e-01  1.60883188e-01
  1.60558641e-01  1.60262197e-01  1.59693852e-01  1.58960387e-01
  1.58559963e-01  1.57784969e-01  1.56968117e-01  1.56637669e-01
  1.56600282e-01  1.56164363e-01  1.55640498e-01  1.55399114e-01
  1.55168429e-01  1.54886335e-01  1.54724315e-01  1.54226974e-01
  1.52878970e-01  1.51534349e-01  1.50802329e-01  1.50019780e-01
  1.48604438e-01  1.47508875e-01  1.46877155e-01  1.45757660e-01
  1.44421607e-01  1.43696129e-01  1.43462047e-01  1.42410591e-01
  1.41246587e-01  1.40710697e-01  1.40379280e-01  1.39556170e-01
  1.38855204e-01  1.38733670e-01  1.38370886e-01  1.37637362e-01
  1.37502417e-01  1.37872025e-01  1.37504801e-01  1.36216015e-01
  1.34691209e-01  1.33215591e-01  1.31442815e-01  1.30171970e-01
  1.29486367e-01  1.28600836e-01  1.27463326e-01  1.27093628e-01
  1.26756206e-01  1.25764608e-01  1.25093088e-01  1.24646634e-01
  1.23722225e-01  1.22397229e-01  1.22027412e-01  1.22136734e-01
  1.21633887e-01  1.20904632e-01  1.20684214e-01  1.20250240e-01
  1.19175330e-01  1.18826665e-01  1.18856139e-01  1.17533825e-01
  1.15031481e-01  1.13605395e-01  1.12643629e-01  1.10638060e-01
  1.08193070e-01  1.06678076e-01  1.05347089e-01  1.03565291e-01
  1.02426343e-01  1.01764902e-01  1.00711957e-01  9.92199555e-02
  9.81521383e-02  9.69215110e-02  9.57891345e-02  9.50857624e-02
  9.45786163e-02  9.35911536e-02  9.24623311e-02  9.19813067e-02
  9.16022435e-02  9.07810107e-02  8.97923335e-02  8.84225741e-02
  8.61203820e-02  8.38758647e-02  8.25181827e-02  8.16639215e-02
  8.01905692e-02  7.87517205e-02  7.77636021e-02  7.64453039e-02
  7.52556324e-02  7.48058558e-02  7.47152045e-02  7.36337900e-02
  7.25310147e-02  7.20990822e-02  7.17324540e-02  7.07210600e-02
  6.99825287e-02  6.97523504e-02  6.92587271e-02  6.84389099e-02
  6.80575967e-02  6.79854676e-02  6.72036931e-02  6.55215308e-02
  6.34705797e-02  6.16431162e-02  5.97907566e-02  5.84170371e-02
  5.72438277e-02  5.55397943e-02  5.36878109e-02  5.24410345e-02
  5.14794849e-02  5.03998511e-02  4.98011895e-02  4.93014082e-02
  4.81919087e-02  4.69100624e-02  4.67161126e-02  4.66107018e-02
  4.58285399e-02  4.52713519e-02  4.53654341e-02  4.49632965e-02
  4.39245552e-02  4.37436625e-02  4.38079573e-02  4.22751829e-02
  3.95586155e-02  3.78885865e-02  3.65141667e-02  3.46332192e-02
  3.26261185e-02  3.14471461e-02  2.96722595e-02  2.75452528e-02
  2.64450870e-02  2.61368155e-02  2.55131759e-02  2.46880446e-02
  2.47670617e-02  2.42776610e-02  2.35297382e-02  2.35440694e-02
  2.40589511e-02  2.38037705e-02  2.34915800e-02  2.38361135e-02
  2.40448155e-02  2.37276405e-02  2.33649388e-02  2.29656901e-02
  2.08958983e-02  1.87255256e-02  1.73652302e-02  1.63226500e-02
  1.45366937e-02  1.32937087e-02  1.31691061e-02  1.26295304e-02
  1.18084848e-02  1.18709523e-02  1.27386628e-02  1.26737971e-02
  1.21551342e-02  1.17915664e-02  1.17509644e-02  1.12696625e-02
  1.07198628e-02  1.07105123e-02  1.04155261e-02  9.32981726e-03
  9.07939579e-03  9.80101153e-03  9.75359604e-03  8.20465572e-03
  6.14098134e-03  4.74412087e-03  3.13056936e-03  1.87773572e-03
  9.02257627e-04 -8.28742996e-05 -1.24813616e-03 -1.83479034e-03
 -2.03787116e-03 -2.55239755e-03 -2.16931198e-03 -2.00814102e-03
 -2.56121485e-03 -3.63315851e-03 -3.31725273e-03 -2.55583902e-03
 -2.73029064e-03 -3.45261977e-03 -3.21865082e-03 -3.04748793e-03
 -3.71885300e-03 -3.30614625e-03 -2.36115325e-03 -3.36803962e-03
 -6.16606837e-03 -7.44761573e-03 -8.05529114e-03 -9.85393487e-03
 -1.19392015e-02 -1.27451299e-02 -1.35254348e-02 -1.44368736e-02
 -1.40575469e-02 -1.32209584e-02 -1.28433285e-02 -1.28532154e-02
 -1.22139463e-02 -1.26622301e-02 -1.30110737e-02 -1.27916224e-02
 -1.20923286e-02 -1.18381307e-02 -1.18943043e-02 -1.18358098e-02
 -1.19105168e-02 -1.18369395e-02 -1.18668200e-02 -1.20853018e-02
 -1.43497102e-02 -1.68269910e-02 -1.78736020e-02 -1.81225557e-02
 -1.97827201e-02 -2.06381418e-02 -2.03791671e-02 -2.08607689e-02
 -2.16490384e-02 -2.12791692e-02 -2.02834904e-02 -2.08688583e-02
 -2.11024135e-02 -2.15078890e-02 -2.09608525e-02 -2.16137581e-02
 -2.26998273e-02 -2.25351099e-02 -2.19609905e-02 -2.31140386e-02
 -2.46798955e-02 -2.41142511e-02 -2.53614560e-02 -2.47341730e-02]
