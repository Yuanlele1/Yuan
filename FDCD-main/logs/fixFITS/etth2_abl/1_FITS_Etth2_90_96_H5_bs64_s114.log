Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=30, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_90_96', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=96, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_90_96_FITS_ETTh2_ftM_sl90_ll48_pl96_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8455
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=30, out_features=62, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1666560.0
params:  1922.0
Trainable parameters:  1922
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.042393207550049
Epoch: 1, Steps: 66 | Train Loss: 0.5836795 Vali Loss: 0.2734843 Test Loss: 0.3666149
Validation loss decreased (inf --> 0.273484).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.9339165687561035
Epoch: 2, Steps: 66 | Train Loss: 0.5088699 Vali Loss: 0.2489066 Test Loss: 0.3334194
Validation loss decreased (0.273484 --> 0.248907).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.33357310295105
Epoch: 3, Steps: 66 | Train Loss: 0.4793750 Vali Loss: 0.2383960 Test Loss: 0.3189974
Validation loss decreased (0.248907 --> 0.238396).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.0524141788482666
Epoch: 4, Steps: 66 | Train Loss: 0.4646463 Vali Loss: 0.2329820 Test Loss: 0.3116249
Validation loss decreased (0.238396 --> 0.232982).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.496063232421875
Epoch: 5, Steps: 66 | Train Loss: 0.4565015 Vali Loss: 0.2286244 Test Loss: 0.3073023
Validation loss decreased (0.232982 --> 0.228624).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 6.904859304428101
Epoch: 6, Steps: 66 | Train Loss: 0.4509515 Vali Loss: 0.2261833 Test Loss: 0.3045635
Validation loss decreased (0.228624 --> 0.226183).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 5.879972457885742
Epoch: 7, Steps: 66 | Train Loss: 0.4471476 Vali Loss: 0.2246095 Test Loss: 0.3025585
Validation loss decreased (0.226183 --> 0.224609).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.581886529922485
Epoch: 8, Steps: 66 | Train Loss: 0.4437348 Vali Loss: 0.2223669 Test Loss: 0.3011680
Validation loss decreased (0.224609 --> 0.222367).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.7674317359924316
Epoch: 9, Steps: 66 | Train Loss: 0.4416467 Vali Loss: 0.2207900 Test Loss: 0.3000020
Validation loss decreased (0.222367 --> 0.220790).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.2281689643859863
Epoch: 10, Steps: 66 | Train Loss: 0.4398979 Vali Loss: 0.2207649 Test Loss: 0.2990870
Validation loss decreased (0.220790 --> 0.220765).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.245266675949097
Epoch: 11, Steps: 66 | Train Loss: 0.4383040 Vali Loss: 0.2195516 Test Loss: 0.2982467
Validation loss decreased (0.220765 --> 0.219552).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.1527180671691895
Epoch: 12, Steps: 66 | Train Loss: 0.4357420 Vali Loss: 0.2199800 Test Loss: 0.2976095
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.563781261444092
Epoch: 13, Steps: 66 | Train Loss: 0.4354821 Vali Loss: 0.2192122 Test Loss: 0.2969993
Validation loss decreased (0.219552 --> 0.219212).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.821120262145996
Epoch: 14, Steps: 66 | Train Loss: 0.4346404 Vali Loss: 0.2190131 Test Loss: 0.2965153
Validation loss decreased (0.219212 --> 0.219013).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.213824510574341
Epoch: 15, Steps: 66 | Train Loss: 0.4334958 Vali Loss: 0.2185650 Test Loss: 0.2960641
Validation loss decreased (0.219013 --> 0.218565).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.179436445236206
Epoch: 16, Steps: 66 | Train Loss: 0.4327799 Vali Loss: 0.2178402 Test Loss: 0.2956760
Validation loss decreased (0.218565 --> 0.217840).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.03544545173645
Epoch: 17, Steps: 66 | Train Loss: 0.4320623 Vali Loss: 0.2169267 Test Loss: 0.2953014
Validation loss decreased (0.217840 --> 0.216927).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.596796751022339
Epoch: 18, Steps: 66 | Train Loss: 0.4303296 Vali Loss: 0.2167091 Test Loss: 0.2950054
Validation loss decreased (0.216927 --> 0.216709).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.752687454223633
Epoch: 19, Steps: 66 | Train Loss: 0.4307261 Vali Loss: 0.2173520 Test Loss: 0.2947510
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.869123935699463
Epoch: 20, Steps: 66 | Train Loss: 0.4299886 Vali Loss: 0.2165734 Test Loss: 0.2944978
Validation loss decreased (0.216709 --> 0.216573).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.9213902950286865
Epoch: 21, Steps: 66 | Train Loss: 0.4297960 Vali Loss: 0.2162165 Test Loss: 0.2942644
Validation loss decreased (0.216573 --> 0.216216).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.464361667633057
Epoch: 22, Steps: 66 | Train Loss: 0.4291129 Vali Loss: 0.2165090 Test Loss: 0.2940671
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.7770113945007324
Epoch: 23, Steps: 66 | Train Loss: 0.4286144 Vali Loss: 0.2163689 Test Loss: 0.2938832
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.4495584964752197
Epoch: 24, Steps: 66 | Train Loss: 0.4284604 Vali Loss: 0.2159413 Test Loss: 0.2937363
Validation loss decreased (0.216216 --> 0.215941).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.423222064971924
Epoch: 25, Steps: 66 | Train Loss: 0.4281902 Vali Loss: 0.2157912 Test Loss: 0.2935646
Validation loss decreased (0.215941 --> 0.215791).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.171546220779419
Epoch: 26, Steps: 66 | Train Loss: 0.4266645 Vali Loss: 0.2152916 Test Loss: 0.2934489
Validation loss decreased (0.215791 --> 0.215292).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.963124990463257
Epoch: 27, Steps: 66 | Train Loss: 0.4270327 Vali Loss: 0.2145373 Test Loss: 0.2933245
Validation loss decreased (0.215292 --> 0.214537).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.6511270999908447
Epoch: 28, Steps: 66 | Train Loss: 0.4274061 Vali Loss: 0.2146077 Test Loss: 0.2932017
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.9803414344787598
Epoch: 29, Steps: 66 | Train Loss: 0.4268304 Vali Loss: 0.2152981 Test Loss: 0.2931055
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.633340358734131
Epoch: 30, Steps: 66 | Train Loss: 0.4267652 Vali Loss: 0.2148658 Test Loss: 0.2930058
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.208467721939087
Epoch: 31, Steps: 66 | Train Loss: 0.4264809 Vali Loss: 0.2149045 Test Loss: 0.2929209
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.3841664791107178
Epoch: 32, Steps: 66 | Train Loss: 0.4263796 Vali Loss: 0.2150768 Test Loss: 0.2928350
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.1487724781036377
Epoch: 33, Steps: 66 | Train Loss: 0.4262665 Vali Loss: 0.2151894 Test Loss: 0.2927771
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.4953830242156982
Epoch: 34, Steps: 66 | Train Loss: 0.4260594 Vali Loss: 0.2150750 Test Loss: 0.2926933
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.80672287940979
Epoch: 35, Steps: 66 | Train Loss: 0.4258283 Vali Loss: 0.2146958 Test Loss: 0.2926457
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 4.1096765995025635
Epoch: 36, Steps: 66 | Train Loss: 0.4257946 Vali Loss: 0.2147845 Test Loss: 0.2925749
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.6583964824676514
Epoch: 37, Steps: 66 | Train Loss: 0.4256128 Vali Loss: 0.2133262 Test Loss: 0.2925188
Validation loss decreased (0.214537 --> 0.213326).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.8081722259521484
Epoch: 38, Steps: 66 | Train Loss: 0.4255142 Vali Loss: 0.2147987 Test Loss: 0.2924652
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 6.567341327667236
Epoch: 39, Steps: 66 | Train Loss: 0.4253808 Vali Loss: 0.2144971 Test Loss: 0.2924255
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 5.399364233016968
Epoch: 40, Steps: 66 | Train Loss: 0.4252127 Vali Loss: 0.2144774 Test Loss: 0.2923959
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 4.4999213218688965
Epoch: 41, Steps: 66 | Train Loss: 0.4248510 Vali Loss: 0.2147278 Test Loss: 0.2923375
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.9351038932800293
Epoch: 42, Steps: 66 | Train Loss: 0.4250915 Vali Loss: 0.2145460 Test Loss: 0.2922968
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.0064494609832764
Epoch: 43, Steps: 66 | Train Loss: 0.4249326 Vali Loss: 0.2142031 Test Loss: 0.2922583
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.456538200378418
Epoch: 44, Steps: 66 | Train Loss: 0.4247194 Vali Loss: 0.2142745 Test Loss: 0.2922338
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 3.600891590118408
Epoch: 45, Steps: 66 | Train Loss: 0.4247035 Vali Loss: 0.2143579 Test Loss: 0.2922024
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.6427271366119385
Epoch: 46, Steps: 66 | Train Loss: 0.4247812 Vali Loss: 0.2137136 Test Loss: 0.2921725
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 4.198484182357788
Epoch: 47, Steps: 66 | Train Loss: 0.4244282 Vali Loss: 0.2140464 Test Loss: 0.2921465
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 4.2885191440582275
Epoch: 48, Steps: 66 | Train Loss: 0.4243416 Vali Loss: 0.2129502 Test Loss: 0.2921214
Validation loss decreased (0.213326 --> 0.212950).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.057335376739502
Epoch: 49, Steps: 66 | Train Loss: 0.4244353 Vali Loss: 0.2137054 Test Loss: 0.2920947
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 4.961831569671631
Epoch: 50, Steps: 66 | Train Loss: 0.4244939 Vali Loss: 0.2142234 Test Loss: 0.2920709
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 4.305393218994141
Epoch: 51, Steps: 66 | Train Loss: 0.4244096 Vali Loss: 0.2132232 Test Loss: 0.2920429
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.4726691246032715
Epoch: 52, Steps: 66 | Train Loss: 0.4242290 Vali Loss: 0.2126616 Test Loss: 0.2920302
Validation loss decreased (0.212950 --> 0.212662).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.2702441215515137
Epoch: 53, Steps: 66 | Train Loss: 0.4225111 Vali Loss: 0.2138088 Test Loss: 0.2920165
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.753361225128174
Epoch: 54, Steps: 66 | Train Loss: 0.4241283 Vali Loss: 0.2136711 Test Loss: 0.2919998
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.2835137844085693
Epoch: 55, Steps: 66 | Train Loss: 0.4238985 Vali Loss: 0.2134698 Test Loss: 0.2919843
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 3.0399763584136963
Epoch: 56, Steps: 66 | Train Loss: 0.4239357 Vali Loss: 0.2132500 Test Loss: 0.2919668
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 3.016885995864868
Epoch: 57, Steps: 66 | Train Loss: 0.4241388 Vali Loss: 0.2133997 Test Loss: 0.2919527
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 3.6999804973602295
Epoch: 58, Steps: 66 | Train Loss: 0.4230324 Vali Loss: 0.2136868 Test Loss: 0.2919416
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 4.407989978790283
Epoch: 59, Steps: 66 | Train Loss: 0.4239925 Vali Loss: 0.2130894 Test Loss: 0.2919249
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 3.6155030727386475
Epoch: 60, Steps: 66 | Train Loss: 0.4239553 Vali Loss: 0.2142149 Test Loss: 0.2919111
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 3.140885829925537
Epoch: 61, Steps: 66 | Train Loss: 0.4222829 Vali Loss: 0.2139489 Test Loss: 0.2918986
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.6682512760162354
Epoch: 62, Steps: 66 | Train Loss: 0.4238210 Vali Loss: 0.2139242 Test Loss: 0.2918877
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.9098756313323975
Epoch: 63, Steps: 66 | Train Loss: 0.4225923 Vali Loss: 0.2140595 Test Loss: 0.2918800
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.45157527923584
Epoch: 64, Steps: 66 | Train Loss: 0.4235594 Vali Loss: 0.2126237 Test Loss: 0.2918699
Validation loss decreased (0.212662 --> 0.212624).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.1658859252929688
Epoch: 65, Steps: 66 | Train Loss: 0.4239160 Vali Loss: 0.2136061 Test Loss: 0.2918610
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.844095468521118
Epoch: 66, Steps: 66 | Train Loss: 0.4237908 Vali Loss: 0.2146395 Test Loss: 0.2918481
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 3.403944492340088
Epoch: 67, Steps: 66 | Train Loss: 0.4238833 Vali Loss: 0.2139035 Test Loss: 0.2918423
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 4.543091297149658
Epoch: 68, Steps: 66 | Train Loss: 0.4238483 Vali Loss: 0.2136538 Test Loss: 0.2918360
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.6643176078796387
Epoch: 69, Steps: 66 | Train Loss: 0.4237577 Vali Loss: 0.2135251 Test Loss: 0.2918281
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.8766613006591797
Epoch: 70, Steps: 66 | Train Loss: 0.4238202 Vali Loss: 0.2137163 Test Loss: 0.2918232
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 3.969280242919922
Epoch: 71, Steps: 66 | Train Loss: 0.4236396 Vali Loss: 0.2141493 Test Loss: 0.2918157
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 5.5030012130737305
Epoch: 72, Steps: 66 | Train Loss: 0.4237063 Vali Loss: 0.2134216 Test Loss: 0.2918102
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 4.443974494934082
Epoch: 73, Steps: 66 | Train Loss: 0.4234771 Vali Loss: 0.2142255 Test Loss: 0.2918042
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 3.6499204635620117
Epoch: 74, Steps: 66 | Train Loss: 0.4236458 Vali Loss: 0.2135406 Test Loss: 0.2917967
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 3.2114481925964355
Epoch: 75, Steps: 66 | Train Loss: 0.4236470 Vali Loss: 0.2141582 Test Loss: 0.2917916
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 3.279975652694702
Epoch: 76, Steps: 66 | Train Loss: 0.4233236 Vali Loss: 0.2136393 Test Loss: 0.2917885
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.7152371406555176
Epoch: 77, Steps: 66 | Train Loss: 0.4235739 Vali Loss: 0.2141801 Test Loss: 0.2917824
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 4.187967777252197
Epoch: 78, Steps: 66 | Train Loss: 0.4233427 Vali Loss: 0.2138069 Test Loss: 0.2917781
EarlyStopping counter: 14 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.834097146987915
Epoch: 79, Steps: 66 | Train Loss: 0.4233929 Vali Loss: 0.2139890 Test Loss: 0.2917757
EarlyStopping counter: 15 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 4.305609226226807
Epoch: 80, Steps: 66 | Train Loss: 0.4236595 Vali Loss: 0.2134262 Test Loss: 0.2917722
EarlyStopping counter: 16 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 4.323676109313965
Epoch: 81, Steps: 66 | Train Loss: 0.4235019 Vali Loss: 0.2135414 Test Loss: 0.2917673
EarlyStopping counter: 17 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 3.485757827758789
Epoch: 82, Steps: 66 | Train Loss: 0.4235408 Vali Loss: 0.2135590 Test Loss: 0.2917614
EarlyStopping counter: 18 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 3.584564685821533
Epoch: 83, Steps: 66 | Train Loss: 0.4235971 Vali Loss: 0.2144879 Test Loss: 0.2917606
EarlyStopping counter: 19 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 3.9557011127471924
Epoch: 84, Steps: 66 | Train Loss: 0.4235540 Vali Loss: 0.2135469 Test Loss: 0.2917568
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_90_96_FITS_ETTh2_ftM_sl90_ll48_pl96_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.292386531829834, mae:0.3401738107204437, rse:0.4357726573944092, corr:[0.2750403  0.27832684 0.2754943  0.27615818 0.27365872 0.27274954
 0.27201715 0.27043217 0.26992413 0.2688354  0.26753998 0.26628563
 0.26438746 0.26315358 0.2623764  0.2612755  0.2607631  0.2605159
 0.25947052 0.25839218 0.25756285 0.25661594 0.25555256 0.25344193
 0.2504059  0.24796213 0.24543773 0.24371746 0.24254018 0.24091505
 0.23982175 0.23899053 0.23746113 0.23602541 0.23529342 0.23382668
 0.23233621 0.23136121 0.23009281 0.22917679 0.22867858 0.22805145
 0.22775252 0.2270624  0.22600585 0.22533624 0.22448884 0.22224244
 0.21904819 0.21624921 0.21344113 0.21175167 0.21013793 0.20822996
 0.20710672 0.20504376 0.20362385 0.2029828  0.20201221 0.20043033
 0.19980653 0.19940683 0.19881672 0.19913478 0.19879572 0.19808228
 0.19799535 0.19691913 0.19624998 0.19644165 0.1952438  0.19336797
 0.19146655 0.18949549 0.18780507 0.18721205 0.1855732  0.18475342
 0.18529294 0.18380658 0.18326448 0.18387075 0.18326217 0.18268934
 0.18266977 0.18186116 0.18209383 0.18291721 0.18139078 0.18128628
 0.18095554 0.17823945 0.17969699 0.17861369 0.1772745  0.17868458]
