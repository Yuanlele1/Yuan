Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=103, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=103, out_features=206, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  19011328.0
params:  21424.0
Trainable parameters:  21424
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.321016550064087
Epoch: 1, Steps: 56 | Train Loss: 1.0662293 Vali Loss: 0.8033640 Test Loss: 0.4478399
Validation loss decreased (inf --> 0.803364).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.041908025741577
Epoch: 2, Steps: 56 | Train Loss: 0.9175111 Vali Loss: 0.7507126 Test Loss: 0.4153876
Validation loss decreased (0.803364 --> 0.750713).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.661325693130493
Epoch: 3, Steps: 56 | Train Loss: 0.8749892 Vali Loss: 0.7208360 Test Loss: 0.4041719
Validation loss decreased (0.750713 --> 0.720836).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.4794113636016846
Epoch: 4, Steps: 56 | Train Loss: 0.8555956 Vali Loss: 0.7070763 Test Loss: 0.3981053
Validation loss decreased (0.720836 --> 0.707076).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.7330551147460938
Epoch: 5, Steps: 56 | Train Loss: 0.8457130 Vali Loss: 0.7000712 Test Loss: 0.3942198
Validation loss decreased (0.707076 --> 0.700071).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.692089319229126
Epoch: 6, Steps: 56 | Train Loss: 0.8382070 Vali Loss: 0.6903272 Test Loss: 0.3913783
Validation loss decreased (0.700071 --> 0.690327).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.634352922439575
Epoch: 7, Steps: 56 | Train Loss: 0.8339911 Vali Loss: 0.6820886 Test Loss: 0.3892525
Validation loss decreased (0.690327 --> 0.682089).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.453305244445801
Epoch: 8, Steps: 56 | Train Loss: 0.8277261 Vali Loss: 0.6834957 Test Loss: 0.3876483
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.9456162452697754
Epoch: 9, Steps: 56 | Train Loss: 0.8242874 Vali Loss: 0.6803086 Test Loss: 0.3865004
Validation loss decreased (0.682089 --> 0.680309).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.4761898517608643
Epoch: 10, Steps: 56 | Train Loss: 0.8227620 Vali Loss: 0.6719076 Test Loss: 0.3854630
Validation loss decreased (0.680309 --> 0.671908).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.369851589202881
Epoch: 11, Steps: 56 | Train Loss: 0.8201071 Vali Loss: 0.6681314 Test Loss: 0.3846047
Validation loss decreased (0.671908 --> 0.668131).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.7404019832611084
Epoch: 12, Steps: 56 | Train Loss: 0.8186744 Vali Loss: 0.6690398 Test Loss: 0.3839014
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.0424818992614746
Epoch: 13, Steps: 56 | Train Loss: 0.8170866 Vali Loss: 0.6638780 Test Loss: 0.3833839
Validation loss decreased (0.668131 --> 0.663878).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.3335466384887695
Epoch: 14, Steps: 56 | Train Loss: 0.8144358 Vali Loss: 0.6646087 Test Loss: 0.3829410
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.102720022201538
Epoch: 15, Steps: 56 | Train Loss: 0.8139213 Vali Loss: 0.6621760 Test Loss: 0.3826070
Validation loss decreased (0.663878 --> 0.662176).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.987199544906616
Epoch: 16, Steps: 56 | Train Loss: 0.8126239 Vali Loss: 0.6641305 Test Loss: 0.3823389
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.745265245437622
Epoch: 17, Steps: 56 | Train Loss: 0.8116021 Vali Loss: 0.6605287 Test Loss: 0.3820058
Validation loss decreased (0.662176 --> 0.660529).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.905571222305298
Epoch: 18, Steps: 56 | Train Loss: 0.8119909 Vali Loss: 0.6614178 Test Loss: 0.3818875
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.106145143508911
Epoch: 19, Steps: 56 | Train Loss: 0.8107452 Vali Loss: 0.6575223 Test Loss: 0.3817340
Validation loss decreased (0.660529 --> 0.657522).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.8784985542297363
Epoch: 20, Steps: 56 | Train Loss: 0.8096434 Vali Loss: 0.6613098 Test Loss: 0.3816178
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.741269111633301
Epoch: 21, Steps: 56 | Train Loss: 0.8107471 Vali Loss: 0.6625102 Test Loss: 0.3814964
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.836669445037842
Epoch: 22, Steps: 56 | Train Loss: 0.8093314 Vali Loss: 0.6577879 Test Loss: 0.3813831
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.862658739089966
Epoch: 23, Steps: 56 | Train Loss: 0.8094123 Vali Loss: 0.6573209 Test Loss: 0.3813193
Validation loss decreased (0.657522 --> 0.657321).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.9554200172424316
Epoch: 24, Steps: 56 | Train Loss: 0.8079974 Vali Loss: 0.6573439 Test Loss: 0.3812443
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.8439829349517822
Epoch: 25, Steps: 56 | Train Loss: 0.8082516 Vali Loss: 0.6516218 Test Loss: 0.3811703
Validation loss decreased (0.657321 --> 0.651622).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.196794033050537
Epoch: 26, Steps: 56 | Train Loss: 0.8086693 Vali Loss: 0.6552539 Test Loss: 0.3811808
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.3893277645111084
Epoch: 27, Steps: 56 | Train Loss: 0.8087615 Vali Loss: 0.6541068 Test Loss: 0.3810537
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.108200788497925
Epoch: 28, Steps: 56 | Train Loss: 0.8080725 Vali Loss: 0.6551271 Test Loss: 0.3810498
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.351860523223877
Epoch: 29, Steps: 56 | Train Loss: 0.8071834 Vali Loss: 0.6549100 Test Loss: 0.3810343
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.565880298614502
Epoch: 30, Steps: 56 | Train Loss: 0.8065453 Vali Loss: 0.6545709 Test Loss: 0.3810043
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.1017332077026367
Epoch: 31, Steps: 56 | Train Loss: 0.8065488 Vali Loss: 0.6536777 Test Loss: 0.3809897
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.638584852218628
Epoch: 32, Steps: 56 | Train Loss: 0.8043972 Vali Loss: 0.6532283 Test Loss: 0.3809817
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.5925631523132324
Epoch: 33, Steps: 56 | Train Loss: 0.8063175 Vali Loss: 0.6585411 Test Loss: 0.3809744
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.0503456592559814
Epoch: 34, Steps: 56 | Train Loss: 0.8044862 Vali Loss: 0.6530581 Test Loss: 0.3809273
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.9144816398620605
Epoch: 35, Steps: 56 | Train Loss: 0.8064020 Vali Loss: 0.6550030 Test Loss: 0.3809265
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.0332579612731934
Epoch: 36, Steps: 56 | Train Loss: 0.8054251 Vali Loss: 0.6572758 Test Loss: 0.3809058
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.5344107151031494
Epoch: 37, Steps: 56 | Train Loss: 0.8053726 Vali Loss: 0.6537533 Test Loss: 0.3809084
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.9300169944763184
Epoch: 38, Steps: 56 | Train Loss: 0.8062974 Vali Loss: 0.6500342 Test Loss: 0.3808920
Validation loss decreased (0.651622 --> 0.650034).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.726588726043701
Epoch: 39, Steps: 56 | Train Loss: 0.8038557 Vali Loss: 0.6509275 Test Loss: 0.3808963
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.31666898727417
Epoch: 40, Steps: 56 | Train Loss: 0.8057130 Vali Loss: 0.6551763 Test Loss: 0.3808825
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.545987606048584
Epoch: 41, Steps: 56 | Train Loss: 0.8041188 Vali Loss: 0.6529939 Test Loss: 0.3808843
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.59028959274292
Epoch: 42, Steps: 56 | Train Loss: 0.8046970 Vali Loss: 0.6536826 Test Loss: 0.3808895
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.6330511569976807
Epoch: 43, Steps: 56 | Train Loss: 0.8030397 Vali Loss: 0.6493685 Test Loss: 0.3808759
Validation loss decreased (0.650034 --> 0.649369).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.37422513961792
Epoch: 44, Steps: 56 | Train Loss: 0.8048360 Vali Loss: 0.6546916 Test Loss: 0.3808720
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.6254727840423584
Epoch: 45, Steps: 56 | Train Loss: 0.8041008 Vali Loss: 0.6532808 Test Loss: 0.3808703
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.7874133586883545
Epoch: 46, Steps: 56 | Train Loss: 0.8059869 Vali Loss: 0.6525233 Test Loss: 0.3808812
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.1002304553985596
Epoch: 47, Steps: 56 | Train Loss: 0.8039917 Vali Loss: 0.6540061 Test Loss: 0.3808804
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.61645770072937
Epoch: 48, Steps: 56 | Train Loss: 0.8055551 Vali Loss: 0.6523371 Test Loss: 0.3808688
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.4867024421691895
Epoch: 49, Steps: 56 | Train Loss: 0.8057172 Vali Loss: 0.6529897 Test Loss: 0.3808767
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.542006731033325
Epoch: 50, Steps: 56 | Train Loss: 0.8042610 Vali Loss: 0.6510202 Test Loss: 0.3808643
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.7393341064453125
Epoch: 51, Steps: 56 | Train Loss: 0.8050477 Vali Loss: 0.6532206 Test Loss: 0.3808582
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.7781171798706055
Epoch: 52, Steps: 56 | Train Loss: 0.8046198 Vali Loss: 0.6528189 Test Loss: 0.3808742
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.353609800338745
Epoch: 53, Steps: 56 | Train Loss: 0.8046175 Vali Loss: 0.6463758 Test Loss: 0.3808733
Validation loss decreased (0.649369 --> 0.646376).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.850050449371338
Epoch: 54, Steps: 56 | Train Loss: 0.8048619 Vali Loss: 0.6513327 Test Loss: 0.3808694
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.741820812225342
Epoch: 55, Steps: 56 | Train Loss: 0.8038626 Vali Loss: 0.6509819 Test Loss: 0.3808685
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.801887273788452
Epoch: 56, Steps: 56 | Train Loss: 0.8038807 Vali Loss: 0.6540638 Test Loss: 0.3808656
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 3.6488044261932373
Epoch: 57, Steps: 56 | Train Loss: 0.8048395 Vali Loss: 0.6498508 Test Loss: 0.3808661
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.5524895191192627
Epoch: 58, Steps: 56 | Train Loss: 0.8035507 Vali Loss: 0.6534734 Test Loss: 0.3808681
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 3.136458158493042
Epoch: 59, Steps: 56 | Train Loss: 0.8044810 Vali Loss: 0.6536708 Test Loss: 0.3808696
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.4255971908569336
Epoch: 60, Steps: 56 | Train Loss: 0.8054711 Vali Loss: 0.6494623 Test Loss: 0.3808721
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.712608575820923
Epoch: 61, Steps: 56 | Train Loss: 0.8052322 Vali Loss: 0.6519529 Test Loss: 0.3808726
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 3.0209546089172363
Epoch: 62, Steps: 56 | Train Loss: 0.8043354 Vali Loss: 0.6517799 Test Loss: 0.3808707
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.207731246948242
Epoch: 63, Steps: 56 | Train Loss: 0.8038131 Vali Loss: 0.6474028 Test Loss: 0.3808692
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.46833872795105
Epoch: 64, Steps: 56 | Train Loss: 0.8046631 Vali Loss: 0.6516062 Test Loss: 0.3808685
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.668704032897949
Epoch: 65, Steps: 56 | Train Loss: 0.8044320 Vali Loss: 0.6504624 Test Loss: 0.3808715
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.573650598526001
Epoch: 66, Steps: 56 | Train Loss: 0.8040794 Vali Loss: 0.6503867 Test Loss: 0.3808742
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.871957778930664
Epoch: 67, Steps: 56 | Train Loss: 0.8052733 Vali Loss: 0.6511720 Test Loss: 0.3808715
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.2106549739837646
Epoch: 68, Steps: 56 | Train Loss: 0.8043371 Vali Loss: 0.6520047 Test Loss: 0.3808731
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 4.037181377410889
Epoch: 69, Steps: 56 | Train Loss: 0.8043965 Vali Loss: 0.6526137 Test Loss: 0.3808741
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.583498477935791
Epoch: 70, Steps: 56 | Train Loss: 0.8039410 Vali Loss: 0.6488535 Test Loss: 0.3808730
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.6024041175842285
Epoch: 71, Steps: 56 | Train Loss: 0.8011514 Vali Loss: 0.6559672 Test Loss: 0.3808740
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.825923442840576
Epoch: 72, Steps: 56 | Train Loss: 0.8042071 Vali Loss: 0.6527007 Test Loss: 0.3808759
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.149578332901001
Epoch: 73, Steps: 56 | Train Loss: 0.8041830 Vali Loss: 0.6459261 Test Loss: 0.3808785
Validation loss decreased (0.646376 --> 0.645926).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.5453808307647705
Epoch: 74, Steps: 56 | Train Loss: 0.8035165 Vali Loss: 0.6518098 Test Loss: 0.3808776
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.5122835636138916
Epoch: 75, Steps: 56 | Train Loss: 0.8027888 Vali Loss: 0.6490305 Test Loss: 0.3808789
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.3759748935699463
Epoch: 76, Steps: 56 | Train Loss: 0.8027780 Vali Loss: 0.6482116 Test Loss: 0.3808761
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.6174581050872803
Epoch: 77, Steps: 56 | Train Loss: 0.8036426 Vali Loss: 0.6489346 Test Loss: 0.3808795
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.2829766273498535
Epoch: 78, Steps: 56 | Train Loss: 0.8026146 Vali Loss: 0.6519182 Test Loss: 0.3808815
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.941532850265503
Epoch: 79, Steps: 56 | Train Loss: 0.8045160 Vali Loss: 0.6512209 Test Loss: 0.3808789
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.214937686920166
Epoch: 80, Steps: 56 | Train Loss: 0.8036798 Vali Loss: 0.6474238 Test Loss: 0.3808778
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 3.0013394355773926
Epoch: 81, Steps: 56 | Train Loss: 0.8025309 Vali Loss: 0.6473906 Test Loss: 0.3808793
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.5721161365509033
Epoch: 82, Steps: 56 | Train Loss: 0.8042226 Vali Loss: 0.6459019 Test Loss: 0.3808842
Validation loss decreased (0.645926 --> 0.645902).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 2.266502857208252
Epoch: 83, Steps: 56 | Train Loss: 0.8030645 Vali Loss: 0.6504381 Test Loss: 0.3808809
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.8050689697265625
Epoch: 84, Steps: 56 | Train Loss: 0.8042889 Vali Loss: 0.6504974 Test Loss: 0.3808815
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 2.386245012283325
Epoch: 85, Steps: 56 | Train Loss: 0.8046920 Vali Loss: 0.6539400 Test Loss: 0.3808806
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.7936289310455322
Epoch: 86, Steps: 56 | Train Loss: 0.8021481 Vali Loss: 0.6493336 Test Loss: 0.3808808
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.541872501373291
Epoch: 87, Steps: 56 | Train Loss: 0.8040085 Vali Loss: 0.6530783 Test Loss: 0.3808834
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.832721471786499
Epoch: 88, Steps: 56 | Train Loss: 0.8036597 Vali Loss: 0.6477443 Test Loss: 0.3808823
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 2.9328670501708984
Epoch: 89, Steps: 56 | Train Loss: 0.8040946 Vali Loss: 0.6488932 Test Loss: 0.3808826
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 2.995666980743408
Epoch: 90, Steps: 56 | Train Loss: 0.8038147 Vali Loss: 0.6471859 Test Loss: 0.3808828
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 2.706568479537964
Epoch: 91, Steps: 56 | Train Loss: 0.8032939 Vali Loss: 0.6475691 Test Loss: 0.3808826
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 2.7086637020111084
Epoch: 92, Steps: 56 | Train Loss: 0.8037786 Vali Loss: 0.6500856 Test Loss: 0.3808824
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 2.3351190090179443
Epoch: 93, Steps: 56 | Train Loss: 0.8047479 Vali Loss: 0.6491472 Test Loss: 0.3808846
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 2.895951509475708
Epoch: 94, Steps: 56 | Train Loss: 0.8034264 Vali Loss: 0.6494836 Test Loss: 0.3808827
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 2.803088903427124
Epoch: 95, Steps: 56 | Train Loss: 0.8043494 Vali Loss: 0.6500627 Test Loss: 0.3808834
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 2.5052239894866943
Epoch: 96, Steps: 56 | Train Loss: 0.8024953 Vali Loss: 0.6468326 Test Loss: 0.3808837
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 2.885082483291626
Epoch: 97, Steps: 56 | Train Loss: 0.8046229 Vali Loss: 0.6500796 Test Loss: 0.3808838
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 2.998570680618286
Epoch: 98, Steps: 56 | Train Loss: 0.8031663 Vali Loss: 0.6502570 Test Loss: 0.3808838
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 2.5162463188171387
Epoch: 99, Steps: 56 | Train Loss: 0.8039959 Vali Loss: 0.6474183 Test Loss: 0.3808838
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 2.980304718017578
Epoch: 100, Steps: 56 | Train Loss: 0.8042720 Vali Loss: 0.6528217 Test Loss: 0.3808838
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.37950071692466736, mae:0.42379194498062134, rse:0.4923931956291199, corr:[ 2.15232372e-01  2.18541518e-01  2.19582796e-01  2.18226433e-01
  2.16767758e-01  2.16022283e-01  2.15779811e-01  2.15402737e-01
  2.14356169e-01  2.12587118e-01  2.10824087e-01  2.09422171e-01
  2.08544910e-01  2.07961515e-01  2.07290053e-01  2.06328154e-01
  2.05206424e-01  2.04210684e-01  2.03576386e-01  2.03149155e-01
  2.02509671e-01  2.01426655e-01  1.99882329e-01  1.98249027e-01
  1.96933150e-01  1.96122780e-01  1.95578709e-01  1.94954067e-01
  1.94129065e-01  1.93034142e-01  1.91882342e-01  1.90836728e-01
  1.90043882e-01  1.89370513e-01  1.88684285e-01  1.87902942e-01
  1.87061444e-01  1.86290890e-01  1.85653344e-01  1.85050830e-01
  1.84373349e-01  1.83571175e-01  1.82800099e-01  1.82244584e-01
  1.81720749e-01  1.81114167e-01  1.80192903e-01  1.78705022e-01
  1.77047521e-01  1.75595075e-01  1.74497738e-01  1.73697770e-01
  1.73099756e-01  1.72521621e-01  1.71812192e-01  1.71109051e-01
  1.70516938e-01  1.69998333e-01  1.69548616e-01  1.69189423e-01
  1.68969631e-01  1.68797120e-01  1.68786272e-01  1.68982238e-01
  1.69123515e-01  1.69277832e-01  1.69340760e-01  1.69309020e-01
  1.69125050e-01  1.68869734e-01  1.68573320e-01  1.68208212e-01
  1.67811826e-01  1.67356998e-01  1.66977197e-01  1.66643694e-01
  1.66486621e-01  1.66464835e-01  1.66371286e-01  1.66167155e-01
  1.65951699e-01  1.65761977e-01  1.65641636e-01  1.65685266e-01
  1.65898219e-01  1.66149750e-01  1.66306779e-01  1.66272059e-01
  1.66059300e-01  1.65871769e-01  1.65833443e-01  1.65993005e-01
  1.66355342e-01  1.66667864e-01  1.66820660e-01  1.66661814e-01
  1.66284442e-01  1.65893570e-01  1.65571153e-01  1.65385455e-01
  1.65408209e-01  1.65335029e-01  1.65213197e-01  1.65018752e-01
  1.64888993e-01  1.64800376e-01  1.64826468e-01  1.64879948e-01
  1.64751694e-01  1.64400667e-01  1.64018258e-01  1.63762063e-01
  1.63673535e-01  1.63746163e-01  1.63817465e-01  1.63771048e-01
  1.63463801e-01  1.62796110e-01  1.62024185e-01  1.61259517e-01
  1.60585806e-01  1.60067976e-01  1.59552470e-01  1.58979326e-01
  1.58252403e-01  1.57595605e-01  1.57016486e-01  1.56601608e-01
  1.56319037e-01  1.55892044e-01  1.55409157e-01  1.54766455e-01
  1.54114351e-01  1.53465599e-01  1.52914509e-01  1.52505770e-01
  1.52001202e-01  1.51523426e-01  1.51091635e-01  1.50805190e-01
  1.50644138e-01  1.50392666e-01  1.49831176e-01  1.48883954e-01
  1.47604287e-01  1.46435484e-01  1.45615712e-01  1.45171165e-01
  1.44932419e-01  1.44740433e-01  1.44496784e-01  1.44113243e-01
  1.43632978e-01  1.43069968e-01  1.42544568e-01  1.42020836e-01
  1.41646653e-01  1.41377926e-01  1.41083986e-01  1.40724868e-01
  1.40341267e-01  1.40265018e-01  1.40548453e-01  1.41033173e-01
  1.41494036e-01  1.41559958e-01  1.41157150e-01  1.40394837e-01
  1.39623404e-01  1.39096245e-01  1.38941675e-01  1.38966843e-01
  1.38922825e-01  1.38528615e-01  1.37694106e-01  1.36663780e-01
  1.35751888e-01  1.35121882e-01  1.34658262e-01  1.34348497e-01
  1.34141997e-01  1.34022564e-01  1.34047985e-01  1.34104386e-01
  1.34195477e-01  1.34373710e-01  1.34571508e-01  1.34773254e-01
  1.35004982e-01  1.35358602e-01  1.35865852e-01  1.36315182e-01
  1.36530563e-01  1.36586368e-01  1.36513188e-01  1.36434957e-01
  1.36461258e-01  1.36468202e-01  1.36603892e-01  1.36697426e-01
  1.36659041e-01  1.36425316e-01  1.36202231e-01  1.35854319e-01
  1.35655954e-01  1.35740414e-01  1.35951683e-01  1.36229172e-01
  1.36506960e-01  1.36783943e-01  1.37013867e-01  1.37340888e-01
  1.37743905e-01  1.38048261e-01  1.38086155e-01  1.37725249e-01
  1.37154922e-01  1.36588797e-01  1.36212766e-01  1.36185676e-01
  1.36254296e-01  1.36384904e-01  1.36315569e-01  1.36239871e-01
  1.36132270e-01  1.36051044e-01  1.36100098e-01  1.36163875e-01
  1.36318073e-01  1.36477768e-01  1.36781752e-01  1.37302607e-01
  1.37956470e-01  1.38649002e-01  1.39278576e-01  1.39762431e-01
  1.40138656e-01  1.40459716e-01  1.40848815e-01  1.41348481e-01
  1.41874284e-01  1.42219037e-01  1.42365351e-01  1.42424092e-01
  1.42393008e-01  1.42325357e-01  1.42488912e-01  1.42824516e-01
  1.43282235e-01  1.43765002e-01  1.44209489e-01  1.44625068e-01
  1.45142272e-01  1.45908579e-01  1.46796018e-01  1.47776872e-01
  1.48527116e-01  1.49038926e-01  1.49435759e-01  1.49985522e-01
  1.50706425e-01  1.51509553e-01  1.52317256e-01  1.52871192e-01
  1.53202042e-01  1.53368086e-01  1.53568849e-01  1.53930202e-01
  1.54289067e-01  1.54663146e-01  1.55009270e-01  1.55304104e-01
  1.55596077e-01  1.55881405e-01  1.56261310e-01  1.56710193e-01
  1.57105848e-01  1.57693416e-01  1.58296302e-01  1.58851415e-01
  1.59407631e-01  1.59939811e-01  1.60361022e-01  1.60761312e-01
  1.61108762e-01  1.61442220e-01  1.61856979e-01  1.62179425e-01
  1.62335083e-01  1.62337884e-01  1.62174568e-01  1.62099510e-01
  1.61984310e-01  1.61867350e-01  1.61848292e-01  1.61986887e-01
  1.62219077e-01  1.62450060e-01  1.62627593e-01  1.62863150e-01
  1.63033932e-01  1.63409248e-01  1.63976043e-01  1.64508417e-01
  1.64764285e-01  1.64899454e-01  1.64942473e-01  1.65185928e-01
  1.65617198e-01  1.66113645e-01  1.66522115e-01  1.66585475e-01
  1.66441053e-01  1.66064158e-01  1.65691927e-01  1.65330514e-01
  1.65095732e-01  1.65013596e-01  1.64985478e-01  1.64928734e-01
  1.64687127e-01  1.64450571e-01  1.64318353e-01  1.64367065e-01
  1.64523408e-01  1.64707631e-01  1.65163919e-01  1.65576592e-01
  1.65900379e-01  1.66157350e-01  1.66428939e-01  1.66937619e-01
  1.67577803e-01  1.68156117e-01  1.68628722e-01  1.68858349e-01
  1.68824628e-01  1.68793678e-01  1.68956012e-01  1.69171005e-01
  1.69441104e-01  1.69541791e-01  1.69611216e-01  1.69622004e-01
  1.69558406e-01  1.69515580e-01  1.69415876e-01  1.69616863e-01
  1.70037404e-01  1.70610562e-01  1.71263635e-01  1.71921209e-01
  1.72457427e-01  1.72918692e-01  1.73298299e-01  1.73786774e-01
  1.74335971e-01  1.74869776e-01  1.75283089e-01  1.75520167e-01
  1.75598100e-01  1.75604925e-01  1.75732374e-01  1.75982237e-01
  1.76378459e-01  1.76800966e-01  1.77212194e-01  1.77601635e-01
  1.77834734e-01  1.78134039e-01  1.78241342e-01  1.78197265e-01
  1.77985400e-01  1.77790850e-01  1.77743524e-01  1.77836820e-01
  1.78029612e-01  1.78174183e-01  1.78157866e-01  1.78127304e-01
  1.78137019e-01  1.78138629e-01  1.78131744e-01  1.78212643e-01
  1.78343445e-01  1.78453460e-01  1.78607613e-01  1.78819418e-01
  1.79087952e-01  1.79373279e-01  1.79696828e-01  1.80067286e-01
  1.80422857e-01  1.80707067e-01  1.80947721e-01  1.81047395e-01
  1.81122676e-01  1.80930227e-01  1.80720389e-01  1.80533275e-01
  1.80220202e-01  1.79830581e-01  1.79447845e-01  1.79228574e-01
  1.79231748e-01  1.79317847e-01  1.79279223e-01  1.79147467e-01
  1.78799763e-01  1.78444862e-01  1.78155378e-01  1.77977040e-01
  1.77939698e-01  1.77953124e-01  1.77948311e-01  1.77996650e-01
  1.78069592e-01  1.78102732e-01  1.78002968e-01  1.77777871e-01
  1.77250311e-01  1.76499844e-01  1.75604925e-01  1.74654886e-01
  1.73691288e-01  1.72687158e-01  1.71819776e-01  1.70993090e-01
  1.70204878e-01  1.69529930e-01  1.68910578e-01  1.68473959e-01
  1.68079481e-01  1.67611957e-01  1.67114601e-01  1.66479319e-01
  1.65822431e-01  1.65303811e-01  1.64996371e-01  1.64837793e-01
  1.64483309e-01  1.64034694e-01  1.63411885e-01  1.62812859e-01
  1.62258178e-01  1.62057251e-01  1.62000328e-01  1.61967427e-01
  1.61657825e-01  1.61062807e-01  1.60354137e-01  1.59986630e-01
  1.59852400e-01  1.59965500e-01  1.60120383e-01  1.60086095e-01
  1.59825355e-01  1.59344032e-01  1.58839107e-01  1.58640385e-01
  1.58632442e-01  1.58569023e-01  1.58343300e-01  1.58024445e-01
  1.57473981e-01  1.57014534e-01  1.56703085e-01  1.56373948e-01
  1.56094909e-01  1.55706614e-01  1.55376434e-01  1.55001611e-01
  1.54656067e-01  1.54355183e-01  1.54239237e-01  1.54131487e-01
  1.53900772e-01  1.53595209e-01  1.53184891e-01  1.52745560e-01
  1.52269363e-01  1.51776373e-01  1.51340768e-01  1.50633499e-01
  1.49773732e-01  1.48944989e-01  1.48300692e-01  1.47715241e-01
  1.47208452e-01  1.46595642e-01  1.45908549e-01  1.44976810e-01
  1.44005343e-01  1.43250868e-01  1.42922670e-01  1.42838493e-01
  1.42526314e-01  1.41992256e-01  1.41299069e-01  1.40641615e-01
  1.40196249e-01  1.39889956e-01  1.39666110e-01  1.39128029e-01
  1.38096765e-01  1.36875629e-01  1.35730073e-01  1.34765491e-01
  1.34095505e-01  1.33785069e-01  1.33469090e-01  1.33062989e-01
  1.32456943e-01  1.31662145e-01  1.30860835e-01  1.30020544e-01
  1.29355326e-01  1.28820345e-01  1.28395140e-01  1.27822936e-01
  1.27053395e-01  1.26365066e-01  1.25825286e-01  1.25493959e-01
  1.25186101e-01  1.24786630e-01  1.24023899e-01  1.22815855e-01
  1.21262357e-01  1.19817823e-01  1.18778750e-01  1.17793806e-01
  1.16827644e-01  1.15798861e-01  1.14752300e-01  1.13793589e-01
  1.12922564e-01  1.12134323e-01  1.11379176e-01  1.10440187e-01
  1.09278865e-01  1.07980505e-01  1.06858529e-01  1.06020413e-01
  1.05347000e-01  1.04872756e-01  1.04234435e-01  1.03474431e-01
  1.02685437e-01  1.01752177e-01  1.00888431e-01  9.99102890e-02
  9.89225730e-02  9.78500620e-02  9.67324600e-02  9.55643952e-02
  9.44545791e-02  9.35253724e-02  9.27696899e-02  9.19531733e-02
  9.13362801e-02  9.07216519e-02  9.03246552e-02  8.99440646e-02
  8.96431208e-02  8.93582627e-02  8.90152082e-02  8.83606970e-02
  8.76596794e-02  8.69264454e-02  8.61735344e-02  8.55021477e-02
  8.48350376e-02  8.41272175e-02  8.31280947e-02  8.17683563e-02
  8.00753608e-02  7.85484985e-02  7.73240328e-02  7.63818771e-02
  7.56046399e-02  7.47457743e-02  7.37267360e-02  7.26016685e-02
  7.15842247e-02  7.08388388e-02  7.02384040e-02  6.97490647e-02
  6.90749884e-02  6.82661310e-02  6.75341338e-02  6.69679269e-02
  6.67145401e-02  6.66801780e-02  6.65139332e-02  6.61528632e-02
  6.54620081e-02  6.45177960e-02  6.32976964e-02  6.20682165e-02
  6.09822720e-02  6.01590201e-02  5.91866300e-02  5.81091046e-02
  5.66906184e-02  5.52411228e-02  5.38123176e-02  5.25233969e-02
  5.13165221e-02  5.03730774e-02  4.97983061e-02  4.91676480e-02
  4.85414974e-02  4.80386950e-02  4.78277244e-02  4.77868877e-02
  4.76627573e-02  4.76475097e-02  4.75693084e-02  4.75563146e-02
  4.75279652e-02  4.75452729e-02  4.72345613e-02  4.67570759e-02
  4.58973050e-02  4.47011180e-02  4.33898494e-02  4.22354266e-02
  4.14003059e-02  4.09691073e-02  4.06647809e-02  4.02494222e-02
  3.98167633e-02  3.95909287e-02  3.94186974e-02  3.92289199e-02
  3.87310013e-02  3.78929488e-02  3.71126570e-02  3.67283672e-02
  3.63745727e-02  3.62670459e-02  3.62703241e-02  3.60496752e-02
  3.57059464e-02  3.52827981e-02  3.44942994e-02  3.34976763e-02
  3.25096473e-02  3.18325236e-02  3.14019471e-02  3.07013150e-02
  2.98106112e-02  2.87108384e-02  2.76621170e-02  2.69996990e-02
  2.67454572e-02  2.66919062e-02  2.65998468e-02  2.62986757e-02
  2.57838871e-02  2.55040545e-02  2.58048512e-02  2.63108052e-02
  2.66996045e-02  2.66481657e-02  2.62945667e-02  2.60145068e-02
  2.58918591e-02  2.61607207e-02  2.65626516e-02  2.67331637e-02
  2.63315979e-02  2.54664496e-02  2.43733320e-02  2.29006782e-02
  2.16969345e-02  2.07339264e-02  2.04585921e-02  2.02143639e-02
  2.02196706e-02  2.02602316e-02  2.01202556e-02  1.96360610e-02
  1.91352833e-02  1.83976442e-02  1.80062708e-02  1.79805756e-02
  1.82665344e-02  1.88713260e-02  1.95945967e-02  1.96198933e-02
  1.89800691e-02  1.77246500e-02  1.62403565e-02  1.46336304e-02
  1.31266881e-02  1.17282504e-02  1.06820948e-02  9.55581386e-03
  8.53333436e-03  7.31724594e-03  6.17081532e-03  4.71039070e-03
  3.22523108e-03  2.04257132e-03  1.21710903e-03  1.04733871e-03
  1.43593387e-03  1.91142212e-03  2.58559361e-03  2.60588480e-03
  1.51562551e-03 -2.02296113e-04 -1.85919355e-03 -2.23075855e-03
 -7.34206580e-04  1.45685743e-03 -3.71932970e-06 -8.43015686e-03]
