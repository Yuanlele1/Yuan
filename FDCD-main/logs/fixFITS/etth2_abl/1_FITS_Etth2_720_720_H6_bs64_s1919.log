Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=1919, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  68841472.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.7883052825927734
Epoch: 1, Steps: 56 | Train Loss: 1.0342788 Vali Loss: 0.7779792 Test Loss: 0.4359601
Validation loss decreased (inf --> 0.777979).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.8167076110839844
Epoch: 2, Steps: 56 | Train Loss: 0.8915603 Vali Loss: 0.7301626 Test Loss: 0.4096187
Validation loss decreased (0.777979 --> 0.730163).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.7990913391113281
Epoch: 3, Steps: 56 | Train Loss: 0.8605340 Vali Loss: 0.7093629 Test Loss: 0.4000026
Validation loss decreased (0.730163 --> 0.709363).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.628042459487915
Epoch: 4, Steps: 56 | Train Loss: 0.8441442 Vali Loss: 0.6949055 Test Loss: 0.3943819
Validation loss decreased (0.709363 --> 0.694905).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.3889358043670654
Epoch: 5, Steps: 56 | Train Loss: 0.8347886 Vali Loss: 0.6855842 Test Loss: 0.3908066
Validation loss decreased (0.694905 --> 0.685584).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.6340179443359375
Epoch: 6, Steps: 56 | Train Loss: 0.8268082 Vali Loss: 0.6786059 Test Loss: 0.3881821
Validation loss decreased (0.685584 --> 0.678606).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.5783178806304932
Epoch: 7, Steps: 56 | Train Loss: 0.8240538 Vali Loss: 0.6787882 Test Loss: 0.3862981
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.8008639812469482
Epoch: 8, Steps: 56 | Train Loss: 0.8204111 Vali Loss: 0.6713347 Test Loss: 0.3847819
Validation loss decreased (0.678606 --> 0.671335).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.8544671535491943
Epoch: 9, Steps: 56 | Train Loss: 0.8171848 Vali Loss: 0.6699058 Test Loss: 0.3836331
Validation loss decreased (0.671335 --> 0.669906).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.6046910285949707
Epoch: 10, Steps: 56 | Train Loss: 0.8155822 Vali Loss: 0.6675920 Test Loss: 0.3826660
Validation loss decreased (0.669906 --> 0.667592).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.208834648132324
Epoch: 11, Steps: 56 | Train Loss: 0.8122536 Vali Loss: 0.6654144 Test Loss: 0.3820673
Validation loss decreased (0.667592 --> 0.665414).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.6053259372711182
Epoch: 12, Steps: 56 | Train Loss: 0.8113620 Vali Loss: 0.6628954 Test Loss: 0.3813993
Validation loss decreased (0.665414 --> 0.662895).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.611616849899292
Epoch: 13, Steps: 56 | Train Loss: 0.8113940 Vali Loss: 0.6562365 Test Loss: 0.3810149
Validation loss decreased (0.662895 --> 0.656237).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.5934979915618896
Epoch: 14, Steps: 56 | Train Loss: 0.8099840 Vali Loss: 0.6603295 Test Loss: 0.3806180
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.5826735496520996
Epoch: 15, Steps: 56 | Train Loss: 0.8076023 Vali Loss: 0.6548520 Test Loss: 0.3802156
Validation loss decreased (0.656237 --> 0.654852).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.5312120914459229
Epoch: 16, Steps: 56 | Train Loss: 0.8084746 Vali Loss: 0.6585994 Test Loss: 0.3800798
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.5377528667449951
Epoch: 17, Steps: 56 | Train Loss: 0.8057643 Vali Loss: 0.6579707 Test Loss: 0.3798810
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.5720148086547852
Epoch: 18, Steps: 56 | Train Loss: 0.8077139 Vali Loss: 0.6563508 Test Loss: 0.3797494
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.5411372184753418
Epoch: 19, Steps: 56 | Train Loss: 0.8073431 Vali Loss: 0.6537559 Test Loss: 0.3795910
Validation loss decreased (0.654852 --> 0.653756).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.5994391441345215
Epoch: 20, Steps: 56 | Train Loss: 0.8060060 Vali Loss: 0.6551821 Test Loss: 0.3794905
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.7811315059661865
Epoch: 21, Steps: 56 | Train Loss: 0.8041848 Vali Loss: 0.6539996 Test Loss: 0.3794094
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.6212599277496338
Epoch: 22, Steps: 56 | Train Loss: 0.8053618 Vali Loss: 0.6511846 Test Loss: 0.3793451
Validation loss decreased (0.653756 --> 0.651185).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.9018945693969727
Epoch: 23, Steps: 56 | Train Loss: 0.8043117 Vali Loss: 0.6533489 Test Loss: 0.3792597
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.6604902744293213
Epoch: 24, Steps: 56 | Train Loss: 0.8035810 Vali Loss: 0.6510628 Test Loss: 0.3792079
Validation loss decreased (0.651185 --> 0.651063).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.8185813426971436
Epoch: 25, Steps: 56 | Train Loss: 0.8032707 Vali Loss: 0.6510004 Test Loss: 0.3791653
Validation loss decreased (0.651063 --> 0.651000).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.6740727424621582
Epoch: 26, Steps: 56 | Train Loss: 0.8029521 Vali Loss: 0.6508559 Test Loss: 0.3791339
Validation loss decreased (0.651000 --> 0.650856).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.4862077236175537
Epoch: 27, Steps: 56 | Train Loss: 0.8029552 Vali Loss: 0.6479636 Test Loss: 0.3791299
Validation loss decreased (0.650856 --> 0.647964).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.5327751636505127
Epoch: 28, Steps: 56 | Train Loss: 0.8021924 Vali Loss: 0.6524553 Test Loss: 0.3791523
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.5800745487213135
Epoch: 29, Steps: 56 | Train Loss: 0.8018918 Vali Loss: 0.6498427 Test Loss: 0.3790943
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.6501004695892334
Epoch: 30, Steps: 56 | Train Loss: 0.8017306 Vali Loss: 0.6516359 Test Loss: 0.3790917
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.5669965744018555
Epoch: 31, Steps: 56 | Train Loss: 0.8015086 Vali Loss: 0.6525530 Test Loss: 0.3790721
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.7822856903076172
Epoch: 32, Steps: 56 | Train Loss: 0.8032939 Vali Loss: 0.6482378 Test Loss: 0.3790375
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.514092206954956
Epoch: 33, Steps: 56 | Train Loss: 0.8023189 Vali Loss: 0.6470721 Test Loss: 0.3790571
Validation loss decreased (0.647964 --> 0.647072).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.7822685241699219
Epoch: 34, Steps: 56 | Train Loss: 0.8013013 Vali Loss: 0.6521933 Test Loss: 0.3790494
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.8637566566467285
Epoch: 35, Steps: 56 | Train Loss: 0.8001428 Vali Loss: 0.6506850 Test Loss: 0.3790544
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.6439659595489502
Epoch: 36, Steps: 56 | Train Loss: 0.8010101 Vali Loss: 0.6494235 Test Loss: 0.3790438
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.6266982555389404
Epoch: 37, Steps: 56 | Train Loss: 0.8022045 Vali Loss: 0.6508633 Test Loss: 0.3790391
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.5338313579559326
Epoch: 38, Steps: 56 | Train Loss: 0.8015687 Vali Loss: 0.6534756 Test Loss: 0.3790263
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.7227141857147217
Epoch: 39, Steps: 56 | Train Loss: 0.7989643 Vali Loss: 0.6492400 Test Loss: 0.3790317
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.8738212585449219
Epoch: 40, Steps: 56 | Train Loss: 0.8013900 Vali Loss: 0.6525636 Test Loss: 0.3790274
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.660984992980957
Epoch: 41, Steps: 56 | Train Loss: 0.8014405 Vali Loss: 0.6506829 Test Loss: 0.3790135
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.2768852710723877
Epoch: 42, Steps: 56 | Train Loss: 0.8010524 Vali Loss: 0.6485916 Test Loss: 0.3790152
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.577392816543579
Epoch: 43, Steps: 56 | Train Loss: 0.8014421 Vali Loss: 0.6469097 Test Loss: 0.3790145
Validation loss decreased (0.647072 --> 0.646910).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.8048131465911865
Epoch: 44, Steps: 56 | Train Loss: 0.7998621 Vali Loss: 0.6457088 Test Loss: 0.3790128
Validation loss decreased (0.646910 --> 0.645709).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.5819282531738281
Epoch: 45, Steps: 56 | Train Loss: 0.8007051 Vali Loss: 0.6526264 Test Loss: 0.3790188
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.6512234210968018
Epoch: 46, Steps: 56 | Train Loss: 0.8007307 Vali Loss: 0.6495377 Test Loss: 0.3790247
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.5971479415893555
Epoch: 47, Steps: 56 | Train Loss: 0.8006071 Vali Loss: 0.6508414 Test Loss: 0.3789991
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.6274096965789795
Epoch: 48, Steps: 56 | Train Loss: 0.8007586 Vali Loss: 0.6453937 Test Loss: 0.3790128
Validation loss decreased (0.645709 --> 0.645394).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.5799612998962402
Epoch: 49, Steps: 56 | Train Loss: 0.8005080 Vali Loss: 0.6481465 Test Loss: 0.3790202
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.5697860717773438
Epoch: 50, Steps: 56 | Train Loss: 0.8014629 Vali Loss: 0.6471578 Test Loss: 0.3790201
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.499406337738037
Epoch: 51, Steps: 56 | Train Loss: 0.8005406 Vali Loss: 0.6477649 Test Loss: 0.3790226
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.4947059154510498
Epoch: 52, Steps: 56 | Train Loss: 0.7990145 Vali Loss: 0.6470172 Test Loss: 0.3790205
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.5427472591400146
Epoch: 53, Steps: 56 | Train Loss: 0.7985448 Vali Loss: 0.6490708 Test Loss: 0.3790169
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.5002524852752686
Epoch: 54, Steps: 56 | Train Loss: 0.7999617 Vali Loss: 0.6469738 Test Loss: 0.3790230
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.6008985042572021
Epoch: 55, Steps: 56 | Train Loss: 0.8002206 Vali Loss: 0.6475456 Test Loss: 0.3790367
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.9109981060028076
Epoch: 56, Steps: 56 | Train Loss: 0.7991801 Vali Loss: 0.6471322 Test Loss: 0.3790320
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.7831392288208008
Epoch: 57, Steps: 56 | Train Loss: 0.8008911 Vali Loss: 0.6493279 Test Loss: 0.3790203
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.7310080528259277
Epoch: 58, Steps: 56 | Train Loss: 0.7986589 Vali Loss: 0.6473192 Test Loss: 0.3790168
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.5752465724945068
Epoch: 59, Steps: 56 | Train Loss: 0.8014790 Vali Loss: 0.6452366 Test Loss: 0.3790249
Validation loss decreased (0.645394 --> 0.645237).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.085814952850342
Epoch: 60, Steps: 56 | Train Loss: 0.7990130 Vali Loss: 0.6438725 Test Loss: 0.3790270
Validation loss decreased (0.645237 --> 0.643872).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.7772836685180664
Epoch: 61, Steps: 56 | Train Loss: 0.8012382 Vali Loss: 0.6482201 Test Loss: 0.3790280
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.7910866737365723
Epoch: 62, Steps: 56 | Train Loss: 0.7997937 Vali Loss: 0.6469051 Test Loss: 0.3790222
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.559408187866211
Epoch: 63, Steps: 56 | Train Loss: 0.7998242 Vali Loss: 0.6479283 Test Loss: 0.3790234
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.6860673427581787
Epoch: 64, Steps: 56 | Train Loss: 0.8004801 Vali Loss: 0.6428574 Test Loss: 0.3790287
Validation loss decreased (0.643872 --> 0.642857).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.7614428997039795
Epoch: 65, Steps: 56 | Train Loss: 0.7997268 Vali Loss: 0.6474729 Test Loss: 0.3790271
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.5124285221099854
Epoch: 66, Steps: 56 | Train Loss: 0.7996807 Vali Loss: 0.6492466 Test Loss: 0.3790299
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.5997416973114014
Epoch: 67, Steps: 56 | Train Loss: 0.7988865 Vali Loss: 0.6453466 Test Loss: 0.3790323
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.734828233718872
Epoch: 68, Steps: 56 | Train Loss: 0.7999268 Vali Loss: 0.6468899 Test Loss: 0.3790345
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.7943789958953857
Epoch: 69, Steps: 56 | Train Loss: 0.7992716 Vali Loss: 0.6480498 Test Loss: 0.3790332
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.5427920818328857
Epoch: 70, Steps: 56 | Train Loss: 0.7990978 Vali Loss: 0.6474640 Test Loss: 0.3790389
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.5637452602386475
Epoch: 71, Steps: 56 | Train Loss: 0.8002973 Vali Loss: 0.6472118 Test Loss: 0.3790358
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.584031343460083
Epoch: 72, Steps: 56 | Train Loss: 0.8005496 Vali Loss: 0.6463640 Test Loss: 0.3790343
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.5485680103302002
Epoch: 73, Steps: 56 | Train Loss: 0.8009075 Vali Loss: 0.6443734 Test Loss: 0.3790369
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.7046513557434082
Epoch: 74, Steps: 56 | Train Loss: 0.7997558 Vali Loss: 0.6467977 Test Loss: 0.3790363
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.556774616241455
Epoch: 75, Steps: 56 | Train Loss: 0.8003713 Vali Loss: 0.6470210 Test Loss: 0.3790410
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.617295265197754
Epoch: 76, Steps: 56 | Train Loss: 0.7992824 Vali Loss: 0.6475261 Test Loss: 0.3790359
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.6004743576049805
Epoch: 77, Steps: 56 | Train Loss: 0.7995942 Vali Loss: 0.6485144 Test Loss: 0.3790356
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.7771172523498535
Epoch: 78, Steps: 56 | Train Loss: 0.8000810 Vali Loss: 0.6489820 Test Loss: 0.3790370
EarlyStopping counter: 14 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.5646312236785889
Epoch: 79, Steps: 56 | Train Loss: 0.8000981 Vali Loss: 0.6469591 Test Loss: 0.3790388
EarlyStopping counter: 15 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.5133695602416992
Epoch: 80, Steps: 56 | Train Loss: 0.8004507 Vali Loss: 0.6460954 Test Loss: 0.3790385
EarlyStopping counter: 16 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.6354756355285645
Epoch: 81, Steps: 56 | Train Loss: 0.8005108 Vali Loss: 0.6477642 Test Loss: 0.3790384
EarlyStopping counter: 17 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.586022138595581
Epoch: 82, Steps: 56 | Train Loss: 0.8006398 Vali Loss: 0.6491938 Test Loss: 0.3790387
EarlyStopping counter: 18 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.5513722896575928
Epoch: 83, Steps: 56 | Train Loss: 0.7986958 Vali Loss: 0.6487724 Test Loss: 0.3790411
EarlyStopping counter: 19 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.573920726776123
Epoch: 84, Steps: 56 | Train Loss: 0.8006081 Vali Loss: 0.6510389 Test Loss: 0.3790399
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.3776319622993469, mae:0.42285215854644775, rse:0.49117934703826904, corr:[ 0.21604806  0.22029373  0.21796939  0.21919525  0.21905062  0.21714313
  0.21700132  0.21712953  0.21541134  0.21380377  0.21326473  0.21221927
  0.21052583  0.20938352  0.2088278   0.20811819  0.20747726  0.20698254
  0.20613217  0.20492451  0.20404561  0.20350336  0.20241113  0.2010493
  0.19969922  0.19861338  0.19746892  0.19646671  0.19583307  0.19527642
  0.19451506  0.19350946  0.19273895  0.19194868  0.1909728   0.1898937
  0.18913038  0.18866765  0.1878641   0.18687242  0.18621725  0.18591064
  0.18553178  0.1849023   0.18413731  0.18358189  0.18280628  0.18130377
  0.17964241  0.17838609  0.17751592  0.17676997  0.1759415   0.1751043
  0.17464392  0.17430179  0.17350641  0.17275082  0.1725693   0.17234367
  0.17160045  0.17123823  0.17179845  0.17222132  0.17202136  0.17224252
  0.1726757   0.17250021  0.17203394  0.17206202  0.17212245  0.17157495
  0.1710204   0.17084102  0.17052084  0.16983308  0.1696285   0.16979824
  0.16949855  0.16911045  0.16923887  0.1693745   0.16897316  0.16868983
  0.169009    0.16931662  0.16918914  0.16913247  0.16945508  0.1695738
  0.16923253  0.16915083  0.16953646  0.16967556  0.16957203  0.16955723
  0.16957103  0.16929354  0.16895352  0.16886182  0.16878934  0.1683604
  0.16811772  0.16810346  0.16796297  0.167651    0.16782671  0.16815448
  0.16785924  0.16728742  0.16726765  0.16742788  0.16714627  0.16690168
  0.167038    0.16703776  0.16651239  0.16599514  0.16586415  0.16536468
  0.16430739  0.16355713  0.16314438  0.16238499  0.16149282  0.161232
  0.16121198  0.16076605  0.16018137  0.15965661  0.15906556  0.15827124
  0.15773468  0.15734835  0.15682128  0.15634637  0.15613678  0.15600912
  0.15534773  0.15475684  0.15486203  0.1549942   0.15443021  0.15331642
  0.15215167  0.15126769  0.15040979  0.14976586  0.14947905  0.14924869
  0.1488355   0.14846233  0.14833127  0.14801688  0.1474163   0.14662746
  0.1458996   0.14554033  0.14550868  0.1453046   0.14477293  0.14474152
  0.14514631  0.1452741   0.14522341  0.14550917  0.14587836  0.14526543
  0.14408964  0.1434676   0.14358415  0.14344619  0.14285718  0.14241856
  0.14215782  0.14148314  0.14059342  0.14010835  0.13971418  0.13917123
  0.13862252  0.13841921  0.13847457  0.13839579  0.13836321  0.1385517
  0.13868167  0.13894235  0.1394124   0.13985892  0.14026369  0.14051315
  0.14061677  0.14056711  0.14048027  0.14073187  0.14110503  0.14113308
  0.14113528  0.14113072  0.14103253  0.14077462  0.14070186  0.14055976
  0.14024387  0.14014076  0.14032891  0.14056788  0.14068604  0.14110489
  0.14175218  0.1420832   0.14203013  0.1421897   0.1425584   0.14241394
  0.14180274  0.14140196  0.14135125  0.14114317  0.14072266  0.14074278
  0.14084867  0.14085346  0.14084113  0.14107601  0.14123671  0.1411242
  0.14122482  0.1414639   0.14158493  0.14190577  0.14269784  0.14346938
  0.14375676  0.14408363  0.14489396  0.14554787  0.14566846  0.14584808
  0.14652328  0.14702162  0.14706881  0.14712165  0.14732146  0.1474412
  0.14744066  0.14760566  0.14825766  0.1489753   0.14945427  0.14982809
  0.15028451  0.15095891  0.1516981   0.15249315  0.15319014  0.15388495
  0.15450515  0.1550395   0.15554467  0.15622398  0.15690695  0.15730925
  0.15774125  0.15825917  0.15876314  0.15901208  0.15908149  0.15919527
  0.15930274  0.15950273  0.1598926   0.16038813  0.16092525  0.16136596
  0.1615735   0.16197668  0.16272487  0.16355264  0.16401     0.1642624
  0.16477907  0.16543879  0.16570349  0.16584344  0.16633505  0.16682445
  0.16691515  0.16675666  0.16675806  0.16687018  0.16679499  0.16674092
  0.16682453  0.16680269  0.16680202  0.16723241  0.16771752  0.1676568
  0.16742015  0.1678811   0.16865611  0.16901536  0.16923197  0.16977347
  0.17001846  0.16993701  0.17002055  0.17054571  0.17096666  0.17088771
  0.1707317   0.17065355  0.17051692  0.17014232  0.16981503  0.16961277
  0.16951458  0.1695891   0.16960992  0.16947612  0.1692312   0.16929099
  0.16935831  0.16924067  0.16958852  0.17035724  0.17102322  0.171182
  0.17127933  0.17184359  0.17225586  0.17237684  0.17278379  0.17326865
  0.17330652  0.1731755   0.17335579  0.17360444  0.1737264   0.17366195
  0.1737404   0.1737846   0.17371006  0.17367369  0.17365418  0.17388454
  0.17411977  0.1745573   0.17509536  0.17556417  0.17607582  0.17674406
  0.17726469  0.17766933  0.17801881  0.17842041  0.17867409  0.17893495
  0.17936315  0.17967589  0.17966662  0.17957246  0.17970434  0.17993014
  0.18036367  0.18103828  0.18136545  0.18127656  0.180945    0.1808575
  0.18080264  0.18069696  0.18078716  0.1811525   0.18135148  0.18108526
  0.18086217  0.1809963   0.18116449  0.1810321   0.1809264   0.18116857
  0.18138662  0.18131158  0.18115461  0.18125822  0.18170124  0.1821426
  0.18236236  0.18256643  0.18295826  0.18334402  0.18355137  0.18367824
  0.18384004  0.18347163  0.18303205  0.1830144   0.18299417  0.18256998
  0.18212625  0.18217286  0.18229689  0.18212849  0.1817952   0.18160407
  0.1812707   0.1808929   0.18076216  0.18084545  0.18079643  0.18046962
  0.18033953  0.18056507  0.18065332  0.18033937  0.18000242  0.17987908
  0.1794424   0.17888737  0.1784061   0.17780209  0.17680976  0.17563018
  0.17458467  0.17351548  0.17282817  0.17251158  0.17196302  0.17121847
  0.17060861  0.17028806  0.16997407  0.16935286  0.16892521  0.16862378
  0.16810298  0.16773129  0.16742283  0.16695973  0.16604097  0.1655334
  0.16539995  0.16515033  0.16462095  0.16454127  0.16460973  0.16412097
  0.16344073  0.16328555  0.16315871  0.16277163  0.16253072  0.162654
  0.16271634  0.16235813  0.1620116   0.16192846  0.1616042   0.16106538
  0.1608889   0.16095795  0.16058595  0.16020069  0.15986809  0.15926439
  0.15879823  0.15855588  0.15841621  0.15791689  0.15738161  0.15714522
  0.15703103  0.15664281  0.15623125  0.15627527  0.15625933  0.1557532
  0.15497544  0.15434326  0.15380445  0.15310232  0.15249941  0.15189426
  0.15128097  0.15067725  0.15001754  0.14909407  0.14838241  0.14797574
  0.14736547  0.14644232  0.14581193  0.14561394  0.14508107  0.14434418
  0.14394689  0.14376909  0.1433813   0.14283574  0.14251554  0.1419764
  0.14080897  0.13959298  0.13880858  0.13818045  0.13750997  0.13706796
  0.13662596  0.13597788  0.13518646  0.1347024   0.13436548  0.13334325
  0.13198721  0.13125832  0.13121222  0.13086982  0.13023396  0.12990186
  0.12947044  0.12871487  0.12799272  0.12759188  0.12689327  0.12566192
  0.12429322  0.12332417  0.12250778  0.12125758  0.12004263  0.11899158
  0.11808702  0.11719899  0.11630601  0.11531121  0.11422464  0.11321943
  0.1122321   0.11120696  0.11038713  0.10955305  0.10848059  0.10759071
  0.10686304  0.1064082   0.10594901  0.10515215  0.10432424  0.10317329
  0.10186411  0.10061796  0.09959082  0.09857545  0.09767602  0.09701703
  0.09626248  0.09506434  0.09423782  0.09383492  0.09359945  0.09303199
  0.09251943  0.09236068  0.0921021   0.09122866  0.09052424  0.09014682
  0.08958503  0.08879077  0.08812669  0.08758272  0.0864904   0.08487931
  0.08331263  0.08231626  0.08140773  0.08038311  0.07938338  0.07834676
  0.07734621  0.07640189  0.07539175  0.07438026  0.07350788  0.07290973
  0.07207513  0.07119366  0.07072918  0.07044625  0.06999928  0.06938364
  0.06901996  0.06900816  0.06852042  0.06755476  0.06661075  0.06584881
  0.06471629  0.06331795  0.06199243  0.06112627  0.05997503  0.05870786
  0.05741615  0.05614746  0.0548508   0.05391097  0.05336015  0.05261591
  0.051934    0.05151889  0.05131364  0.05108962  0.05090743  0.05111277
  0.05112798  0.05109145  0.05119206  0.05143267  0.05098929  0.05021333
  0.049404    0.04863933  0.04783211  0.04686244  0.04586338  0.04526474
  0.04491871  0.0445493   0.04403639  0.04357624  0.04315555  0.0429536
  0.04250839  0.04184441  0.04170597  0.04164548  0.040698    0.03992086
  0.03981021  0.03950344  0.03889734  0.03879192  0.03872367  0.03805628
  0.03668746  0.03560659  0.03512863  0.03441405  0.03359715  0.03268761
  0.03183259  0.03121342  0.03096853  0.03094573  0.03073015  0.03049463
  0.03039964  0.03036456  0.03025201  0.03022047  0.03075867  0.03098633
  0.03058598  0.0306341   0.03121976  0.03154073  0.03090747  0.03019654
  0.02982218  0.02944601  0.02888853  0.02800764  0.02727497  0.02620692
  0.02567399  0.02500006  0.0245909   0.0245731   0.02482277  0.02453444
  0.02356373  0.02275938  0.02335666  0.02351838  0.02250591  0.02252066
  0.02388547  0.02345844  0.02173242  0.02117642  0.02166686  0.02022547
  0.01723251  0.01514236  0.01438966  0.01293118  0.01135313  0.010283
  0.00973495  0.00887101  0.00826118  0.00742395  0.00587951  0.00578775
  0.00660485  0.00520745  0.00379361  0.00487144  0.00576733  0.00322842
  0.00185714  0.00460238  0.0031488  -0.00338769 -0.00111213  0.01073564]
