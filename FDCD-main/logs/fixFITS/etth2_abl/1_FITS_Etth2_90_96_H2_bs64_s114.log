Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=18, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_90_96', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=96, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_90_96_FITS_ETTh2_ftM_sl90_ll48_pl96_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8455
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=18, out_features=37, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  596736.0
params:  703.0
Trainable parameters:  703
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.5536129474639893
Epoch: 1, Steps: 66 | Train Loss: 0.6114063 Vali Loss: 0.2923399 Test Loss: 0.3927628
Validation loss decreased (inf --> 0.292340).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.7414321899414062
Epoch: 2, Steps: 66 | Train Loss: 0.5334239 Vali Loss: 0.2611449 Test Loss: 0.3514200
Validation loss decreased (0.292340 --> 0.261145).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.2120697498321533
Epoch: 3, Steps: 66 | Train Loss: 0.4959714 Vali Loss: 0.2456293 Test Loss: 0.3305125
Validation loss decreased (0.261145 --> 0.245629).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.4958505630493164
Epoch: 4, Steps: 66 | Train Loss: 0.4758616 Vali Loss: 0.2377431 Test Loss: 0.3193328
Validation loss decreased (0.245629 --> 0.237743).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.2019600868225098
Epoch: 5, Steps: 66 | Train Loss: 0.4645132 Vali Loss: 0.2327045 Test Loss: 0.3128098
Validation loss decreased (0.237743 --> 0.232704).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.507117748260498
Epoch: 6, Steps: 66 | Train Loss: 0.4574067 Vali Loss: 0.2299915 Test Loss: 0.3087564
Validation loss decreased (0.232704 --> 0.229991).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.7796144485473633
Epoch: 7, Steps: 66 | Train Loss: 0.4526449 Vali Loss: 0.2265476 Test Loss: 0.3060828
Validation loss decreased (0.229991 --> 0.226548).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.9404399394989014
Epoch: 8, Steps: 66 | Train Loss: 0.4490417 Vali Loss: 0.2257071 Test Loss: 0.3041886
Validation loss decreased (0.226548 --> 0.225707).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.409851312637329
Epoch: 9, Steps: 66 | Train Loss: 0.4462807 Vali Loss: 0.2233896 Test Loss: 0.3027455
Validation loss decreased (0.225707 --> 0.223390).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.0428102016448975
Epoch: 10, Steps: 66 | Train Loss: 0.4436304 Vali Loss: 0.2219677 Test Loss: 0.3016597
Validation loss decreased (0.223390 --> 0.221968).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.6618833541870117
Epoch: 11, Steps: 66 | Train Loss: 0.4423766 Vali Loss: 0.2228723 Test Loss: 0.3007130
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.0462045669555664
Epoch: 12, Steps: 66 | Train Loss: 0.4410935 Vali Loss: 0.2207558 Test Loss: 0.2999851
Validation loss decreased (0.221968 --> 0.220756).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.6594557762145996
Epoch: 13, Steps: 66 | Train Loss: 0.4398770 Vali Loss: 0.2210264 Test Loss: 0.2993351
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.3846757411956787
Epoch: 14, Steps: 66 | Train Loss: 0.4387501 Vali Loss: 0.2206562 Test Loss: 0.2987643
Validation loss decreased (0.220756 --> 0.220656).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.4271905422210693
Epoch: 15, Steps: 66 | Train Loss: 0.4361741 Vali Loss: 0.2185200 Test Loss: 0.2982908
Validation loss decreased (0.220656 --> 0.218520).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.565049648284912
Epoch: 16, Steps: 66 | Train Loss: 0.4366798 Vali Loss: 0.2181617 Test Loss: 0.2978298
Validation loss decreased (0.218520 --> 0.218162).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.3858163356781006
Epoch: 17, Steps: 66 | Train Loss: 0.4355306 Vali Loss: 0.2195586 Test Loss: 0.2974731
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.0995404720306396
Epoch: 18, Steps: 66 | Train Loss: 0.4354574 Vali Loss: 0.2178194 Test Loss: 0.2971094
Validation loss decreased (0.218162 --> 0.217819).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.4955389499664307
Epoch: 19, Steps: 66 | Train Loss: 0.4346113 Vali Loss: 0.2176772 Test Loss: 0.2968246
Validation loss decreased (0.217819 --> 0.217677).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.7284033298492432
Epoch: 20, Steps: 66 | Train Loss: 0.4340127 Vali Loss: 0.2187959 Test Loss: 0.2965451
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.768646240234375
Epoch: 21, Steps: 66 | Train Loss: 0.4336330 Vali Loss: 0.2174529 Test Loss: 0.2963019
Validation loss decreased (0.217677 --> 0.217453).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.917276382446289
Epoch: 22, Steps: 66 | Train Loss: 0.4331228 Vali Loss: 0.2177127 Test Loss: 0.2960728
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.8540840148925781
Epoch: 23, Steps: 66 | Train Loss: 0.4321963 Vali Loss: 0.2177201 Test Loss: 0.2958458
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.3434388637542725
Epoch: 24, Steps: 66 | Train Loss: 0.4322639 Vali Loss: 0.2167375 Test Loss: 0.2956590
Validation loss decreased (0.217453 --> 0.216738).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.7974472045898438
Epoch: 25, Steps: 66 | Train Loss: 0.4318224 Vali Loss: 0.2162641 Test Loss: 0.2954990
Validation loss decreased (0.216738 --> 0.216264).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.8201923370361328
Epoch: 26, Steps: 66 | Train Loss: 0.4309166 Vali Loss: 0.2170509 Test Loss: 0.2953489
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.6505308151245117
Epoch: 27, Steps: 66 | Train Loss: 0.4312928 Vali Loss: 0.2178368 Test Loss: 0.2952068
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.421194314956665
Epoch: 28, Steps: 66 | Train Loss: 0.4309493 Vali Loss: 0.2168869 Test Loss: 0.2950712
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.6022529602050781
Epoch: 29, Steps: 66 | Train Loss: 0.4304065 Vali Loss: 0.2162884 Test Loss: 0.2949461
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.6842198371887207
Epoch: 30, Steps: 66 | Train Loss: 0.4301979 Vali Loss: 0.2162034 Test Loss: 0.2948418
Validation loss decreased (0.216264 --> 0.216203).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.0601694583892822
Epoch: 31, Steps: 66 | Train Loss: 0.4300851 Vali Loss: 0.2160412 Test Loss: 0.2947300
Validation loss decreased (0.216203 --> 0.216041).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.407008409500122
Epoch: 32, Steps: 66 | Train Loss: 0.4298991 Vali Loss: 0.2160055 Test Loss: 0.2946272
Validation loss decreased (0.216041 --> 0.216006).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.1390554904937744
Epoch: 33, Steps: 66 | Train Loss: 0.4297860 Vali Loss: 0.2171121 Test Loss: 0.2945360
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.4260034561157227
Epoch: 34, Steps: 66 | Train Loss: 0.4289854 Vali Loss: 0.2158474 Test Loss: 0.2944669
Validation loss decreased (0.216006 --> 0.215847).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.2571146488189697
Epoch: 35, Steps: 66 | Train Loss: 0.4294094 Vali Loss: 0.2151055 Test Loss: 0.2943850
Validation loss decreased (0.215847 --> 0.215106).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.8108649253845215
Epoch: 36, Steps: 66 | Train Loss: 0.4293002 Vali Loss: 0.2169373 Test Loss: 0.2943057
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.5933837890625
Epoch: 37, Steps: 66 | Train Loss: 0.4290509 Vali Loss: 0.2158047 Test Loss: 0.2942446
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.8607618808746338
Epoch: 38, Steps: 66 | Train Loss: 0.4288246 Vali Loss: 0.2159552 Test Loss: 0.2941927
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.6019351482391357
Epoch: 39, Steps: 66 | Train Loss: 0.4288214 Vali Loss: 0.2146529 Test Loss: 0.2941163
Validation loss decreased (0.215106 --> 0.214653).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.6233818531036377
Epoch: 40, Steps: 66 | Train Loss: 0.4286191 Vali Loss: 0.2143742 Test Loss: 0.2940762
Validation loss decreased (0.214653 --> 0.214374).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.5794439315795898
Epoch: 41, Steps: 66 | Train Loss: 0.4285267 Vali Loss: 0.2159721 Test Loss: 0.2940146
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.7160451412200928
Epoch: 42, Steps: 66 | Train Loss: 0.4284511 Vali Loss: 0.2154564 Test Loss: 0.2939710
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.5916388034820557
Epoch: 43, Steps: 66 | Train Loss: 0.4283503 Vali Loss: 0.2154656 Test Loss: 0.2939231
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.2905123233795166
Epoch: 44, Steps: 66 | Train Loss: 0.4282805 Vali Loss: 0.2155596 Test Loss: 0.2938828
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.828094482421875
Epoch: 45, Steps: 66 | Train Loss: 0.4280624 Vali Loss: 0.2159256 Test Loss: 0.2938494
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.1559627056121826
Epoch: 46, Steps: 66 | Train Loss: 0.4280342 Vali Loss: 0.2144613 Test Loss: 0.2938065
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.341217279434204
Epoch: 47, Steps: 66 | Train Loss: 0.4280037 Vali Loss: 0.2141482 Test Loss: 0.2937755
Validation loss decreased (0.214374 --> 0.214148).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.128261089324951
Epoch: 48, Steps: 66 | Train Loss: 0.4277455 Vali Loss: 0.2150060 Test Loss: 0.2937427
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.102391481399536
Epoch: 49, Steps: 66 | Train Loss: 0.4274496 Vali Loss: 0.2159238 Test Loss: 0.2937093
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.9577138423919678
Epoch: 50, Steps: 66 | Train Loss: 0.4277915 Vali Loss: 0.2160758 Test Loss: 0.2936820
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.57271409034729
Epoch: 51, Steps: 66 | Train Loss: 0.4275278 Vali Loss: 0.2153981 Test Loss: 0.2936513
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.092794179916382
Epoch: 52, Steps: 66 | Train Loss: 0.4273540 Vali Loss: 0.2140408 Test Loss: 0.2936313
Validation loss decreased (0.214148 --> 0.214041).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.2824690341949463
Epoch: 53, Steps: 66 | Train Loss: 0.4262676 Vali Loss: 0.2154523 Test Loss: 0.2936024
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.568751335144043
Epoch: 54, Steps: 66 | Train Loss: 0.4264005 Vali Loss: 0.2153979 Test Loss: 0.2935781
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.9936130046844482
Epoch: 55, Steps: 66 | Train Loss: 0.4274463 Vali Loss: 0.2157063 Test Loss: 0.2935599
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.652949571609497
Epoch: 56, Steps: 66 | Train Loss: 0.4273656 Vali Loss: 0.2153961 Test Loss: 0.2935367
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.5414109230041504
Epoch: 57, Steps: 66 | Train Loss: 0.4273452 Vali Loss: 0.2150506 Test Loss: 0.2935226
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.497957229614258
Epoch: 58, Steps: 66 | Train Loss: 0.4271340 Vali Loss: 0.2151859 Test Loss: 0.2935054
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.6094415187835693
Epoch: 59, Steps: 66 | Train Loss: 0.4272133 Vali Loss: 0.2149017 Test Loss: 0.2934859
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.9550836086273193
Epoch: 60, Steps: 66 | Train Loss: 0.4272348 Vali Loss: 0.2148574 Test Loss: 0.2934704
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.330350875854492
Epoch: 61, Steps: 66 | Train Loss: 0.4269864 Vali Loss: 0.2152546 Test Loss: 0.2934546
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.512068748474121
Epoch: 62, Steps: 66 | Train Loss: 0.4269933 Vali Loss: 0.2140442 Test Loss: 0.2934433
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.4504735469818115
Epoch: 63, Steps: 66 | Train Loss: 0.4265799 Vali Loss: 0.2152521 Test Loss: 0.2934302
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.51352858543396
Epoch: 64, Steps: 66 | Train Loss: 0.4268746 Vali Loss: 0.2145207 Test Loss: 0.2934155
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.562849760055542
Epoch: 65, Steps: 66 | Train Loss: 0.4270723 Vali Loss: 0.2147442 Test Loss: 0.2934017
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.6842677593231201
Epoch: 66, Steps: 66 | Train Loss: 0.4269088 Vali Loss: 0.2136611 Test Loss: 0.2933917
Validation loss decreased (0.214041 --> 0.213661).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.7465753555297852
Epoch: 67, Steps: 66 | Train Loss: 0.4267206 Vali Loss: 0.2148364 Test Loss: 0.2933827
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.4786262512207031
Epoch: 68, Steps: 66 | Train Loss: 0.4269509 Vali Loss: 0.2147179 Test Loss: 0.2933742
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.5069236755371094
Epoch: 69, Steps: 66 | Train Loss: 0.4269112 Vali Loss: 0.2144256 Test Loss: 0.2933619
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.5654206275939941
Epoch: 70, Steps: 66 | Train Loss: 0.4268055 Vali Loss: 0.2147252 Test Loss: 0.2933526
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.6818585395812988
Epoch: 71, Steps: 66 | Train Loss: 0.4268346 Vali Loss: 0.2161481 Test Loss: 0.2933455
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.1878597736358643
Epoch: 72, Steps: 66 | Train Loss: 0.4269008 Vali Loss: 0.2147730 Test Loss: 0.2933350
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.9949939250946045
Epoch: 73, Steps: 66 | Train Loss: 0.4265386 Vali Loss: 0.2145945 Test Loss: 0.2933289
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.7698180675506592
Epoch: 74, Steps: 66 | Train Loss: 0.4269036 Vali Loss: 0.2149287 Test Loss: 0.2933213
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.386199474334717
Epoch: 75, Steps: 66 | Train Loss: 0.4266985 Vali Loss: 0.2159197 Test Loss: 0.2933147
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.8025143146514893
Epoch: 76, Steps: 66 | Train Loss: 0.4267074 Vali Loss: 0.2149458 Test Loss: 0.2933077
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.376333236694336
Epoch: 77, Steps: 66 | Train Loss: 0.4265874 Vali Loss: 0.2138248 Test Loss: 0.2933017
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.4016013145446777
Epoch: 78, Steps: 66 | Train Loss: 0.4267785 Vali Loss: 0.2147625 Test Loss: 0.2932948
EarlyStopping counter: 12 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.8245656490325928
Epoch: 79, Steps: 66 | Train Loss: 0.4268519 Vali Loss: 0.2147984 Test Loss: 0.2932903
EarlyStopping counter: 13 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.919079065322876
Epoch: 80, Steps: 66 | Train Loss: 0.4268005 Vali Loss: 0.2148441 Test Loss: 0.2932837
EarlyStopping counter: 14 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.6491854190826416
Epoch: 81, Steps: 66 | Train Loss: 0.4258008 Vali Loss: 0.2147502 Test Loss: 0.2932787
EarlyStopping counter: 15 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.520850658416748
Epoch: 82, Steps: 66 | Train Loss: 0.4266422 Vali Loss: 0.2148634 Test Loss: 0.2932742
EarlyStopping counter: 16 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 2.092366933822632
Epoch: 83, Steps: 66 | Train Loss: 0.4264985 Vali Loss: 0.2140948 Test Loss: 0.2932692
EarlyStopping counter: 17 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.5871782302856445
Epoch: 84, Steps: 66 | Train Loss: 0.4267441 Vali Loss: 0.2142729 Test Loss: 0.2932643
EarlyStopping counter: 18 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 2.6948201656341553
Epoch: 85, Steps: 66 | Train Loss: 0.4266040 Vali Loss: 0.2146755 Test Loss: 0.2932615
EarlyStopping counter: 19 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.1408519744873047
Epoch: 86, Steps: 66 | Train Loss: 0.4265812 Vali Loss: 0.2160109 Test Loss: 0.2932566
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_90_96_FITS_ETTh2_ftM_sl90_ll48_pl96_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.2938891053199768, mae:0.3411276936531067, rse:0.43689092993736267, corr:[0.27302563 0.27786255 0.27607164 0.27357024 0.27284428 0.27246624
 0.27123198 0.26967725 0.26891366 0.26837832 0.26719874 0.26526305
 0.2634981  0.26256472 0.26192096 0.26117882 0.2603183  0.25964487
 0.25904816 0.25814182 0.2569746  0.25566265 0.2541179  0.25206232
 0.24923299 0.24667524 0.24446328 0.24279638 0.24146011 0.24011064
 0.23885177 0.23739073 0.23622882 0.2351778  0.23406468 0.23263727
 0.23123047 0.23025918 0.22947797 0.22870146 0.22812329 0.22776516
 0.22736837 0.22648363 0.2256043  0.22479214 0.22352664 0.22124407
 0.21775852 0.2149527  0.21291353 0.211423   0.20988378 0.20799682
 0.2061323  0.20398891 0.20275983 0.20180309 0.20090188 0.1997629
 0.19886507 0.1988873  0.19897114 0.19853984 0.19786634 0.19755147
 0.19741653 0.19675    0.19593592 0.19528429 0.19448873 0.19305257
 0.19036503 0.18831745 0.1869377  0.18622378 0.1855896  0.18495104
 0.18431751 0.18302464 0.18269159 0.18285733 0.18269368 0.18191892
 0.18116432 0.18128467 0.18140057 0.180878   0.18004303 0.1799476
 0.17984526 0.17808847 0.17641081 0.17747541 0.1800698  0.17586674]
