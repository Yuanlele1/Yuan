Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_360_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_360_336_FITS_ETTh2_ftM_sl360_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7945
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=106, out_features=204, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  19375104.0
params:  21828.0
Trainable parameters:  21828
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.6138954162597656
Epoch: 1, Steps: 62 | Train Loss: 0.8013074 Vali Loss: 0.4688650 Test Loss: 0.4070460
Validation loss decreased (inf --> 0.468865).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.946096658706665
Epoch: 2, Steps: 62 | Train Loss: 0.6845023 Vali Loss: 0.4326655 Test Loss: 0.3832845
Validation loss decreased (0.468865 --> 0.432665).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.940833330154419
Epoch: 3, Steps: 62 | Train Loss: 0.6548059 Vali Loss: 0.4141432 Test Loss: 0.3770275
Validation loss decreased (0.432665 --> 0.414143).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.6977405548095703
Epoch: 4, Steps: 62 | Train Loss: 0.6419738 Vali Loss: 0.4053873 Test Loss: 0.3739743
Validation loss decreased (0.414143 --> 0.405387).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.043208122253418
Epoch: 5, Steps: 62 | Train Loss: 0.6344054 Vali Loss: 0.3992199 Test Loss: 0.3718913
Validation loss decreased (0.405387 --> 0.399220).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.9973640441894531
Epoch: 6, Steps: 62 | Train Loss: 0.6290726 Vali Loss: 0.3946350 Test Loss: 0.3700668
Validation loss decreased (0.399220 --> 0.394635).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.5992932319641113
Epoch: 7, Steps: 62 | Train Loss: 0.6247463 Vali Loss: 0.3923421 Test Loss: 0.3688810
Validation loss decreased (0.394635 --> 0.392342).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.648761510848999
Epoch: 8, Steps: 62 | Train Loss: 0.6216762 Vali Loss: 0.3865946 Test Loss: 0.3676032
Validation loss decreased (0.392342 --> 0.386595).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.8801872730255127
Epoch: 9, Steps: 62 | Train Loss: 0.6196145 Vali Loss: 0.3864267 Test Loss: 0.3667443
Validation loss decreased (0.386595 --> 0.386427).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.7333672046661377
Epoch: 10, Steps: 62 | Train Loss: 0.6182245 Vali Loss: 0.3855075 Test Loss: 0.3660227
Validation loss decreased (0.386427 --> 0.385507).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.9560739994049072
Epoch: 11, Steps: 62 | Train Loss: 0.6169564 Vali Loss: 0.3833376 Test Loss: 0.3654526
Validation loss decreased (0.385507 --> 0.383338).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.4303925037384033
Epoch: 12, Steps: 62 | Train Loss: 0.6156859 Vali Loss: 0.3826160 Test Loss: 0.3649518
Validation loss decreased (0.383338 --> 0.382616).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.9939768314361572
Epoch: 13, Steps: 62 | Train Loss: 0.6135890 Vali Loss: 0.3822825 Test Loss: 0.3647124
Validation loss decreased (0.382616 --> 0.382283).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.8026397228240967
Epoch: 14, Steps: 62 | Train Loss: 0.6134787 Vali Loss: 0.3832387 Test Loss: 0.3644095
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.366819381713867
Epoch: 15, Steps: 62 | Train Loss: 0.6131928 Vali Loss: 0.3811961 Test Loss: 0.3640824
Validation loss decreased (0.382283 --> 0.381196).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.083348035812378
Epoch: 16, Steps: 62 | Train Loss: 0.6125344 Vali Loss: 0.3815962 Test Loss: 0.3638556
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.9996387958526611
Epoch: 17, Steps: 62 | Train Loss: 0.6117747 Vali Loss: 0.3807507 Test Loss: 0.3637417
Validation loss decreased (0.381196 --> 0.380751).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.9538967609405518
Epoch: 18, Steps: 62 | Train Loss: 0.6115279 Vali Loss: 0.3811531 Test Loss: 0.3636574
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.4039454460144043
Epoch: 19, Steps: 62 | Train Loss: 0.6109627 Vali Loss: 0.3785191 Test Loss: 0.3635868
Validation loss decreased (0.380751 --> 0.378519).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.5153131484985352
Epoch: 20, Steps: 62 | Train Loss: 0.6101509 Vali Loss: 0.3788307 Test Loss: 0.3634396
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.8243505954742432
Epoch: 21, Steps: 62 | Train Loss: 0.6102466 Vali Loss: 0.3784506 Test Loss: 0.3633767
Validation loss decreased (0.378519 --> 0.378451).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.7106897830963135
Epoch: 22, Steps: 62 | Train Loss: 0.6102527 Vali Loss: 0.3778961 Test Loss: 0.3632674
Validation loss decreased (0.378451 --> 0.377896).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.9269921779632568
Epoch: 23, Steps: 62 | Train Loss: 0.6097909 Vali Loss: 0.3769848 Test Loss: 0.3631306
Validation loss decreased (0.377896 --> 0.376985).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.7366719245910645
Epoch: 24, Steps: 62 | Train Loss: 0.6096087 Vali Loss: 0.3770174 Test Loss: 0.3631855
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.1533164978027344
Epoch: 25, Steps: 62 | Train Loss: 0.6087355 Vali Loss: 0.3766939 Test Loss: 0.3631314
Validation loss decreased (0.376985 --> 0.376694).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.7782793045043945
Epoch: 26, Steps: 62 | Train Loss: 0.6092111 Vali Loss: 0.3764074 Test Loss: 0.3630961
Validation loss decreased (0.376694 --> 0.376407).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.782109260559082
Epoch: 27, Steps: 62 | Train Loss: 0.6090271 Vali Loss: 0.3791058 Test Loss: 0.3629559
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.6688663959503174
Epoch: 28, Steps: 62 | Train Loss: 0.6088487 Vali Loss: 0.3757786 Test Loss: 0.3629505
Validation loss decreased (0.376407 --> 0.375779).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.078031063079834
Epoch: 29, Steps: 62 | Train Loss: 0.6078523 Vali Loss: 0.3765869 Test Loss: 0.3629522
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.9675517082214355
Epoch: 30, Steps: 62 | Train Loss: 0.6084919 Vali Loss: 0.3780724 Test Loss: 0.3628370
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.046818971633911
Epoch: 31, Steps: 62 | Train Loss: 0.6083270 Vali Loss: 0.3761511 Test Loss: 0.3628709
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.647693395614624
Epoch: 32, Steps: 62 | Train Loss: 0.6081271 Vali Loss: 0.3773409 Test Loss: 0.3628446
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.357469320297241
Epoch: 33, Steps: 62 | Train Loss: 0.6078727 Vali Loss: 0.3747511 Test Loss: 0.3628439
Validation loss decreased (0.375779 --> 0.374751).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.549312114715576
Epoch: 34, Steps: 62 | Train Loss: 0.6071376 Vali Loss: 0.3762755 Test Loss: 0.3627605
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.9713499546051025
Epoch: 35, Steps: 62 | Train Loss: 0.6078487 Vali Loss: 0.3773395 Test Loss: 0.3627958
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.8949320316314697
Epoch: 36, Steps: 62 | Train Loss: 0.6080206 Vali Loss: 0.3753769 Test Loss: 0.3627677
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.678030014038086
Epoch: 37, Steps: 62 | Train Loss: 0.6076022 Vali Loss: 0.3769403 Test Loss: 0.3627005
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.9292070865631104
Epoch: 38, Steps: 62 | Train Loss: 0.6076736 Vali Loss: 0.3747106 Test Loss: 0.3627483
Validation loss decreased (0.374751 --> 0.374711).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.5150146484375
Epoch: 39, Steps: 62 | Train Loss: 0.6064112 Vali Loss: 0.3754690 Test Loss: 0.3627264
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.432877540588379
Epoch: 40, Steps: 62 | Train Loss: 0.6077563 Vali Loss: 0.3743040 Test Loss: 0.3627237
Validation loss decreased (0.374711 --> 0.374304).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.2845256328582764
Epoch: 41, Steps: 62 | Train Loss: 0.6063161 Vali Loss: 0.3733013 Test Loss: 0.3626807
Validation loss decreased (0.374304 --> 0.373301).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 3.3355016708374023
Epoch: 42, Steps: 62 | Train Loss: 0.6060464 Vali Loss: 0.3739098 Test Loss: 0.3626392
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.56091046333313
Epoch: 43, Steps: 62 | Train Loss: 0.6072444 Vali Loss: 0.3730122 Test Loss: 0.3626544
Validation loss decreased (0.373301 --> 0.373012).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.062586545944214
Epoch: 44, Steps: 62 | Train Loss: 0.6066810 Vali Loss: 0.3726766 Test Loss: 0.3626609
Validation loss decreased (0.373012 --> 0.372677).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.522838830947876
Epoch: 45, Steps: 62 | Train Loss: 0.6071924 Vali Loss: 0.3746890 Test Loss: 0.3626546
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.6959187984466553
Epoch: 46, Steps: 62 | Train Loss: 0.6069787 Vali Loss: 0.3759473 Test Loss: 0.3626385
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.0532283782958984
Epoch: 47, Steps: 62 | Train Loss: 0.6066289 Vali Loss: 0.3741401 Test Loss: 0.3626052
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.171348810195923
Epoch: 48, Steps: 62 | Train Loss: 0.6072258 Vali Loss: 0.3738436 Test Loss: 0.3626100
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.5650782585144043
Epoch: 49, Steps: 62 | Train Loss: 0.6070865 Vali Loss: 0.3750017 Test Loss: 0.3625706
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.8810021877288818
Epoch: 50, Steps: 62 | Train Loss: 0.6070812 Vali Loss: 0.3742711 Test Loss: 0.3626043
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.0146982669830322
Epoch: 51, Steps: 62 | Train Loss: 0.6062872 Vali Loss: 0.3745836 Test Loss: 0.3626126
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.600235939025879
Epoch: 52, Steps: 62 | Train Loss: 0.6068340 Vali Loss: 0.3765822 Test Loss: 0.3625782
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.6128160953521729
Epoch: 53, Steps: 62 | Train Loss: 0.6068534 Vali Loss: 0.3732337 Test Loss: 0.3625918
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.5642640590667725
Epoch: 54, Steps: 62 | Train Loss: 0.6058121 Vali Loss: 0.3741223 Test Loss: 0.3625519
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.459486484527588
Epoch: 55, Steps: 62 | Train Loss: 0.6053764 Vali Loss: 0.3734081 Test Loss: 0.3625681
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.7777295112609863
Epoch: 56, Steps: 62 | Train Loss: 0.6064424 Vali Loss: 0.3757208 Test Loss: 0.3625599
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.095759868621826
Epoch: 57, Steps: 62 | Train Loss: 0.6068074 Vali Loss: 0.3750325 Test Loss: 0.3625380
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.303359270095825
Epoch: 58, Steps: 62 | Train Loss: 0.6054837 Vali Loss: 0.3727338 Test Loss: 0.3625503
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.022082805633545
Epoch: 59, Steps: 62 | Train Loss: 0.6063871 Vali Loss: 0.3748089 Test Loss: 0.3625358
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.120743989944458
Epoch: 60, Steps: 62 | Train Loss: 0.6063550 Vali Loss: 0.3758546 Test Loss: 0.3625380
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.638397455215454
Epoch: 61, Steps: 62 | Train Loss: 0.6063465 Vali Loss: 0.3750823 Test Loss: 0.3625448
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.5764367580413818
Epoch: 62, Steps: 62 | Train Loss: 0.6059594 Vali Loss: 0.3725813 Test Loss: 0.3625317
Validation loss decreased (0.372677 --> 0.372581).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.1951677799224854
Epoch: 63, Steps: 62 | Train Loss: 0.6062435 Vali Loss: 0.3742970 Test Loss: 0.3625401
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.7859017848968506
Epoch: 64, Steps: 62 | Train Loss: 0.6061178 Vali Loss: 0.3733601 Test Loss: 0.3625180
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.447112798690796
Epoch: 65, Steps: 62 | Train Loss: 0.6058983 Vali Loss: 0.3747682 Test Loss: 0.3625138
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.6599812507629395
Epoch: 66, Steps: 62 | Train Loss: 0.6064483 Vali Loss: 0.3737423 Test Loss: 0.3625210
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.440420150756836
Epoch: 67, Steps: 62 | Train Loss: 0.6057877 Vali Loss: 0.3737309 Test Loss: 0.3625225
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.502159833908081
Epoch: 68, Steps: 62 | Train Loss: 0.6059178 Vali Loss: 0.3745476 Test Loss: 0.3625085
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.662536859512329
Epoch: 69, Steps: 62 | Train Loss: 0.6066388 Vali Loss: 0.3754384 Test Loss: 0.3625122
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.7114381790161133
Epoch: 70, Steps: 62 | Train Loss: 0.6059605 Vali Loss: 0.3737500 Test Loss: 0.3625017
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.0076262950897217
Epoch: 71, Steps: 62 | Train Loss: 0.6060305 Vali Loss: 0.3751214 Test Loss: 0.3625178
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.5307087898254395
Epoch: 72, Steps: 62 | Train Loss: 0.6053157 Vali Loss: 0.3767043 Test Loss: 0.3625084
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.687349557876587
Epoch: 73, Steps: 62 | Train Loss: 0.6064909 Vali Loss: 0.3722476 Test Loss: 0.3624983
Validation loss decreased (0.372581 --> 0.372248).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.034367799758911
Epoch: 74, Steps: 62 | Train Loss: 0.6066449 Vali Loss: 0.3759186 Test Loss: 0.3625003
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.834547758102417
Epoch: 75, Steps: 62 | Train Loss: 0.6056654 Vali Loss: 0.3728246 Test Loss: 0.3624963
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.419180154800415
Epoch: 76, Steps: 62 | Train Loss: 0.6061687 Vali Loss: 0.3759113 Test Loss: 0.3624984
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.6383144855499268
Epoch: 77, Steps: 62 | Train Loss: 0.6065816 Vali Loss: 0.3730928 Test Loss: 0.3624979
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.9443483352661133
Epoch: 78, Steps: 62 | Train Loss: 0.6064271 Vali Loss: 0.3739626 Test Loss: 0.3624904
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.6781339645385742
Epoch: 79, Steps: 62 | Train Loss: 0.6062873 Vali Loss: 0.3766194 Test Loss: 0.3624834
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.8402633666992188
Epoch: 80, Steps: 62 | Train Loss: 0.6063853 Vali Loss: 0.3746225 Test Loss: 0.3624878
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.6052916049957275
Epoch: 81, Steps: 62 | Train Loss: 0.6053830 Vali Loss: 0.3761651 Test Loss: 0.3624851
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.6462287902832031
Epoch: 82, Steps: 62 | Train Loss: 0.6063731 Vali Loss: 0.3762949 Test Loss: 0.3624862
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.576108694076538
Epoch: 83, Steps: 62 | Train Loss: 0.6056733 Vali Loss: 0.3743613 Test Loss: 0.3624863
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.1342995166778564
Epoch: 84, Steps: 62 | Train Loss: 0.6059574 Vali Loss: 0.3726896 Test Loss: 0.3624822
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 2.0699784755706787
Epoch: 85, Steps: 62 | Train Loss: 0.6059480 Vali Loss: 0.3731822 Test Loss: 0.3624817
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.471095085144043
Epoch: 86, Steps: 62 | Train Loss: 0.6059489 Vali Loss: 0.3743415 Test Loss: 0.3624803
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.7386598587036133
Epoch: 87, Steps: 62 | Train Loss: 0.6062738 Vali Loss: 0.3738463 Test Loss: 0.3624809
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.2520108222961426
Epoch: 88, Steps: 62 | Train Loss: 0.6056771 Vali Loss: 0.3750289 Test Loss: 0.3624832
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 2.9475183486938477
Epoch: 89, Steps: 62 | Train Loss: 0.6054845 Vali Loss: 0.3740367 Test Loss: 0.3624781
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 3.978288412094116
Epoch: 90, Steps: 62 | Train Loss: 0.6063510 Vali Loss: 0.3716605 Test Loss: 0.3624809
Validation loss decreased (0.372248 --> 0.371661).  Saving model ...
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 4.013016223907471
Epoch: 91, Steps: 62 | Train Loss: 0.6056387 Vali Loss: 0.3726657 Test Loss: 0.3624804
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 3.345094919204712
Epoch: 92, Steps: 62 | Train Loss: 0.6057356 Vali Loss: 0.3724611 Test Loss: 0.3624808
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 2.1353373527526855
Epoch: 93, Steps: 62 | Train Loss: 0.6059079 Vali Loss: 0.3742053 Test Loss: 0.3624783
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 2.03498911857605
Epoch: 94, Steps: 62 | Train Loss: 0.6064210 Vali Loss: 0.3755179 Test Loss: 0.3624763
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.9622750282287598
Epoch: 95, Steps: 62 | Train Loss: 0.6051029 Vali Loss: 0.3734262 Test Loss: 0.3624791
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 2.162302017211914
Epoch: 96, Steps: 62 | Train Loss: 0.6060140 Vali Loss: 0.3742724 Test Loss: 0.3624757
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.6129937171936035
Epoch: 97, Steps: 62 | Train Loss: 0.6062040 Vali Loss: 0.3745866 Test Loss: 0.3624761
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 1.8028264045715332
Epoch: 98, Steps: 62 | Train Loss: 0.6056178 Vali Loss: 0.3743361 Test Loss: 0.3624770
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 1.4807231426239014
Epoch: 99, Steps: 62 | Train Loss: 0.6063322 Vali Loss: 0.3722224 Test Loss: 0.3624749
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 1.7795071601867676
Epoch: 100, Steps: 62 | Train Loss: 0.6062331 Vali Loss: 0.3745078 Test Loss: 0.3624720
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh2_360_336_FITS_ETTh2_ftM_sl360_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.3580106496810913, mae:0.3967081904411316, rse:0.478395938873291, corr:[0.26094607 0.2655347  0.26271498 0.26437888 0.26248264 0.26059958
 0.26086575 0.25944737 0.25749883 0.25688463 0.25567627 0.25380012
 0.2528273  0.25176632 0.25059047 0.2500989  0.24959196 0.24833879
 0.24737056 0.24663271 0.24525051 0.24414346 0.24321193 0.24153066
 0.23924516 0.23787725 0.23677346 0.23552518 0.23456582 0.23362425
 0.23231821 0.23096004 0.22969958 0.22838311 0.22734317 0.22658055
 0.22518717 0.22384848 0.22312582 0.22218595 0.22075765 0.21983442
 0.21919546 0.21779889 0.21623066 0.21545404 0.21402423 0.21153195
 0.20961213 0.2081457  0.20620416 0.20464776 0.20348598 0.20156816
 0.19942753 0.19819729 0.19699474 0.1951623  0.1941858  0.19372074
 0.19269347 0.19207403 0.19239596 0.19205067 0.19096538 0.1905772
 0.19024505 0.18936817 0.18881854 0.18890408 0.18814321 0.1867049
 0.1859712  0.18556073 0.1846875  0.18385768 0.18334717 0.18259744
 0.1819867  0.18169056 0.18115847 0.18057214 0.18075944 0.18066107
 0.1799503  0.17984827 0.18024881 0.17968586 0.17884885 0.1787937
 0.17857607 0.17793846 0.17817335 0.17864884 0.17822126 0.17748462
 0.17715894 0.17625867 0.17508882 0.17474477 0.17456035 0.17358509
 0.17310873 0.1734511  0.17336841 0.17265211 0.1728372  0.17318277
 0.17238502 0.17184862 0.17190342 0.17127408 0.17030327 0.17044687
 0.17037256 0.16940157 0.1690775  0.1689074  0.16765253 0.16611299
 0.16532537 0.16444416 0.16346619 0.16308193 0.16259521 0.1614374
 0.1608181  0.16089754 0.16004413 0.15893619 0.15917075 0.15918158
 0.15794435 0.15730773 0.15771563 0.15730801 0.15645304 0.15659782
 0.15647328 0.15567152 0.1555753  0.15584384 0.154946   0.15325846
 0.15221739 0.15130182 0.1501761  0.14979014 0.14957291 0.14868657
 0.1480958  0.14798483 0.14753805 0.146918   0.14699094 0.14676012
 0.14568025 0.14547086 0.14602326 0.1458415  0.14520746 0.14541131
 0.14544266 0.14464402 0.14465089 0.14532906 0.14485364 0.14350711
 0.14288555 0.1421879  0.14108531 0.14056723 0.14004503 0.13872282
 0.13778196 0.13742916 0.13678005 0.13593459 0.13576965 0.1354426
 0.13469504 0.13450874 0.13477114 0.13455974 0.13421951 0.13455105
 0.13472895 0.13448708 0.13503912 0.13596296 0.13577504 0.13525999
 0.13524581 0.13467309 0.13392042 0.13399115 0.1339531  0.13312037
 0.13266654 0.1327177  0.132306   0.13198715 0.13249014 0.13237649
 0.13148506 0.13118334 0.13148825 0.1314235  0.13118196 0.13172053
 0.1320059  0.131737   0.13193148 0.1322717  0.13193941 0.13122615
 0.13070542 0.12952235 0.12846664 0.12826042 0.12797451 0.12724613
 0.1268673  0.12727682 0.12716919 0.12672949 0.12672172 0.12615742
 0.12499564 0.12441609 0.12445321 0.12437422 0.12438026 0.12494722
 0.12511657 0.12512179 0.1256413  0.12614797 0.12588324 0.1254535
 0.12518711 0.12443796 0.12337025 0.12295384 0.12278061 0.12169334
 0.12091189 0.12103129 0.12119369 0.12114366 0.12148695 0.12150718
 0.12113608 0.12164642 0.1224381  0.12327931 0.12400954 0.12482393
 0.12510835 0.12528025 0.12597178 0.12644754 0.12678975 0.12704389
 0.12721623 0.12667271 0.12634659 0.12658677 0.12637632 0.12581919
 0.12575161 0.1262085  0.1260579  0.1259663  0.12682828 0.1265798
 0.12580672 0.12574339 0.12605453 0.12605646 0.12608297 0.12643498
 0.12630977 0.12607607 0.1259313  0.12606315 0.12611356 0.12631993
 0.12595469 0.1249712  0.12399828 0.12346403 0.12285846 0.12209182
 0.12106916 0.12017114 0.11973008 0.11929403 0.11938416 0.1191036
 0.11799238 0.11746743 0.11784367 0.11830124 0.11801721 0.11853853
 0.11841086 0.11804261 0.11837057 0.11848983 0.11843549 0.11784998
 0.11722279 0.11584897 0.11447012 0.11406251 0.11346276 0.11201716
 0.11089982 0.11136363 0.11143824 0.111083   0.11129616 0.11114832
 0.11046568 0.1107922  0.11140412 0.11078375 0.11089613 0.11209936
 0.11055592 0.10948557 0.11251608 0.11209211 0.11080065 0.12297536]
