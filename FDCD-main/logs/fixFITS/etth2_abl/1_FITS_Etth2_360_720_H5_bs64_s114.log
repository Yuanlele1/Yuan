Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_360_720_FITS_ETTh2_ftM_sl360_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=90, out_features=270, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  21772800.0
params:  24570.0
Trainable parameters:  24570
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.183957099914551
Epoch: 1, Steps: 59 | Train Loss: 1.0619331 Vali Loss: 0.7689750 Test Loss: 0.4781142
Validation loss decreased (inf --> 0.768975).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.222673177719116
Epoch: 2, Steps: 59 | Train Loss: 0.9016059 Vali Loss: 0.7163435 Test Loss: 0.4310753
Validation loss decreased (0.768975 --> 0.716343).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.51947021484375
Epoch: 3, Steps: 59 | Train Loss: 0.8538165 Vali Loss: 0.6928714 Test Loss: 0.4148292
Validation loss decreased (0.716343 --> 0.692871).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.2180402278900146
Epoch: 4, Steps: 59 | Train Loss: 0.8343407 Vali Loss: 0.6809736 Test Loss: 0.4074031
Validation loss decreased (0.692871 --> 0.680974).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.8035755157470703
Epoch: 5, Steps: 59 | Train Loss: 0.8257946 Vali Loss: 0.6721892 Test Loss: 0.4031611
Validation loss decreased (0.680974 --> 0.672189).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.335590362548828
Epoch: 6, Steps: 59 | Train Loss: 0.8211314 Vali Loss: 0.6664406 Test Loss: 0.4002777
Validation loss decreased (0.672189 --> 0.666441).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.01712965965271
Epoch: 7, Steps: 59 | Train Loss: 0.8159712 Vali Loss: 0.6627315 Test Loss: 0.3981903
Validation loss decreased (0.666441 --> 0.662731).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.8727121353149414
Epoch: 8, Steps: 59 | Train Loss: 0.8141258 Vali Loss: 0.6612968 Test Loss: 0.3965232
Validation loss decreased (0.662731 --> 0.661297).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.9554708003997803
Epoch: 9, Steps: 59 | Train Loss: 0.8110218 Vali Loss: 0.6589379 Test Loss: 0.3952157
Validation loss decreased (0.661297 --> 0.658938).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.7362020015716553
Epoch: 10, Steps: 59 | Train Loss: 0.8094259 Vali Loss: 0.6563246 Test Loss: 0.3941002
Validation loss decreased (0.658938 --> 0.656325).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.5619356632232666
Epoch: 11, Steps: 59 | Train Loss: 0.8082190 Vali Loss: 0.6533011 Test Loss: 0.3931079
Validation loss decreased (0.656325 --> 0.653301).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.5047385692596436
Epoch: 12, Steps: 59 | Train Loss: 0.8067554 Vali Loss: 0.6519306 Test Loss: 0.3923952
Validation loss decreased (0.653301 --> 0.651931).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.2798521518707275
Epoch: 13, Steps: 59 | Train Loss: 0.8059471 Vali Loss: 0.6513385 Test Loss: 0.3917411
Validation loss decreased (0.651931 --> 0.651338).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.527376174926758
Epoch: 14, Steps: 59 | Train Loss: 0.8048556 Vali Loss: 0.6513318 Test Loss: 0.3912058
Validation loss decreased (0.651338 --> 0.651332).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.056216239929199
Epoch: 15, Steps: 59 | Train Loss: 0.8043877 Vali Loss: 0.6497653 Test Loss: 0.3907582
Validation loss decreased (0.651332 --> 0.649765).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.0042660236358643
Epoch: 16, Steps: 59 | Train Loss: 0.8036056 Vali Loss: 0.6523111 Test Loss: 0.3903437
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.10049843788147
Epoch: 17, Steps: 59 | Train Loss: 0.8024287 Vali Loss: 0.6494609 Test Loss: 0.3899685
Validation loss decreased (0.649765 --> 0.649461).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.181293487548828
Epoch: 18, Steps: 59 | Train Loss: 0.8016765 Vali Loss: 0.6493465 Test Loss: 0.3897074
Validation loss decreased (0.649461 --> 0.649347).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.052266836166382
Epoch: 19, Steps: 59 | Train Loss: 0.8017464 Vali Loss: 0.6470155 Test Loss: 0.3895130
Validation loss decreased (0.649347 --> 0.647016).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.7821264266967773
Epoch: 20, Steps: 59 | Train Loss: 0.8003020 Vali Loss: 0.6454672 Test Loss: 0.3892564
Validation loss decreased (0.647016 --> 0.645467).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.92484974861145
Epoch: 21, Steps: 59 | Train Loss: 0.8008119 Vali Loss: 0.6438853 Test Loss: 0.3891231
Validation loss decreased (0.645467 --> 0.643885).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.948392391204834
Epoch: 22, Steps: 59 | Train Loss: 0.8007246 Vali Loss: 0.6425789 Test Loss: 0.3889055
Validation loss decreased (0.643885 --> 0.642579).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.2603394985198975
Epoch: 23, Steps: 59 | Train Loss: 0.8006214 Vali Loss: 0.6471316 Test Loss: 0.3887640
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.852648973464966
Epoch: 24, Steps: 59 | Train Loss: 0.7999590 Vali Loss: 0.6448594 Test Loss: 0.3886659
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.2896242141723633
Epoch: 25, Steps: 59 | Train Loss: 0.7988841 Vali Loss: 0.6426543 Test Loss: 0.3885556
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.758836269378662
Epoch: 26, Steps: 59 | Train Loss: 0.7999755 Vali Loss: 0.6439950 Test Loss: 0.3884737
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 5.78252387046814
Epoch: 27, Steps: 59 | Train Loss: 0.7987979 Vali Loss: 0.6422359 Test Loss: 0.3883424
Validation loss decreased (0.642579 --> 0.642236).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 4.872222900390625
Epoch: 28, Steps: 59 | Train Loss: 0.7996875 Vali Loss: 0.6448154 Test Loss: 0.3882468
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.277338743209839
Epoch: 29, Steps: 59 | Train Loss: 0.7988084 Vali Loss: 0.6436577 Test Loss: 0.3881749
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.6786890029907227
Epoch: 30, Steps: 59 | Train Loss: 0.7991503 Vali Loss: 0.6493685 Test Loss: 0.3881028
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.8087761402130127
Epoch: 31, Steps: 59 | Train Loss: 0.7985009 Vali Loss: 0.6428888 Test Loss: 0.3881080
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.286763906478882
Epoch: 32, Steps: 59 | Train Loss: 0.7988632 Vali Loss: 0.6447147 Test Loss: 0.3880383
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.4864091873168945
Epoch: 33, Steps: 59 | Train Loss: 0.7988175 Vali Loss: 0.6418979 Test Loss: 0.3879849
Validation loss decreased (0.642236 --> 0.641898).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.139289379119873
Epoch: 34, Steps: 59 | Train Loss: 0.7974582 Vali Loss: 0.6410557 Test Loss: 0.3879337
Validation loss decreased (0.641898 --> 0.641056).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.5783157348632812
Epoch: 35, Steps: 59 | Train Loss: 0.7986494 Vali Loss: 0.6418296 Test Loss: 0.3879058
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.0359697341918945
Epoch: 36, Steps: 59 | Train Loss: 0.7987046 Vali Loss: 0.6433477 Test Loss: 0.3878554
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.1138670444488525
Epoch: 37, Steps: 59 | Train Loss: 0.7976387 Vali Loss: 0.6405010 Test Loss: 0.3878340
Validation loss decreased (0.641056 --> 0.640501).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.724468946456909
Epoch: 38, Steps: 59 | Train Loss: 0.7981312 Vali Loss: 0.6452142 Test Loss: 0.3878186
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.946488380432129
Epoch: 39, Steps: 59 | Train Loss: 0.7979894 Vali Loss: 0.6453160 Test Loss: 0.3877596
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.9192898273468018
Epoch: 40, Steps: 59 | Train Loss: 0.7979922 Vali Loss: 0.6417730 Test Loss: 0.3877383
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.4499425888061523
Epoch: 41, Steps: 59 | Train Loss: 0.7980061 Vali Loss: 0.6438993 Test Loss: 0.3877173
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.4281678199768066
Epoch: 42, Steps: 59 | Train Loss: 0.7978398 Vali Loss: 0.6424350 Test Loss: 0.3876839
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.416670083999634
Epoch: 43, Steps: 59 | Train Loss: 0.7980390 Vali Loss: 0.6417649 Test Loss: 0.3876984
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.141479015350342
Epoch: 44, Steps: 59 | Train Loss: 0.7979780 Vali Loss: 0.6412679 Test Loss: 0.3876405
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 3.0608179569244385
Epoch: 45, Steps: 59 | Train Loss: 0.7979062 Vali Loss: 0.6411086 Test Loss: 0.3876434
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 3.3605308532714844
Epoch: 46, Steps: 59 | Train Loss: 0.7974427 Vali Loss: 0.6416155 Test Loss: 0.3876268
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.8270585536956787
Epoch: 47, Steps: 59 | Train Loss: 0.7976802 Vali Loss: 0.6433073 Test Loss: 0.3876087
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.6201839447021484
Epoch: 48, Steps: 59 | Train Loss: 0.7980142 Vali Loss: 0.6394122 Test Loss: 0.3876078
Validation loss decreased (0.640501 --> 0.639412).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.963361978530884
Epoch: 49, Steps: 59 | Train Loss: 0.7971936 Vali Loss: 0.6447364 Test Loss: 0.3875776
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.8595027923583984
Epoch: 50, Steps: 59 | Train Loss: 0.7971617 Vali Loss: 0.6444743 Test Loss: 0.3875749
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.434386730194092
Epoch: 51, Steps: 59 | Train Loss: 0.7974382 Vali Loss: 0.6445737 Test Loss: 0.3875617
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.4095942974090576
Epoch: 52, Steps: 59 | Train Loss: 0.7976092 Vali Loss: 0.6415187 Test Loss: 0.3875436
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.2839341163635254
Epoch: 53, Steps: 59 | Train Loss: 0.7974220 Vali Loss: 0.6409911 Test Loss: 0.3875358
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.4283931255340576
Epoch: 54, Steps: 59 | Train Loss: 0.7970651 Vali Loss: 0.6386827 Test Loss: 0.3875406
Validation loss decreased (0.639412 --> 0.638683).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 3.0764365196228027
Epoch: 55, Steps: 59 | Train Loss: 0.7968579 Vali Loss: 0.6376097 Test Loss: 0.3875141
Validation loss decreased (0.638683 --> 0.637610).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.7026119232177734
Epoch: 56, Steps: 59 | Train Loss: 0.7975895 Vali Loss: 0.6420048 Test Loss: 0.3875067
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 3.4813380241394043
Epoch: 57, Steps: 59 | Train Loss: 0.7975586 Vali Loss: 0.6397341 Test Loss: 0.3874969
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 3.1488289833068848
Epoch: 58, Steps: 59 | Train Loss: 0.7974838 Vali Loss: 0.6418807 Test Loss: 0.3875013
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 4.234078884124756
Epoch: 59, Steps: 59 | Train Loss: 0.7975103 Vali Loss: 0.6439464 Test Loss: 0.3874867
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 3.884459972381592
Epoch: 60, Steps: 59 | Train Loss: 0.7970061 Vali Loss: 0.6370856 Test Loss: 0.3874842
Validation loss decreased (0.637610 --> 0.637086).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 3.4357571601867676
Epoch: 61, Steps: 59 | Train Loss: 0.7974519 Vali Loss: 0.6414158 Test Loss: 0.3874753
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 4.153742074966431
Epoch: 62, Steps: 59 | Train Loss: 0.7972203 Vali Loss: 0.6425425 Test Loss: 0.3874683
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.4444656372070312
Epoch: 63, Steps: 59 | Train Loss: 0.7970597 Vali Loss: 0.6405427 Test Loss: 0.3874645
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 3.0968987941741943
Epoch: 64, Steps: 59 | Train Loss: 0.7973007 Vali Loss: 0.6422384 Test Loss: 0.3874600
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.813615560531616
Epoch: 65, Steps: 59 | Train Loss: 0.7973271 Vali Loss: 0.6392300 Test Loss: 0.3874490
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.487337112426758
Epoch: 66, Steps: 59 | Train Loss: 0.7971783 Vali Loss: 0.6413949 Test Loss: 0.3874427
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 3.23262095451355
Epoch: 67, Steps: 59 | Train Loss: 0.7968356 Vali Loss: 0.6409751 Test Loss: 0.3874391
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.9991846084594727
Epoch: 68, Steps: 59 | Train Loss: 0.7967259 Vali Loss: 0.6467085 Test Loss: 0.3874352
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 3.939312696456909
Epoch: 69, Steps: 59 | Train Loss: 0.7973224 Vali Loss: 0.6400446 Test Loss: 0.3874346
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 4.4050116539001465
Epoch: 70, Steps: 59 | Train Loss: 0.7968742 Vali Loss: 0.6388879 Test Loss: 0.3874286
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.9073755741119385
Epoch: 71, Steps: 59 | Train Loss: 0.7968851 Vali Loss: 0.6418213 Test Loss: 0.3874272
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 3.356163263320923
Epoch: 72, Steps: 59 | Train Loss: 0.7970180 Vali Loss: 0.6417524 Test Loss: 0.3874253
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 4.697393178939819
Epoch: 73, Steps: 59 | Train Loss: 0.7961659 Vali Loss: 0.6421390 Test Loss: 0.3874159
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 3.4345078468322754
Epoch: 74, Steps: 59 | Train Loss: 0.7966164 Vali Loss: 0.6397320 Test Loss: 0.3874179
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.5612294673919678
Epoch: 75, Steps: 59 | Train Loss: 0.7968254 Vali Loss: 0.6423362 Test Loss: 0.3874128
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 3.558131456375122
Epoch: 76, Steps: 59 | Train Loss: 0.7957533 Vali Loss: 0.6393820 Test Loss: 0.3874141
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.679917573928833
Epoch: 77, Steps: 59 | Train Loss: 0.7972005 Vali Loss: 0.6373574 Test Loss: 0.3874081
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.7523281574249268
Epoch: 78, Steps: 59 | Train Loss: 0.7974949 Vali Loss: 0.6427412 Test Loss: 0.3874059
EarlyStopping counter: 18 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 4.1302878856658936
Epoch: 79, Steps: 59 | Train Loss: 0.7971129 Vali Loss: 0.6394711 Test Loss: 0.3874019
EarlyStopping counter: 19 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 3.3435795307159424
Epoch: 80, Steps: 59 | Train Loss: 0.7964139 Vali Loss: 0.6444471 Test Loss: 0.3874006
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_360_720_FITS_ETTh2_ftM_sl360_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.38548892736434937, mae:0.4232577681541443, rse:0.49626272916793823, corr:[ 0.21710917  0.22187373  0.21933939  0.21875003  0.21903369  0.21788743
  0.21672732  0.21633548  0.21565707  0.21408948  0.21276301  0.21166244
  0.21049426  0.20937541  0.20863834  0.20807566  0.2073694   0.20654191
  0.20575598  0.20491444  0.203894    0.20300725  0.20198232  0.20054445
  0.1988055   0.19745469  0.19646075  0.1954967   0.19461729  0.19370897
  0.19267385  0.191407    0.19030963  0.18940729  0.18839803  0.18723324
  0.18617658  0.18529361  0.18437076  0.18336163  0.1826283   0.18217519
  0.18159595  0.18092473  0.18031645  0.17977783  0.17885563  0.17732573
  0.17574467  0.1743096   0.1729802   0.17206535  0.17135777  0.1704987
  0.16932835  0.16841105  0.16797926  0.16750945  0.16678506  0.16649044
  0.16666822  0.16663282  0.16632979  0.16620724  0.16645025  0.16657658
  0.16626905  0.16609406  0.16629577  0.16645291  0.16605899  0.16548139
  0.16512582  0.1648126   0.16430654  0.163859    0.16375326  0.16364148
  0.16317885  0.16278516  0.1627911   0.1627901   0.16252656  0.16242054
  0.16278222  0.16306461  0.16286299  0.1626196   0.16270609  0.16285735
  0.16263154  0.16245046  0.1628504   0.16329686  0.16323696  0.16279793
  0.16253382  0.16234583  0.16184309  0.16129711  0.16117117  0.16108167
  0.16077906  0.16041756  0.16057649  0.16080542  0.16063625  0.16037756
  0.16036098  0.1604021   0.1600201   0.15950505  0.15931113  0.15944697
  0.15934725  0.15905558  0.15898742  0.15885727  0.15830284  0.15733038
  0.15646744  0.15601417  0.15555759  0.15487531  0.1542513   0.15395744
  0.15352055  0.15288846  0.1524596   0.15219434  0.15186337  0.15131341
  0.15081711  0.15029767  0.14961474  0.14905244  0.14876299  0.14864588
  0.14812218  0.14763983  0.14763321  0.14776655  0.14734428  0.14614221
  0.14462037  0.14360093  0.14308983  0.14284077  0.14269383  0.14251524
  0.14213979  0.14162607  0.14146309  0.14148593  0.14113204  0.14032927
  0.13977326  0.13970535  0.13953917  0.13926232  0.13912855  0.13926123
  0.13932922  0.13933903  0.13968347  0.14015843  0.14005452  0.13910353
  0.1381893   0.13785261  0.1378864   0.13782999  0.13756615  0.13728762
  0.13691173  0.13647914  0.13605459  0.13579215  0.1355063   0.13516568
  0.13495393  0.13491087  0.1347911   0.13451417  0.13446538  0.1347531
  0.13508616  0.13520847  0.13544108  0.13615452  0.13679673  0.13685778
  0.13669209  0.13676256  0.13709268  0.13736343  0.1375558   0.13776888
  0.13794638  0.13786799  0.13769746  0.13778117  0.13800214  0.13783723
  0.13764983  0.13769013  0.13777043  0.13781653  0.13808137  0.13873045
  0.13924949  0.13941167  0.13942322  0.13972154  0.13995756  0.1397093
  0.13921723  0.13894677  0.1390925   0.13910967  0.13896863  0.13899456
  0.13927071  0.13961698  0.13983306  0.14003871  0.14030425  0.14036645
  0.14034699  0.14040925  0.14062953  0.14096826  0.14141263  0.14201722
  0.14260182  0.14304513  0.14338133  0.14394769  0.1446833   0.14514527
  0.14534327  0.14563397  0.1460762   0.14629065  0.14620832  0.14597188
  0.14597037  0.14631434  0.14678518  0.14730477  0.14776203  0.14794438
  0.14817116  0.14874215  0.14938375  0.15017927  0.15092003  0.15157098
  0.15192915  0.15206346  0.15245335  0.1533513   0.15448466  0.15504983
  0.15525822  0.15542838  0.15571709  0.15601835  0.15625325  0.1567116
  0.15718134  0.15773019  0.15832488  0.15913047  0.15991078  0.16010402
  0.160156    0.16066395  0.16124986  0.16170706  0.16220789  0.16296774
  0.16342968  0.16354536  0.16366306  0.16433126  0.16526346  0.16572462
  0.16560131  0.16551086  0.16553104  0.1655475   0.16549699  0.16564167
  0.1657082   0.16565067  0.16587827  0.16645482  0.16695863  0.16694799
  0.16652884  0.16649842  0.16705918  0.16763546  0.16780512  0.16804051
  0.16815671  0.1681475   0.1680536   0.16825178  0.16886786  0.16902538
  0.16872467  0.16844183  0.16864777  0.16874957  0.16835773  0.1680715
  0.16826017  0.16852053  0.1684965   0.16834173  0.1683608   0.168332
  0.16818441  0.16818357  0.16862832  0.16899219  0.1692956   0.16959484
  0.16979769  0.17003934  0.1701379   0.17052364  0.171273    0.17163178
  0.17143399  0.1710452   0.17113094  0.17144586  0.17174545  0.17190568
  0.17207628  0.17206849  0.17195173  0.17207594  0.17228374  0.17234544
  0.17228039  0.17238697  0.1728547   0.17341915  0.17382936  0.17430578
  0.17480586  0.17519987  0.17537852  0.17585076  0.17635038  0.17642345
  0.17595148  0.17539094  0.17528325  0.17532697  0.17543325  0.17578517
  0.17636296  0.17687127  0.17684722  0.17672971  0.17663987  0.17661183
  0.17639408  0.17616893  0.17614886  0.17616001  0.17604756  0.1759148
  0.17589292  0.17597575  0.1759065   0.17579617  0.17579103  0.17589079
  0.1757701   0.17532963  0.17491622  0.17477979  0.17486262  0.17509967
  0.17548892  0.17588244  0.17615758  0.17640382  0.1766852   0.17677377
  0.17659159  0.17610689  0.17584997  0.17574455  0.1753891   0.17486294
  0.1745039   0.17439814  0.17410609  0.17359614  0.1732063   0.1730109
  0.17265517  0.17221601  0.17196733  0.1718855   0.17162126  0.17108352
  0.17070481  0.17087366  0.17094427  0.17044234  0.16970403  0.16915254
  0.16860628  0.1677965   0.16691591  0.16618618  0.16534029  0.16430643
  0.16323876  0.16239673  0.16175319  0.16120811  0.16051301  0.15970087
  0.15878175  0.15797141  0.15741023  0.15681535  0.15603718  0.15525629
  0.1549246   0.15464485  0.15392531  0.15285532  0.15202591  0.15160903
  0.1510221   0.15029174  0.14965728  0.1495361   0.1494071   0.14887637
  0.14816102  0.1477733   0.14739501  0.14699584  0.14679703  0.14676237
  0.14647904  0.14588292  0.14542861  0.14540128  0.14518704  0.14454974
  0.14399399  0.14382821  0.14354584  0.1430245   0.1426074   0.14224982
  0.14160751  0.14062461  0.13997777  0.1398432   0.13975327  0.13921213
  0.13858314  0.13828826  0.13813539  0.137894    0.13740811  0.13672608
  0.13591386  0.13516326  0.13468306  0.13414608  0.13335596  0.13233328
  0.13157411  0.13105068  0.13043104  0.12955666  0.12889518  0.12837884
  0.12782049  0.12674598  0.12571126  0.12517677  0.12480491  0.1241728
  0.12337621  0.12276341  0.12248456  0.12205543  0.12115878  0.11985357
  0.11837319  0.11709224  0.11598711  0.11499777  0.11415276  0.11353622
  0.11296518  0.11262178  0.11219168  0.11143202  0.11055385  0.10971411
  0.10897209  0.10797708  0.10704191  0.10637985  0.10582513  0.10513344
  0.10419587  0.10356375  0.10324292  0.10294274  0.10207712  0.10068591
  0.09907399  0.09770769  0.09677851  0.09574386  0.09449507  0.09311986
  0.09206994  0.09133466  0.09069136  0.08974478  0.08855354  0.08766186
  0.08678914  0.08568217  0.08464573  0.0839929   0.08342645  0.08283725
  0.08170016  0.08080721  0.08051227  0.08019596  0.07942674  0.07787469
  0.07604222  0.0746018   0.07355139  0.07288949  0.07222191  0.07135793
  0.07043394  0.06962415  0.06913067  0.06854121  0.06803277  0.06764965
  0.06724489  0.06688961  0.06644271  0.06581177  0.06528191  0.06472559
  0.06397204  0.06331229  0.06269103  0.06201552  0.06089751  0.05946295
  0.05795759  0.05687787  0.05589698  0.05490003  0.05389151  0.05287728
  0.05205616  0.05132119  0.05056826  0.04990845  0.04923597  0.04883448
  0.04829474  0.04758689  0.04683866  0.04647942  0.04623131  0.04586976
  0.04547668  0.04518233  0.04495949  0.04458019  0.04364264  0.0424299
  0.04145503  0.04070551  0.03979757  0.03892716  0.03791061  0.03695231
  0.03613114  0.03528044  0.03450896  0.03404726  0.03367996  0.0333855
  0.03296688  0.03250166  0.03227928  0.03227597  0.03231225  0.0324048
  0.03249377  0.03276253  0.033018    0.03303265  0.03264148  0.03196946
  0.03141695  0.0309152   0.03037748  0.02975526  0.0291207   0.02883389
  0.02895796  0.02907183  0.02909291  0.02883324  0.02882453  0.02910913
  0.02908734  0.02835299  0.02746075  0.02740759  0.0273996   0.02701788
  0.0263837   0.02608978  0.02628182  0.02625637  0.02556718  0.02440267
  0.02331061  0.02246215  0.02153349  0.02079625  0.02029445  0.01956332
  0.0191959   0.01900587  0.01874276  0.0183914   0.01846192  0.01843575
  0.01780959  0.01698142  0.01656818  0.01703705  0.01738291  0.01702836
  0.01643885  0.01650443  0.0168194   0.01704275  0.0163613   0.01547368
  0.01499572  0.01454321  0.01379848  0.01286893  0.01233789  0.01180807
  0.01144936  0.01125028  0.011315    0.01184904  0.01213454  0.01219998
  0.01159409  0.01071398  0.01060116  0.01111177  0.01158074  0.01147893
  0.01149914  0.01205206  0.01255346  0.01235896  0.01142895  0.01077974
  0.00995021  0.00894482  0.00744697  0.00602842  0.00581692  0.00647342
  0.00683676  0.00575018  0.00476494  0.00473968  0.00491575  0.00447072
  0.00282264  0.00206568  0.00285716  0.00328511  0.00152754  0.00093344
  0.00269479  0.00323363 -0.00094455 -0.00512501 -0.00285144 -0.00540717]
