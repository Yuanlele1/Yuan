Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_360_720_FITS_ETTh2_ftM_sl360_ll48_pl720_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=42, out_features=126, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4741632.0
params:  5418.0
Trainable parameters:  5418
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.2272486686706543
Epoch: 1, Steps: 59 | Train Loss: 0.9574137 Vali Loss: 0.8411640 Test Loss: 0.5427098
Validation loss decreased (inf --> 0.841164).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.318183183670044
Epoch: 2, Steps: 59 | Train Loss: 0.8129953 Vali Loss: 0.7851875 Test Loss: 0.4962969
Validation loss decreased (0.841164 --> 0.785188).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.877819299697876
Epoch: 3, Steps: 59 | Train Loss: 0.7274933 Vali Loss: 0.7486542 Test Loss: 0.4683335
Validation loss decreased (0.785188 --> 0.748654).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.8688762187957764
Epoch: 4, Steps: 59 | Train Loss: 0.6770546 Vali Loss: 0.7284033 Test Loss: 0.4510445
Validation loss decreased (0.748654 --> 0.728403).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.066861867904663
Epoch: 5, Steps: 59 | Train Loss: 0.6443411 Vali Loss: 0.7153149 Test Loss: 0.4397887
Validation loss decreased (0.728403 --> 0.715315).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.0570919513702393
Epoch: 6, Steps: 59 | Train Loss: 0.6234486 Vali Loss: 0.7025453 Test Loss: 0.4323798
Validation loss decreased (0.715315 --> 0.702545).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.000671148300171
Epoch: 7, Steps: 59 | Train Loss: 0.6089373 Vali Loss: 0.6979023 Test Loss: 0.4273295
Validation loss decreased (0.702545 --> 0.697902).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.214629888534546
Epoch: 8, Steps: 59 | Train Loss: 0.5997163 Vali Loss: 0.6899863 Test Loss: 0.4235994
Validation loss decreased (0.697902 --> 0.689986).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.7852284908294678
Epoch: 9, Steps: 59 | Train Loss: 0.5931454 Vali Loss: 0.6873607 Test Loss: 0.4207429
Validation loss decreased (0.689986 --> 0.687361).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.055811882019043
Epoch: 10, Steps: 59 | Train Loss: 0.5877325 Vali Loss: 0.6809380 Test Loss: 0.4184462
Validation loss decreased (0.687361 --> 0.680938).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.7744722366333008
Epoch: 11, Steps: 59 | Train Loss: 0.5833133 Vali Loss: 0.6743293 Test Loss: 0.4165278
Validation loss decreased (0.680938 --> 0.674329).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.79359769821167
Epoch: 12, Steps: 59 | Train Loss: 0.5803976 Vali Loss: 0.6751728 Test Loss: 0.4148947
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.417144298553467
Epoch: 13, Steps: 59 | Train Loss: 0.5773167 Vali Loss: 0.6735179 Test Loss: 0.4134400
Validation loss decreased (0.674329 --> 0.673518).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.362647771835327
Epoch: 14, Steps: 59 | Train Loss: 0.5746264 Vali Loss: 0.6696309 Test Loss: 0.4121610
Validation loss decreased (0.673518 --> 0.669631).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.1531059741973877
Epoch: 15, Steps: 59 | Train Loss: 0.5736166 Vali Loss: 0.6741164 Test Loss: 0.4109987
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.623049020767212
Epoch: 16, Steps: 59 | Train Loss: 0.5717929 Vali Loss: 0.6685621 Test Loss: 0.4099610
Validation loss decreased (0.669631 --> 0.668562).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.9846248626708984
Epoch: 17, Steps: 59 | Train Loss: 0.5704215 Vali Loss: 0.6680683 Test Loss: 0.4090216
Validation loss decreased (0.668562 --> 0.668068).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.8985581398010254
Epoch: 18, Steps: 59 | Train Loss: 0.5686480 Vali Loss: 0.6670668 Test Loss: 0.4081435
Validation loss decreased (0.668068 --> 0.667067).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.7089667320251465
Epoch: 19, Steps: 59 | Train Loss: 0.5677362 Vali Loss: 0.6647006 Test Loss: 0.4073707
Validation loss decreased (0.667067 --> 0.664701).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.120669364929199
Epoch: 20, Steps: 59 | Train Loss: 0.5669078 Vali Loss: 0.6621639 Test Loss: 0.4066243
Validation loss decreased (0.664701 --> 0.662164).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.8676512241363525
Epoch: 21, Steps: 59 | Train Loss: 0.5656719 Vali Loss: 0.6644909 Test Loss: 0.4059448
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.077568769454956
Epoch: 22, Steps: 59 | Train Loss: 0.5652605 Vali Loss: 0.6622806 Test Loss: 0.4053057
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.8667490482330322
Epoch: 23, Steps: 59 | Train Loss: 0.5638992 Vali Loss: 0.6623074 Test Loss: 0.4047487
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.3822922706604004
Epoch: 24, Steps: 59 | Train Loss: 0.5634284 Vali Loss: 0.6597805 Test Loss: 0.4042117
Validation loss decreased (0.662164 --> 0.659781).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.282029151916504
Epoch: 25, Steps: 59 | Train Loss: 0.5631593 Vali Loss: 0.6610892 Test Loss: 0.4037299
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.8394544124603271
Epoch: 26, Steps: 59 | Train Loss: 0.5625518 Vali Loss: 0.6580927 Test Loss: 0.4032464
Validation loss decreased (0.659781 --> 0.658093).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.8855936527252197
Epoch: 27, Steps: 59 | Train Loss: 0.5621249 Vali Loss: 0.6532766 Test Loss: 0.4028117
Validation loss decreased (0.658093 --> 0.653277).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.042631149291992
Epoch: 28, Steps: 59 | Train Loss: 0.5616298 Vali Loss: 0.6542825 Test Loss: 0.4024168
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.4529640674591064
Epoch: 29, Steps: 59 | Train Loss: 0.5610167 Vali Loss: 0.6563982 Test Loss: 0.4020388
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.283508062362671
Epoch: 30, Steps: 59 | Train Loss: 0.5604035 Vali Loss: 0.6604733 Test Loss: 0.4017150
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.042844772338867
Epoch: 31, Steps: 59 | Train Loss: 0.5594316 Vali Loss: 0.6559093 Test Loss: 0.4013854
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.0395772457122803
Epoch: 32, Steps: 59 | Train Loss: 0.5596034 Vali Loss: 0.6587695 Test Loss: 0.4010852
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.225949764251709
Epoch: 33, Steps: 59 | Train Loss: 0.5593427 Vali Loss: 0.6574006 Test Loss: 0.4008026
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.2013494968414307
Epoch: 34, Steps: 59 | Train Loss: 0.5593609 Vali Loss: 0.6540778 Test Loss: 0.4005263
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.2125988006591797
Epoch: 35, Steps: 59 | Train Loss: 0.5589355 Vali Loss: 0.6541376 Test Loss: 0.4002774
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.224247455596924
Epoch: 36, Steps: 59 | Train Loss: 0.5584418 Vali Loss: 0.6539509 Test Loss: 0.4000432
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.152195692062378
Epoch: 37, Steps: 59 | Train Loss: 0.5585758 Vali Loss: 0.6536242 Test Loss: 0.3998268
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.7947659492492676
Epoch: 38, Steps: 59 | Train Loss: 0.5584020 Vali Loss: 0.6560750 Test Loss: 0.3996498
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.0100033283233643
Epoch: 39, Steps: 59 | Train Loss: 0.5580636 Vali Loss: 0.6574541 Test Loss: 0.3994323
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.83329439163208
Epoch: 40, Steps: 59 | Train Loss: 0.5576424 Vali Loss: 0.6564294 Test Loss: 0.3992509
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.0913236141204834
Epoch: 41, Steps: 59 | Train Loss: 0.5571326 Vali Loss: 0.6498314 Test Loss: 0.3990847
Validation loss decreased (0.653277 --> 0.649831).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.439427137374878
Epoch: 42, Steps: 59 | Train Loss: 0.5576206 Vali Loss: 0.6533084 Test Loss: 0.3989310
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.9161391258239746
Epoch: 43, Steps: 59 | Train Loss: 0.5572749 Vali Loss: 0.6546414 Test Loss: 0.3987631
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.287743330001831
Epoch: 44, Steps: 59 | Train Loss: 0.5570378 Vali Loss: 0.6551024 Test Loss: 0.3986349
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.2057855129241943
Epoch: 45, Steps: 59 | Train Loss: 0.5563133 Vali Loss: 0.6502872 Test Loss: 0.3984920
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.9053282737731934
Epoch: 46, Steps: 59 | Train Loss: 0.5560030 Vali Loss: 0.6547512 Test Loss: 0.3983699
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.0524826049804688
Epoch: 47, Steps: 59 | Train Loss: 0.5569520 Vali Loss: 0.6579884 Test Loss: 0.3982536
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.3312668800354004
Epoch: 48, Steps: 59 | Train Loss: 0.5566005 Vali Loss: 0.6553074 Test Loss: 0.3981431
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.9993617534637451
Epoch: 49, Steps: 59 | Train Loss: 0.5565596 Vali Loss: 0.6571153 Test Loss: 0.3980289
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.290632963180542
Epoch: 50, Steps: 59 | Train Loss: 0.5558827 Vali Loss: 0.6535537 Test Loss: 0.3979274
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.1933438777923584
Epoch: 51, Steps: 59 | Train Loss: 0.5562351 Vali Loss: 0.6536948 Test Loss: 0.3978278
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.328542709350586
Epoch: 52, Steps: 59 | Train Loss: 0.5558083 Vali Loss: 0.6585610 Test Loss: 0.3977330
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.5075254440307617
Epoch: 53, Steps: 59 | Train Loss: 0.5561299 Vali Loss: 0.6513202 Test Loss: 0.3976636
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.0107650756835938
Epoch: 54, Steps: 59 | Train Loss: 0.5560889 Vali Loss: 0.6539372 Test Loss: 0.3975698
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.015829086303711
Epoch: 55, Steps: 59 | Train Loss: 0.5557916 Vali Loss: 0.6542417 Test Loss: 0.3975022
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.2247767448425293
Epoch: 56, Steps: 59 | Train Loss: 0.5560511 Vali Loss: 0.6501565 Test Loss: 0.3974297
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.9089453220367432
Epoch: 57, Steps: 59 | Train Loss: 0.5556229 Vali Loss: 0.6522256 Test Loss: 0.3973677
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.4409701824188232
Epoch: 58, Steps: 59 | Train Loss: 0.5554513 Vali Loss: 0.6546084 Test Loss: 0.3973047
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.1576452255249023
Epoch: 59, Steps: 59 | Train Loss: 0.5555019 Vali Loss: 0.6530528 Test Loss: 0.3972359
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.9173777103424072
Epoch: 60, Steps: 59 | Train Loss: 0.5551931 Vali Loss: 0.6520866 Test Loss: 0.3971813
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.0765562057495117
Epoch: 61, Steps: 59 | Train Loss: 0.5555050 Vali Loss: 0.6540211 Test Loss: 0.3971202
EarlyStopping counter: 20 out of 20
Early stopping
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=42, out_features=126, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4741632.0
params:  5418.0
Trainable parameters:  5418
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.7255537509918213
Epoch: 1, Steps: 59 | Train Loss: 0.8102902 Vali Loss: 0.6513287 Test Loss: 0.3950926
Validation loss decreased (inf --> 0.651329).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.8487508296966553
Epoch: 2, Steps: 59 | Train Loss: 0.8051361 Vali Loss: 0.6500589 Test Loss: 0.3928752
Validation loss decreased (0.651329 --> 0.650059).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.7429773807525635
Epoch: 3, Steps: 59 | Train Loss: 0.8047026 Vali Loss: 0.6456767 Test Loss: 0.3913608
Validation loss decreased (0.650059 --> 0.645677).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.4358508586883545
Epoch: 4, Steps: 59 | Train Loss: 0.8030520 Vali Loss: 0.6467601 Test Loss: 0.3903962
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.3205134868621826
Epoch: 5, Steps: 59 | Train Loss: 0.8019349 Vali Loss: 0.6432912 Test Loss: 0.3897786
Validation loss decreased (0.645677 --> 0.643291).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.286168098449707
Epoch: 6, Steps: 59 | Train Loss: 0.8011904 Vali Loss: 0.6433554 Test Loss: 0.3892677
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.171724319458008
Epoch: 7, Steps: 59 | Train Loss: 0.7999009 Vali Loss: 0.6445760 Test Loss: 0.3889118
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.022101879119873
Epoch: 8, Steps: 59 | Train Loss: 0.7997913 Vali Loss: 0.6416475 Test Loss: 0.3887005
Validation loss decreased (0.643291 --> 0.641648).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.0214645862579346
Epoch: 9, Steps: 59 | Train Loss: 0.7991325 Vali Loss: 0.6428621 Test Loss: 0.3885966
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.1537210941314697
Epoch: 10, Steps: 59 | Train Loss: 0.7994501 Vali Loss: 0.6455981 Test Loss: 0.3884943
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.326876163482666
Epoch: 11, Steps: 59 | Train Loss: 0.7990146 Vali Loss: 0.6407182 Test Loss: 0.3884442
Validation loss decreased (0.641648 --> 0.640718).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.121180295944214
Epoch: 12, Steps: 59 | Train Loss: 0.7994072 Vali Loss: 0.6385664 Test Loss: 0.3882827
Validation loss decreased (0.640718 --> 0.638566).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.0787692070007324
Epoch: 13, Steps: 59 | Train Loss: 0.7990963 Vali Loss: 0.6426098 Test Loss: 0.3882971
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.612750768661499
Epoch: 14, Steps: 59 | Train Loss: 0.7986848 Vali Loss: 0.6423867 Test Loss: 0.3882908
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.116447687149048
Epoch: 15, Steps: 59 | Train Loss: 0.7985101 Vali Loss: 0.6428196 Test Loss: 0.3881747
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.5051090717315674
Epoch: 16, Steps: 59 | Train Loss: 0.7984539 Vali Loss: 0.6404366 Test Loss: 0.3882110
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.1405222415924072
Epoch: 17, Steps: 59 | Train Loss: 0.7982513 Vali Loss: 0.6398624 Test Loss: 0.3882574
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.342366933822632
Epoch: 18, Steps: 59 | Train Loss: 0.7986598 Vali Loss: 0.6432769 Test Loss: 0.3881876
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.2348296642303467
Epoch: 19, Steps: 59 | Train Loss: 0.7987444 Vali Loss: 0.6405222 Test Loss: 0.3881859
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.280017137527466
Epoch: 20, Steps: 59 | Train Loss: 0.7980450 Vali Loss: 0.6392570 Test Loss: 0.3881869
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.782219648361206
Epoch: 21, Steps: 59 | Train Loss: 0.7975016 Vali Loss: 0.6378921 Test Loss: 0.3881946
Validation loss decreased (0.638566 --> 0.637892).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.3333733081817627
Epoch: 22, Steps: 59 | Train Loss: 0.7983353 Vali Loss: 0.6406781 Test Loss: 0.3881214
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.5444300174713135
Epoch: 23, Steps: 59 | Train Loss: 0.7984165 Vali Loss: 0.6340376 Test Loss: 0.3881962
Validation loss decreased (0.637892 --> 0.634038).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.1364755630493164
Epoch: 24, Steps: 59 | Train Loss: 0.7979040 Vali Loss: 0.6404160 Test Loss: 0.3881594
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.917306900024414
Epoch: 25, Steps: 59 | Train Loss: 0.7984571 Vali Loss: 0.6381480 Test Loss: 0.3881751
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.3634674549102783
Epoch: 26, Steps: 59 | Train Loss: 0.7974822 Vali Loss: 0.6374347 Test Loss: 0.3881696
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.1062071323394775
Epoch: 27, Steps: 59 | Train Loss: 0.7975251 Vali Loss: 0.6428627 Test Loss: 0.3881466
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.0647976398468018
Epoch: 28, Steps: 59 | Train Loss: 0.7978191 Vali Loss: 0.6396982 Test Loss: 0.3881598
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.5283801555633545
Epoch: 29, Steps: 59 | Train Loss: 0.7971053 Vali Loss: 0.6385949 Test Loss: 0.3881201
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.8382396697998047
Epoch: 30, Steps: 59 | Train Loss: 0.7976878 Vali Loss: 0.6385184 Test Loss: 0.3881468
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.03218674659729
Epoch: 31, Steps: 59 | Train Loss: 0.7983153 Vali Loss: 0.6401374 Test Loss: 0.3881170
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.560150146484375
Epoch: 32, Steps: 59 | Train Loss: 0.7974891 Vali Loss: 0.6398126 Test Loss: 0.3881384
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.050837516784668
Epoch: 33, Steps: 59 | Train Loss: 0.7973277 Vali Loss: 0.6390921 Test Loss: 0.3881146
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.0576119422912598
Epoch: 34, Steps: 59 | Train Loss: 0.7978691 Vali Loss: 0.6352575 Test Loss: 0.3881251
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.162764310836792
Epoch: 35, Steps: 59 | Train Loss: 0.7975114 Vali Loss: 0.6378546 Test Loss: 0.3881402
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.185659885406494
Epoch: 36, Steps: 59 | Train Loss: 0.7981958 Vali Loss: 0.6398131 Test Loss: 0.3881339
EarlyStopping counter: 13 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.8233442306518555
Epoch: 37, Steps: 59 | Train Loss: 0.7978980 Vali Loss: 0.6379128 Test Loss: 0.3881285
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.178924560546875
Epoch: 38, Steps: 59 | Train Loss: 0.7974875 Vali Loss: 0.6376255 Test Loss: 0.3881156
EarlyStopping counter: 15 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.4415481090545654
Epoch: 39, Steps: 59 | Train Loss: 0.7979281 Vali Loss: 0.6387556 Test Loss: 0.3881254
EarlyStopping counter: 16 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.3690128326416016
Epoch: 40, Steps: 59 | Train Loss: 0.7981436 Vali Loss: 0.6354462 Test Loss: 0.3881294
EarlyStopping counter: 17 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.9923202991485596
Epoch: 41, Steps: 59 | Train Loss: 0.7976410 Vali Loss: 0.6394129 Test Loss: 0.3881482
EarlyStopping counter: 18 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.586792230606079
Epoch: 42, Steps: 59 | Train Loss: 0.7975344 Vali Loss: 0.6336870 Test Loss: 0.3881182
Validation loss decreased (0.634038 --> 0.633687).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.3987839221954346
Epoch: 43, Steps: 59 | Train Loss: 0.7977233 Vali Loss: 0.6399558 Test Loss: 0.3881164
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.435368061065674
Epoch: 44, Steps: 59 | Train Loss: 0.7978553 Vali Loss: 0.6343336 Test Loss: 0.3881246
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.2493083477020264
Epoch: 45, Steps: 59 | Train Loss: 0.7968591 Vali Loss: 0.6392074 Test Loss: 0.3881221
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.1003878116607666
Epoch: 46, Steps: 59 | Train Loss: 0.7971295 Vali Loss: 0.6392449 Test Loss: 0.3881170
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.7153103351593018
Epoch: 47, Steps: 59 | Train Loss: 0.7972218 Vali Loss: 0.6388326 Test Loss: 0.3881229
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.28879976272583
Epoch: 48, Steps: 59 | Train Loss: 0.7978602 Vali Loss: 0.6367634 Test Loss: 0.3881133
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.171663999557495
Epoch: 49, Steps: 59 | Train Loss: 0.7976352 Vali Loss: 0.6362330 Test Loss: 0.3881236
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.3455111980438232
Epoch: 50, Steps: 59 | Train Loss: 0.7973846 Vali Loss: 0.6380255 Test Loss: 0.3881287
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.0706138610839844
Epoch: 51, Steps: 59 | Train Loss: 0.7977601 Vali Loss: 0.6384831 Test Loss: 0.3881173
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.9040584564208984
Epoch: 52, Steps: 59 | Train Loss: 0.7975491 Vali Loss: 0.6353680 Test Loss: 0.3881185
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.2593319416046143
Epoch: 53, Steps: 59 | Train Loss: 0.7970456 Vali Loss: 0.6371893 Test Loss: 0.3881264
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.168574571609497
Epoch: 54, Steps: 59 | Train Loss: 0.7968670 Vali Loss: 0.6354017 Test Loss: 0.3881150
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.032299041748047
Epoch: 55, Steps: 59 | Train Loss: 0.7978665 Vali Loss: 0.6380202 Test Loss: 0.3881221
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.126634120941162
Epoch: 56, Steps: 59 | Train Loss: 0.7974061 Vali Loss: 0.6369398 Test Loss: 0.3881205
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.266566753387451
Epoch: 57, Steps: 59 | Train Loss: 0.7971105 Vali Loss: 0.6376026 Test Loss: 0.3881232
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.8821790218353271
Epoch: 58, Steps: 59 | Train Loss: 0.7971028 Vali Loss: 0.6439901 Test Loss: 0.3881209
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.137160062789917
Epoch: 59, Steps: 59 | Train Loss: 0.7977939 Vali Loss: 0.6411915 Test Loss: 0.3881252
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.4602110385894775
Epoch: 60, Steps: 59 | Train Loss: 0.7976539 Vali Loss: 0.6348900 Test Loss: 0.3881235
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.0656485557556152
Epoch: 61, Steps: 59 | Train Loss: 0.7970454 Vali Loss: 0.6363361 Test Loss: 0.3881275
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.611093282699585
Epoch: 62, Steps: 59 | Train Loss: 0.7971093 Vali Loss: 0.6380259 Test Loss: 0.3881226
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_360_720_FITS_ETTh2_ftM_sl360_ll48_pl720_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.3862346112728119, mae:0.42341288924217224, rse:0.49674251675605774, corr:[ 0.21862304  0.21957453  0.2198476   0.2189131   0.21732844  0.21589875
  0.2150368   0.21439187  0.2139473   0.21310695  0.21198636  0.21053602
  0.20914449  0.2080121   0.20709838  0.20656636  0.20604068  0.20547943
  0.20470935  0.20372619  0.20265204  0.20150726  0.20041211  0.19920145
  0.19803493  0.19689257  0.19590808  0.19484015  0.19371054  0.1925684
  0.19154957  0.19064161  0.18970074  0.18877529  0.18783252  0.1868287
  0.18575186  0.18461534  0.183648    0.18289937  0.18230803  0.18181555
  0.18125112  0.18045235  0.17941028  0.17821772  0.17694944  0.17544015
  0.1739567   0.17282413  0.1719198   0.1712282   0.17039332  0.16940853
  0.16826263  0.16709661  0.16625257  0.16573927  0.16545877  0.16543137
  0.1655861   0.16563514  0.16558765  0.16539806  0.16514744  0.16496637
  0.16492029  0.16499822  0.16498905  0.164893    0.16464582  0.16424815
  0.163736    0.16320625  0.16282764  0.16256168  0.16244721  0.16242823
  0.1623443   0.16213065  0.16190927  0.16174364  0.1616794   0.16167907
  0.16182858  0.16206278  0.16227056  0.16235158  0.16229297  0.16221164
  0.16211209  0.1620129   0.16215661  0.16239412  0.16266727  0.16275327
  0.16270761  0.16251087  0.16207424  0.16150312  0.161051    0.16068725
  0.16067158  0.16080272  0.16112456  0.16137698  0.16143152  0.16140285
  0.16109958  0.16069782  0.16028585  0.16013902  0.16012341  0.16012682
  0.16010907  0.15987474  0.15950203  0.1588097   0.1579726   0.1570881
  0.15630324  0.1556053   0.15492475  0.15436056  0.15376249  0.15319037
  0.15262115  0.15212542  0.1517509   0.15130472  0.15093526  0.1505075
  0.15000196  0.14932397  0.14854464  0.14789674  0.14739044  0.14710173
  0.146839    0.14670199  0.14663455  0.14628461  0.14539804  0.1442621
  0.14295833  0.14185795  0.14101027  0.14052941  0.14023572  0.13990481
  0.13966644  0.13937707  0.1391159   0.13882548  0.13849004  0.1379671
  0.13740511  0.13705373  0.13665411  0.13639857  0.13630699  0.13638315
  0.13657126  0.13676542  0.13692726  0.1368561   0.1366072   0.13611692
  0.13556181  0.13504723  0.13447884  0.13400745  0.13362586  0.13322435
  0.13266136  0.13215242  0.1317605   0.13139857  0.13103133  0.13063654
  0.13033883  0.13014635  0.12999208  0.12993903  0.12996489  0.13008429
  0.13040233  0.13080418  0.13119744  0.1316841   0.1320746   0.13231765
  0.13248262  0.13254255  0.13259082  0.13274996  0.13293791  0.1330334
  0.13319017  0.13332911  0.13329786  0.13310489  0.13298674  0.13276634
  0.13262431  0.13268167  0.13278152  0.1330532   0.13338514  0.13381368
  0.13419257  0.13452683  0.13485275  0.13494456  0.13493766  0.13465662
  0.1343601   0.13406976  0.13386485  0.1338798   0.13391548  0.134048
  0.13415568  0.13433316  0.13449338  0.13450125  0.13453908  0.13452819
  0.13457026  0.13467796  0.13485596  0.13520946  0.13566238  0.13622017
  0.1368326   0.13751812  0.13816887  0.13857347  0.13879691  0.13895983
  0.13912788  0.13926604  0.1395277   0.1397815   0.14006385  0.1400786
  0.13995977  0.1398769   0.13997187  0.14016412  0.1406569   0.1412694
  0.14196569  0.14273962  0.14323445  0.1436647   0.14403762  0.1445268
  0.14514014  0.14593641  0.146941    0.14779754  0.14851172  0.14889808
  0.1490608   0.14911205  0.14918455  0.14950095  0.14982764  0.15033114
  0.15082331  0.15137173  0.1518342   0.15221368  0.1528158   0.1534043
  0.15393564  0.15464915  0.1552659   0.15571348  0.15600339  0.15633449
  0.15667275  0.15721437  0.1577807   0.15836006  0.15899976  0.15954885
  0.15968549  0.15946694  0.15903595  0.15880792  0.15863784  0.15870397
  0.15884183  0.15905693  0.1593899   0.1596343   0.15982835  0.16012533
  0.16032532  0.1605164   0.16066574  0.16086803  0.16093573  0.16112642
  0.16122906  0.16144727  0.16178638  0.16200204  0.1623839   0.16266619
  0.16279452  0.16262653  0.16236247  0.16210914  0.16175775  0.16145849
  0.16126166  0.16112453  0.1609959   0.1608421   0.16086271  0.16097082
  0.16110522  0.16115081  0.16135074  0.16150253  0.16177464  0.16211036
  0.16249214  0.16310142  0.16375932  0.16418979  0.1645219   0.16467391
  0.16472067  0.16458914  0.16452436  0.1644836   0.16454229  0.16455114
  0.16467607  0.16485615  0.16489471  0.16488443  0.16480245  0.16474473
  0.16483927  0.16501835  0.16534418  0.16579357  0.1662358   0.16674563
  0.1673843   0.16821894  0.16887079  0.16933765  0.16953674  0.16936369
  0.16900913  0.16860232  0.16847238  0.16858988  0.1689458   0.16949566
  0.17002292  0.17042488  0.1705492   0.17063512  0.17060919  0.17061019
  0.17057495  0.17051609  0.17047045  0.17041135  0.17030375  0.1702597
  0.17025335  0.1704594   0.17076357  0.17105062  0.17111805  0.17094979
  0.17061049  0.17021336  0.1699663   0.16997735  0.17024755  0.1706357
  0.17106968  0.17147054  0.171806    0.17212637  0.17245777  0.17258966
  0.17255186  0.17220287  0.17186496  0.17159097  0.17132951  0.17108256
  0.1708073   0.17055422  0.17034185  0.1701396   0.16997442  0.16988498
  0.16972238  0.1695326   0.16927995  0.16896425  0.16869178  0.16850504
  0.16829528  0.16826776  0.1683014   0.1682866   0.16815713  0.16778454
  0.16706562  0.16608702  0.16501027  0.16400686  0.16305323  0.1622028
  0.16151656  0.1610111   0.16050409  0.15999153  0.1592837   0.15846787
  0.1576448   0.15694794  0.15644138  0.15595368  0.15545794  0.15492794
  0.15448798  0.15392576  0.15325888  0.15253118  0.15185633  0.1512819
  0.15072966  0.15045482  0.15008132  0.14966062  0.14910163  0.14856148
  0.14807251  0.14793397  0.1479021   0.14791876  0.14784752  0.14760447
  0.14714223  0.14654325  0.14591962  0.14550851  0.14525208  0.14505914
  0.14487872  0.14464743  0.14418246  0.14357866  0.14301568  0.14248258
  0.14198913  0.14152898  0.14123067  0.14095095  0.1406137   0.14015609
  0.13965638  0.13914905  0.13868488  0.13832209  0.1379348   0.13737643
  0.1366732   0.1359291   0.1353154   0.13453244  0.1336431   0.13273819
  0.13196616  0.13121374  0.13057356  0.130029    0.1295992   0.12888214
  0.12809835  0.12717415  0.1264293   0.12586679  0.12537694  0.1249698
  0.12459347  0.12411372  0.12346961  0.12263769  0.12176187  0.12073255
  0.11950555  0.1183544   0.11738486  0.11652184  0.1156482   0.11496498
  0.11432169  0.11387039  0.11352745  0.11315911  0.1126499   0.11174353
  0.11075398  0.10972296  0.10890152  0.10823517  0.10754999  0.10699461
  0.10651521  0.10607152  0.1054035   0.10455526  0.10342595  0.10209084
  0.10062134  0.09914254  0.09789017  0.09663971  0.09544948  0.09432343
  0.093329    0.09244718  0.09161562  0.09076108  0.08981348  0.08881044
  0.08776038  0.08672533  0.08583792  0.08504956  0.08420417  0.08364272
  0.08306704  0.0824925   0.08182363  0.0808233   0.07970686  0.07836889
  0.07691976  0.07559197  0.07435708  0.0733957   0.07259917  0.07182275
  0.07100717  0.07015564  0.06954516  0.06894618  0.06850255  0.06804541
  0.06763119  0.06734943  0.06705433  0.0664973   0.06588243  0.06523339
  0.06444861  0.06362216  0.06271079  0.06188197  0.06092623  0.05981461
  0.05849869  0.05729073  0.05608321  0.05493014  0.05400454  0.05327971
  0.05269149  0.05199797  0.05120075  0.05040455  0.04954851  0.04892246
  0.04838485  0.04801875  0.04772778  0.04749127  0.04712388  0.04667946
  0.04620135  0.04556941  0.04476704  0.04406193  0.04339502  0.04269309
  0.0419472   0.04110876  0.04002495  0.03901719  0.03798168  0.03704392
  0.03629578  0.03560532  0.03491809  0.0344379   0.03406188  0.03366103
  0.03313739  0.0326368   0.03249402  0.03265883  0.03292154  0.03322915
  0.03337043  0.03333221  0.03296754  0.03243121  0.03185612  0.03130634
  0.03081118  0.0302837   0.02976011  0.02929159  0.02884134  0.02849448
  0.02825546  0.02804356  0.02801113  0.02795839  0.02793833  0.02778056
  0.02735716  0.02671045  0.02590759  0.02547953  0.02515784  0.02502019
  0.02487115  0.02457581  0.02416677  0.02351429  0.02261448  0.02149778
  0.02047017  0.01970337  0.01895629  0.01822123  0.01757791  0.01675667
  0.01607123  0.01558291  0.01529609  0.015007    0.0148766   0.0145695
  0.01405525  0.01369591  0.01339439  0.01322555  0.01304242  0.01294887
  0.0130244   0.01319384  0.01313248  0.01305168  0.01255234  0.01172568
  0.01086339  0.0100282   0.00928048  0.00846595  0.00790235  0.0074595
  0.00721993  0.00704979  0.00687994  0.00693985  0.00687964  0.0068148
  0.00666652  0.00647986  0.00646254  0.00637446  0.00629814  0.00625005
  0.00632808  0.00651705  0.00676301  0.0070869   0.00704938  0.0064817
  0.00503908  0.00341678  0.0019225   0.00058529 -0.00032627 -0.00086068
 -0.00099757 -0.00130581 -0.00157565 -0.00202585 -0.00270239 -0.00314171
 -0.00347994 -0.00358767 -0.00381647 -0.00409927 -0.00444989 -0.00428412
 -0.00405408 -0.0037579  -0.0034589  -0.00394356 -0.0056347  -0.01011221]
