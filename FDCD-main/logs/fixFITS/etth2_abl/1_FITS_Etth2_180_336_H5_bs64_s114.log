Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=50, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_180_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_180_336_FITS_ETTh2_ftM_sl180_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8125
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=50, out_features=143, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6406400.0
params:  7293.0
Trainable parameters:  7293
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 6.434890985488892
Epoch: 1, Steps: 63 | Train Loss: 0.8359373 Vali Loss: 0.4516783 Test Loss: 0.4765062
Validation loss decreased (inf --> 0.451678).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.097659111022949
Epoch: 2, Steps: 63 | Train Loss: 0.7219842 Vali Loss: 0.4135749 Test Loss: 0.4335448
Validation loss decreased (0.451678 --> 0.413575).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.607161283493042
Epoch: 3, Steps: 63 | Train Loss: 0.6756427 Vali Loss: 0.3992987 Test Loss: 0.4161158
Validation loss decreased (0.413575 --> 0.399299).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.545959949493408
Epoch: 4, Steps: 63 | Train Loss: 0.6551313 Vali Loss: 0.3902985 Test Loss: 0.4080513
Validation loss decreased (0.399299 --> 0.390299).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.659183025360107
Epoch: 5, Steps: 63 | Train Loss: 0.6452030 Vali Loss: 0.3817937 Test Loss: 0.4041868
Validation loss decreased (0.390299 --> 0.381794).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.6140084266662598
Epoch: 6, Steps: 63 | Train Loss: 0.6387960 Vali Loss: 0.3792500 Test Loss: 0.4020177
Validation loss decreased (0.381794 --> 0.379250).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.3773720264434814
Epoch: 7, Steps: 63 | Train Loss: 0.6376207 Vali Loss: 0.3799051 Test Loss: 0.4004837
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.02536678314209
Epoch: 8, Steps: 63 | Train Loss: 0.6344173 Vali Loss: 0.3748547 Test Loss: 0.3995199
Validation loss decreased (0.379250 --> 0.374855).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.9072351455688477
Epoch: 9, Steps: 63 | Train Loss: 0.6328359 Vali Loss: 0.3731785 Test Loss: 0.3987232
Validation loss decreased (0.374855 --> 0.373178).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.674743175506592
Epoch: 10, Steps: 63 | Train Loss: 0.6310294 Vali Loss: 0.3748462 Test Loss: 0.3980899
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.799264907836914
Epoch: 11, Steps: 63 | Train Loss: 0.6312257 Vali Loss: 0.3716368 Test Loss: 0.3976023
Validation loss decreased (0.373178 --> 0.371637).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.7592718601226807
Epoch: 12, Steps: 63 | Train Loss: 0.6270535 Vali Loss: 0.3739375 Test Loss: 0.3973607
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.4426627159118652
Epoch: 13, Steps: 63 | Train Loss: 0.6285471 Vali Loss: 0.3702075 Test Loss: 0.3968002
Validation loss decreased (0.371637 --> 0.370208).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.5738654136657715
Epoch: 14, Steps: 63 | Train Loss: 0.6273263 Vali Loss: 0.3707460 Test Loss: 0.3964238
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.4236583709716797
Epoch: 15, Steps: 63 | Train Loss: 0.6270299 Vali Loss: 0.3679269 Test Loss: 0.3961335
Validation loss decreased (0.370208 --> 0.367927).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.500624418258667
Epoch: 16, Steps: 63 | Train Loss: 0.6280125 Vali Loss: 0.3695601 Test Loss: 0.3960027
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.6566615104675293
Epoch: 17, Steps: 63 | Train Loss: 0.6263265 Vali Loss: 0.3693234 Test Loss: 0.3956707
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.40248703956604
Epoch: 18, Steps: 63 | Train Loss: 0.6251087 Vali Loss: 0.3710465 Test Loss: 0.3955093
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.301378011703491
Epoch: 19, Steps: 63 | Train Loss: 0.6244653 Vali Loss: 0.3675797 Test Loss: 0.3953987
Validation loss decreased (0.367927 --> 0.367580).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.7747268676757812
Epoch: 20, Steps: 63 | Train Loss: 0.6260024 Vali Loss: 0.3689260 Test Loss: 0.3952761
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.9026436805725098
Epoch: 21, Steps: 63 | Train Loss: 0.6212090 Vali Loss: 0.3672218 Test Loss: 0.3950745
Validation loss decreased (0.367580 --> 0.367222).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.1706838607788086
Epoch: 22, Steps: 63 | Train Loss: 0.6250072 Vali Loss: 0.3691431 Test Loss: 0.3949531
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.406770944595337
Epoch: 23, Steps: 63 | Train Loss: 0.6230927 Vali Loss: 0.3681800 Test Loss: 0.3948390
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.3137283325195312
Epoch: 24, Steps: 63 | Train Loss: 0.6242214 Vali Loss: 0.3688925 Test Loss: 0.3946978
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.504976272583008
Epoch: 25, Steps: 63 | Train Loss: 0.6241918 Vali Loss: 0.3675757 Test Loss: 0.3945988
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.080751895904541
Epoch: 26, Steps: 63 | Train Loss: 0.6216456 Vali Loss: 0.3681996 Test Loss: 0.3945813
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.4967408180236816
Epoch: 27, Steps: 63 | Train Loss: 0.6214203 Vali Loss: 0.3663389 Test Loss: 0.3944458
Validation loss decreased (0.367222 --> 0.366339).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.8360440731048584
Epoch: 28, Steps: 63 | Train Loss: 0.6232197 Vali Loss: 0.3677031 Test Loss: 0.3944182
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.4312450885772705
Epoch: 29, Steps: 63 | Train Loss: 0.6224570 Vali Loss: 0.3679192 Test Loss: 0.3943478
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.0727322101593018
Epoch: 30, Steps: 63 | Train Loss: 0.6239711 Vali Loss: 0.3673872 Test Loss: 0.3942357
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.025940179824829
Epoch: 31, Steps: 63 | Train Loss: 0.6222871 Vali Loss: 0.3686626 Test Loss: 0.3942032
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.972353458404541
Epoch: 32, Steps: 63 | Train Loss: 0.6222950 Vali Loss: 0.3686165 Test Loss: 0.3941462
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.9872066974639893
Epoch: 33, Steps: 63 | Train Loss: 0.6212500 Vali Loss: 0.3685877 Test Loss: 0.3940884
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.7049365043640137
Epoch: 34, Steps: 63 | Train Loss: 0.6207188 Vali Loss: 0.3692501 Test Loss: 0.3941174
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.0920090675354004
Epoch: 35, Steps: 63 | Train Loss: 0.6199407 Vali Loss: 0.3662074 Test Loss: 0.3940084
Validation loss decreased (0.366339 --> 0.366207).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.8158700466156006
Epoch: 36, Steps: 63 | Train Loss: 0.6219805 Vali Loss: 0.3655312 Test Loss: 0.3939898
Validation loss decreased (0.366207 --> 0.365531).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.5758843421936035
Epoch: 37, Steps: 63 | Train Loss: 0.6213759 Vali Loss: 0.3674624 Test Loss: 0.3939326
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.3116042613983154
Epoch: 38, Steps: 63 | Train Loss: 0.6221229 Vali Loss: 0.3671955 Test Loss: 0.3938686
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.916706085205078
Epoch: 39, Steps: 63 | Train Loss: 0.6220879 Vali Loss: 0.3689170 Test Loss: 0.3938469
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 4.59591817855835
Epoch: 40, Steps: 63 | Train Loss: 0.6210564 Vali Loss: 0.3676487 Test Loss: 0.3938368
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.77640700340271
Epoch: 41, Steps: 63 | Train Loss: 0.6225676 Vali Loss: 0.3684564 Test Loss: 0.3938113
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 4.009172201156616
Epoch: 42, Steps: 63 | Train Loss: 0.6210893 Vali Loss: 0.3655283 Test Loss: 0.3937904
Validation loss decreased (0.365531 --> 0.365528).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.990067720413208
Epoch: 43, Steps: 63 | Train Loss: 0.6197924 Vali Loss: 0.3666770 Test Loss: 0.3937622
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 4.229285955429077
Epoch: 44, Steps: 63 | Train Loss: 0.6210471 Vali Loss: 0.3677427 Test Loss: 0.3937510
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.964219570159912
Epoch: 45, Steps: 63 | Train Loss: 0.6198423 Vali Loss: 0.3639174 Test Loss: 0.3937126
Validation loss decreased (0.365528 --> 0.363917).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.3993141651153564
Epoch: 46, Steps: 63 | Train Loss: 0.6176223 Vali Loss: 0.3681692 Test Loss: 0.3937070
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.586214780807495
Epoch: 47, Steps: 63 | Train Loss: 0.6221410 Vali Loss: 0.3659752 Test Loss: 0.3936607
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 4.2747509479522705
Epoch: 48, Steps: 63 | Train Loss: 0.6200267 Vali Loss: 0.3624735 Test Loss: 0.3936337
Validation loss decreased (0.363917 --> 0.362474).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 4.0216758251190186
Epoch: 49, Steps: 63 | Train Loss: 0.6214951 Vali Loss: 0.3660667 Test Loss: 0.3935963
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 3.405630588531494
Epoch: 50, Steps: 63 | Train Loss: 0.6204658 Vali Loss: 0.3652693 Test Loss: 0.3935840
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.7388508319854736
Epoch: 51, Steps: 63 | Train Loss: 0.6173036 Vali Loss: 0.3674448 Test Loss: 0.3935899
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.330998659133911
Epoch: 52, Steps: 63 | Train Loss: 0.6212917 Vali Loss: 0.3683059 Test Loss: 0.3935736
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 3.5953991413116455
Epoch: 53, Steps: 63 | Train Loss: 0.6214939 Vali Loss: 0.3647618 Test Loss: 0.3935729
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.285581588745117
Epoch: 54, Steps: 63 | Train Loss: 0.6199523 Vali Loss: 0.3645372 Test Loss: 0.3935391
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.9695866107940674
Epoch: 55, Steps: 63 | Train Loss: 0.6200501 Vali Loss: 0.3672405 Test Loss: 0.3935419
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.6198439598083496
Epoch: 56, Steps: 63 | Train Loss: 0.6212903 Vali Loss: 0.3652171 Test Loss: 0.3935213
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.2247743606567383
Epoch: 57, Steps: 63 | Train Loss: 0.6219090 Vali Loss: 0.3669866 Test Loss: 0.3935113
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 3.4762122631073
Epoch: 58, Steps: 63 | Train Loss: 0.6203401 Vali Loss: 0.3676294 Test Loss: 0.3935010
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.9761252403259277
Epoch: 59, Steps: 63 | Train Loss: 0.6203486 Vali Loss: 0.3663374 Test Loss: 0.3934805
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 3.2738332748413086
Epoch: 60, Steps: 63 | Train Loss: 0.6205426 Vali Loss: 0.3659677 Test Loss: 0.3934828
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 3.0024726390838623
Epoch: 61, Steps: 63 | Train Loss: 0.6181466 Vali Loss: 0.3665248 Test Loss: 0.3934683
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.5828840732574463
Epoch: 62, Steps: 63 | Train Loss: 0.6208362 Vali Loss: 0.3660753 Test Loss: 0.3934588
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.558539867401123
Epoch: 63, Steps: 63 | Train Loss: 0.6197414 Vali Loss: 0.3672268 Test Loss: 0.3934465
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.562952756881714
Epoch: 64, Steps: 63 | Train Loss: 0.6212303 Vali Loss: 0.3657599 Test Loss: 0.3934363
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.9838814735412598
Epoch: 65, Steps: 63 | Train Loss: 0.6188465 Vali Loss: 0.3637638 Test Loss: 0.3934378
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.587320327758789
Epoch: 66, Steps: 63 | Train Loss: 0.6193695 Vali Loss: 0.3668059 Test Loss: 0.3934270
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.4800221920013428
Epoch: 67, Steps: 63 | Train Loss: 0.6213390 Vali Loss: 0.3656286 Test Loss: 0.3934192
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 3.366633892059326
Epoch: 68, Steps: 63 | Train Loss: 0.6176489 Vali Loss: 0.3667980 Test Loss: 0.3934227
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_180_336_FITS_ETTh2_ftM_sl180_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.38897162675857544, mae:0.4130445718765259, rse:0.4986530840396881, corr:[0.26197916 0.26643732 0.26324862 0.2642329  0.26346225 0.2608284
 0.26036972 0.25993413 0.25800043 0.25661057 0.25609002 0.25460783
 0.25289768 0.25185403 0.2511691  0.2503384  0.24959773 0.2487999
 0.24777167 0.24664927 0.24567232 0.24486451 0.2437852  0.24205066
 0.23966959 0.23782995 0.23662557 0.23544389 0.23383848 0.23233537
 0.2307756  0.22898425 0.22742663 0.22637643 0.22508098 0.22351596
 0.22241916 0.22150192 0.22017267 0.21881557 0.21811523 0.21727291
 0.21591114 0.21471922 0.21388207 0.21248479 0.21041729 0.20819612
 0.20565599 0.20310672 0.20102075 0.19949001 0.1975816  0.19549045
 0.19367553 0.19199382 0.19031163 0.18887135 0.18817937 0.18722497
 0.18602447 0.1852506  0.18532327 0.18477403 0.18381937 0.18337645
 0.18283445 0.18167776 0.18093875 0.18094522 0.180189   0.17847273
 0.17654057 0.17533109 0.17401129 0.17247684 0.17125995 0.17060411
 0.16979717 0.1688618  0.16852495 0.16831574 0.16807596 0.16779506
 0.167623   0.16720958 0.16711354 0.16699854 0.16636066 0.16546854
 0.16531086 0.16529751 0.16498184 0.16469549 0.16474335 0.16423926
 0.16256183 0.16119674 0.16039732 0.1596411  0.15846916 0.15762986
 0.15736416 0.15705144 0.15692724 0.15683217 0.15688033 0.15663572
 0.15655024 0.15619172 0.15564239 0.15493691 0.15456086 0.15435134
 0.15384367 0.15323372 0.1531043  0.1528636  0.15188546 0.15036267
 0.14856635 0.1469493  0.14553067 0.14449807 0.14336637 0.14208108
 0.14088987 0.13999669 0.13946527 0.13888338 0.1381843  0.13749865
 0.13715242 0.13666387 0.13638948 0.13617437 0.13564852 0.13488984
 0.13448037 0.13415542 0.13385461 0.1336676  0.13312624 0.13157247
 0.12897962 0.12732066 0.12585141 0.1243586  0.12300966 0.12236038
 0.12192823 0.12097781 0.12046587 0.12075692 0.12125284 0.12110052
 0.12138817 0.12149671 0.12142076 0.12130381 0.12149009 0.12139201
 0.1209754  0.12066995 0.12059636 0.12071829 0.12056771 0.11941712
 0.11712687 0.11574858 0.11504912 0.11436833 0.11311699 0.11176924
 0.1111049  0.11070564 0.11061089 0.11027781 0.11033867 0.11057851
 0.11083121 0.11070859 0.11088427 0.11120071 0.11109311 0.11076681
 0.11089991 0.1110228  0.11108099 0.11145949 0.11180833 0.11152461
 0.11058622 0.11016393 0.10967597 0.10925092 0.10893159 0.10890771
 0.10867003 0.1083216  0.10873559 0.10921799 0.10942031 0.1091342
 0.1093476  0.10943095 0.10928424 0.10929558 0.10946316 0.10950099
 0.1093936  0.10947447 0.10965566 0.10989515 0.11015309 0.10968857
 0.10834081 0.10717168 0.10652021 0.10609335 0.10531028 0.10509703
 0.10517844 0.1053267  0.10537291 0.10581509 0.10638252 0.10618395
 0.10588335 0.10577683 0.1059924  0.10610408 0.10646432 0.10700843
 0.10732598 0.10754222 0.10806306 0.10878485 0.10905999 0.10869539
 0.10751653 0.10646725 0.1055112  0.10532007 0.10517901 0.1046195
 0.10398854 0.10441447 0.10537425 0.10569101 0.10630175 0.10726567
 0.10824    0.10844796 0.10883996 0.10973977 0.11019119 0.11042169
 0.11081304 0.11129277 0.11162606 0.11214501 0.11291763 0.11314031
 0.11251953 0.11200745 0.11156379 0.1114829  0.11138681 0.11169533
 0.11158923 0.1113412  0.11194815 0.11275715 0.11333262 0.11348225
 0.11429692 0.1145872  0.11454263 0.11466416 0.11529181 0.11557693
 0.1155187  0.1155654  0.1158088  0.11610695 0.11643633 0.11690001
 0.11641942 0.11578371 0.1151882  0.11493351 0.11420161 0.11382919
 0.11391035 0.11376834 0.11399523 0.11457344 0.11608102 0.11703879
 0.11750668 0.11770457 0.11851105 0.11909643 0.11939962 0.12009434
 0.12060411 0.12064223 0.12070303 0.12118423 0.12161781 0.12136795
 0.12021948 0.11926766 0.11830361 0.11809097 0.11754507 0.11665422
 0.11554906 0.11626357 0.11757498 0.11738869 0.11762843 0.11955061
 0.12152752 0.12029956 0.11987428 0.12110919 0.1210629  0.11913201
 0.11876424 0.11854939 0.11598054 0.11462352 0.11567705 0.10950011]
