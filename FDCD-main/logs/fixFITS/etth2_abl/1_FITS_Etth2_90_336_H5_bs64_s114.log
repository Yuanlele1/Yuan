Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=30, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_90_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_90_336_FITS_ETTh2_ftM_sl90_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8215
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=30, out_features=142, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3816960.0
params:  4402.0
Trainable parameters:  4402
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.495280027389526
Epoch: 1, Steps: 64 | Train Loss: 0.9179595 Vali Loss: 0.4739270 Test Loss: 0.5570816
Validation loss decreased (inf --> 0.473927).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.324073553085327
Epoch: 2, Steps: 64 | Train Loss: 0.7967145 Vali Loss: 0.4327341 Test Loss: 0.5018612
Validation loss decreased (0.473927 --> 0.432734).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.637411594390869
Epoch: 3, Steps: 64 | Train Loss: 0.7419386 Vali Loss: 0.4084750 Test Loss: 0.4736692
Validation loss decreased (0.432734 --> 0.408475).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.9611315727233887
Epoch: 4, Steps: 64 | Train Loss: 0.7114958 Vali Loss: 0.3962726 Test Loss: 0.4576035
Validation loss decreased (0.408475 --> 0.396273).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.269665002822876
Epoch: 5, Steps: 64 | Train Loss: 0.6955380 Vali Loss: 0.3907298 Test Loss: 0.4479967
Validation loss decreased (0.396273 --> 0.390730).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.9573142528533936
Epoch: 6, Steps: 64 | Train Loss: 0.6851955 Vali Loss: 0.3864504 Test Loss: 0.4420730
Validation loss decreased (0.390730 --> 0.386450).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.636094808578491
Epoch: 7, Steps: 64 | Train Loss: 0.6779693 Vali Loss: 0.3807517 Test Loss: 0.4381078
Validation loss decreased (0.386450 --> 0.380752).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.1521570682525635
Epoch: 8, Steps: 64 | Train Loss: 0.6745980 Vali Loss: 0.3803586 Test Loss: 0.4354426
Validation loss decreased (0.380752 --> 0.380359).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.228794574737549
Epoch: 9, Steps: 64 | Train Loss: 0.6703403 Vali Loss: 0.3780356 Test Loss: 0.4335233
Validation loss decreased (0.380359 --> 0.378036).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.16202187538147
Epoch: 10, Steps: 64 | Train Loss: 0.6683672 Vali Loss: 0.3770036 Test Loss: 0.4320838
Validation loss decreased (0.378036 --> 0.377004).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.0041663646698
Epoch: 11, Steps: 64 | Train Loss: 0.6666694 Vali Loss: 0.3737615 Test Loss: 0.4309447
Validation loss decreased (0.377004 --> 0.373762).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.5079667568206787
Epoch: 12, Steps: 64 | Train Loss: 0.6648364 Vali Loss: 0.3728729 Test Loss: 0.4300267
Validation loss decreased (0.373762 --> 0.372873).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.369788408279419
Epoch: 13, Steps: 64 | Train Loss: 0.6634213 Vali Loss: 0.3735230 Test Loss: 0.4293564
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.0575225353240967
Epoch: 14, Steps: 64 | Train Loss: 0.6620379 Vali Loss: 0.3701606 Test Loss: 0.4286817
Validation loss decreased (0.372873 --> 0.370161).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.8182621002197266
Epoch: 15, Steps: 64 | Train Loss: 0.6615645 Vali Loss: 0.3695908 Test Loss: 0.4281458
Validation loss decreased (0.370161 --> 0.369591).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.048477411270142
Epoch: 16, Steps: 64 | Train Loss: 0.6610431 Vali Loss: 0.3726778 Test Loss: 0.4276739
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.386079788208008
Epoch: 17, Steps: 64 | Train Loss: 0.6587589 Vali Loss: 0.3704962 Test Loss: 0.4273134
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.515130996704102
Epoch: 18, Steps: 64 | Train Loss: 0.6573085 Vali Loss: 0.3715918 Test Loss: 0.4269366
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.6670873165130615
Epoch: 19, Steps: 64 | Train Loss: 0.6580850 Vali Loss: 0.3715250 Test Loss: 0.4266399
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 6.083270788192749
Epoch: 20, Steps: 64 | Train Loss: 0.6588048 Vali Loss: 0.3713436 Test Loss: 0.4263048
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.9892072677612305
Epoch: 21, Steps: 64 | Train Loss: 0.6577404 Vali Loss: 0.3676228 Test Loss: 0.4260879
Validation loss decreased (0.369591 --> 0.367623).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 5.228620290756226
Epoch: 22, Steps: 64 | Train Loss: 0.6571028 Vali Loss: 0.3688243 Test Loss: 0.4258327
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.8835694789886475
Epoch: 23, Steps: 64 | Train Loss: 0.6560055 Vali Loss: 0.3705080 Test Loss: 0.4256301
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.413072347640991
Epoch: 24, Steps: 64 | Train Loss: 0.6549586 Vali Loss: 0.3681546 Test Loss: 0.4254544
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.433131217956543
Epoch: 25, Steps: 64 | Train Loss: 0.6557647 Vali Loss: 0.3676941 Test Loss: 0.4253058
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.9139699935913086
Epoch: 26, Steps: 64 | Train Loss: 0.6556390 Vali Loss: 0.3673759 Test Loss: 0.4251152
Validation loss decreased (0.367623 --> 0.367376).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.4527804851531982
Epoch: 27, Steps: 64 | Train Loss: 0.6556857 Vali Loss: 0.3652732 Test Loss: 0.4249842
Validation loss decreased (0.367376 --> 0.365273).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.2255356311798096
Epoch: 28, Steps: 64 | Train Loss: 0.6552713 Vali Loss: 0.3659413 Test Loss: 0.4248628
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.6839261054992676
Epoch: 29, Steps: 64 | Train Loss: 0.6547654 Vali Loss: 0.3695921 Test Loss: 0.4247096
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.8619699478149414
Epoch: 30, Steps: 64 | Train Loss: 0.6544423 Vali Loss: 0.3678602 Test Loss: 0.4246388
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.0457494258880615
Epoch: 31, Steps: 64 | Train Loss: 0.6533725 Vali Loss: 0.3694972 Test Loss: 0.4244972
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 4.6509010791778564
Epoch: 32, Steps: 64 | Train Loss: 0.6531944 Vali Loss: 0.3672273 Test Loss: 0.4244253
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.7754297256469727
Epoch: 33, Steps: 64 | Train Loss: 0.6541209 Vali Loss: 0.3649890 Test Loss: 0.4243466
Validation loss decreased (0.365273 --> 0.364989).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.868633270263672
Epoch: 34, Steps: 64 | Train Loss: 0.6527261 Vali Loss: 0.3690137 Test Loss: 0.4242670
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.085707664489746
Epoch: 35, Steps: 64 | Train Loss: 0.6537143 Vali Loss: 0.3674932 Test Loss: 0.4241849
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 4.46941065788269
Epoch: 36, Steps: 64 | Train Loss: 0.6533642 Vali Loss: 0.3691731 Test Loss: 0.4241136
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.531200408935547
Epoch: 37, Steps: 64 | Train Loss: 0.6539065 Vali Loss: 0.3684253 Test Loss: 0.4240645
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 4.651358127593994
Epoch: 38, Steps: 64 | Train Loss: 0.6534305 Vali Loss: 0.3672257 Test Loss: 0.4239740
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.8171191215515137
Epoch: 39, Steps: 64 | Train Loss: 0.6535797 Vali Loss: 0.3655464 Test Loss: 0.4239155
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.869194269180298
Epoch: 40, Steps: 64 | Train Loss: 0.6533472 Vali Loss: 0.3661064 Test Loss: 0.4238757
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.092958450317383
Epoch: 41, Steps: 64 | Train Loss: 0.6530174 Vali Loss: 0.3649937 Test Loss: 0.4238302
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 3.109724760055542
Epoch: 42, Steps: 64 | Train Loss: 0.6535039 Vali Loss: 0.3683808 Test Loss: 0.4237751
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.985684871673584
Epoch: 43, Steps: 64 | Train Loss: 0.6522937 Vali Loss: 0.3640037 Test Loss: 0.4237354
Validation loss decreased (0.364989 --> 0.364004).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 4.685481548309326
Epoch: 44, Steps: 64 | Train Loss: 0.6528286 Vali Loss: 0.3669582 Test Loss: 0.4236851
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 4.736853361129761
Epoch: 45, Steps: 64 | Train Loss: 0.6530823 Vali Loss: 0.3662565 Test Loss: 0.4236617
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 4.48529839515686
Epoch: 46, Steps: 64 | Train Loss: 0.6503849 Vali Loss: 0.3670835 Test Loss: 0.4236297
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 3.365596294403076
Epoch: 47, Steps: 64 | Train Loss: 0.6515210 Vali Loss: 0.3656193 Test Loss: 0.4235863
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 3.759942054748535
Epoch: 48, Steps: 64 | Train Loss: 0.6515894 Vali Loss: 0.3664372 Test Loss: 0.4235459
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.7821907997131348
Epoch: 49, Steps: 64 | Train Loss: 0.6516034 Vali Loss: 0.3640492 Test Loss: 0.4235293
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 5.474820613861084
Epoch: 50, Steps: 64 | Train Loss: 0.6516070 Vali Loss: 0.3672222 Test Loss: 0.4235016
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 5.2432074546813965
Epoch: 51, Steps: 64 | Train Loss: 0.6524848 Vali Loss: 0.3691227 Test Loss: 0.4234768
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 3.0426549911499023
Epoch: 52, Steps: 64 | Train Loss: 0.6519480 Vali Loss: 0.3659649 Test Loss: 0.4234518
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 4.115870475769043
Epoch: 53, Steps: 64 | Train Loss: 0.6522332 Vali Loss: 0.3656825 Test Loss: 0.4234268
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 3.9419877529144287
Epoch: 54, Steps: 64 | Train Loss: 0.6515613 Vali Loss: 0.3690892 Test Loss: 0.4234021
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 4.873735666275024
Epoch: 55, Steps: 64 | Train Loss: 0.6526391 Vali Loss: 0.3673917 Test Loss: 0.4233848
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 3.351576089859009
Epoch: 56, Steps: 64 | Train Loss: 0.6518298 Vali Loss: 0.3657664 Test Loss: 0.4233599
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 3.8731791973114014
Epoch: 57, Steps: 64 | Train Loss: 0.6524768 Vali Loss: 0.3668157 Test Loss: 0.4233456
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 3.351890802383423
Epoch: 58, Steps: 64 | Train Loss: 0.6526072 Vali Loss: 0.3656442 Test Loss: 0.4233266
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 3.2181499004364014
Epoch: 59, Steps: 64 | Train Loss: 0.6513348 Vali Loss: 0.3668770 Test Loss: 0.4233086
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 4.531647443771362
Epoch: 60, Steps: 64 | Train Loss: 0.6519148 Vali Loss: 0.3680172 Test Loss: 0.4232937
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 3.2091751098632812
Epoch: 61, Steps: 64 | Train Loss: 0.6517174 Vali Loss: 0.3662529 Test Loss: 0.4232867
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 3.156623363494873
Epoch: 62, Steps: 64 | Train Loss: 0.6518273 Vali Loss: 0.3668731 Test Loss: 0.4232695
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.4783899784088135
Epoch: 63, Steps: 64 | Train Loss: 0.6514429 Vali Loss: 0.3657750 Test Loss: 0.4232531
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_90_336_FITS_ETTh2_ftM_sl90_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4189850687980652, mae:0.4263865649700165, rse:0.5175338983535767, corr:[0.26360705 0.26559612 0.2623273  0.26300395 0.25969914 0.25867537
 0.25808585 0.25610378 0.2556754  0.254833   0.2532343  0.2523148
 0.25087357 0.24928018 0.24877559 0.24777627 0.24672236 0.24634652
 0.24519077 0.24378948 0.24295487 0.24179788 0.24042268 0.23854156
 0.2354142  0.23302427 0.23138759 0.22964822 0.22817197 0.2273527
 0.22637895 0.22554182 0.22487123 0.2236827  0.2231258  0.22254245
 0.22119355 0.22036189 0.21981172 0.21885477 0.21826677 0.21786447
 0.21695684 0.21602875 0.2151649  0.2139384  0.21261701 0.21028921
 0.20660564 0.2036145  0.20102246 0.19905455 0.19738953 0.19539507
 0.19385743 0.19241183 0.19160546 0.19040416 0.18970981 0.18891032
 0.18802862 0.18768743 0.18738276 0.18733598 0.18694822 0.18608259
 0.18540792 0.18473805 0.18379825 0.18304247 0.18204372 0.18011256
 0.17729566 0.1755286  0.1737413  0.17233118 0.17150408 0.17094238
 0.17070445 0.17028014 0.16999817 0.16948208 0.16947296 0.16919714
 0.16842215 0.16822603 0.16830112 0.16809183 0.16766629 0.16734041
 0.16698976 0.16630161 0.16587988 0.16549513 0.16467346 0.16317183
 0.16086553 0.15879753 0.15690872 0.15538019 0.15418833 0.15343986
 0.15346228 0.15329026 0.15352504 0.15322593 0.15329498 0.15337235
 0.1529192  0.15245825 0.15204155 0.15185775 0.15128674 0.1506366
 0.15033692 0.14961061 0.14874442 0.14785856 0.14621554 0.14363028
 0.14075544 0.13843569 0.13621388 0.13491465 0.13372742 0.13268463
 0.13218598 0.13198315 0.1318675  0.13127442 0.13118424 0.13099915
 0.13031156 0.12986751 0.1297347  0.1293255  0.12875052 0.12832366
 0.12768555 0.12675665 0.12634759 0.12584838 0.12426318 0.12166082
 0.11830378 0.11544074 0.1129984  0.11159762 0.11016884 0.10903573
 0.10893926 0.10863151 0.10854497 0.10824021 0.10837901 0.10832466
 0.1079366  0.10788461 0.10794321 0.1081382  0.10806571 0.10774004
 0.10749165 0.10694706 0.10664514 0.10647233 0.10563751 0.10341377
 0.10022061 0.09799666 0.09600142 0.0949721  0.09418032 0.0935401
 0.09396543 0.09419334 0.09448968 0.09443288 0.09484901 0.09495452
 0.09473443 0.09496278 0.09510988 0.09523295 0.09529063 0.09535478
 0.09521192 0.09501367 0.0951533  0.09519536 0.09479248 0.09400914
 0.09230397 0.09109266 0.09012581 0.08949219 0.08921061 0.08928095
 0.09020281 0.09111849 0.09214239 0.09232264 0.09276819 0.09331999
 0.09326699 0.09317686 0.09335528 0.09344811 0.09324191 0.09341776
 0.09337672 0.09304139 0.09305722 0.09310136 0.09241874 0.09083197
 0.08866679 0.08650915 0.0844732  0.08364634 0.08306994 0.0831246
 0.0843223  0.08571199 0.08678775 0.08717505 0.08758692 0.0876861
 0.08760694 0.08748747 0.08746741 0.08769776 0.08802496 0.08826482
 0.08843848 0.08881498 0.08920419 0.08928392 0.08855437 0.08734323
 0.08550911 0.08373109 0.08197249 0.08157675 0.08175156 0.08213144
 0.08347778 0.08497535 0.08651482 0.08703379 0.08785439 0.08857874
 0.08844714 0.08896051 0.08965279 0.09025843 0.09063027 0.0913643
 0.09143721 0.09132799 0.09156147 0.09182305 0.09139425 0.09061966
 0.08956413 0.08864699 0.08791082 0.08788018 0.08799012 0.08865982
 0.09011643 0.09144552 0.09248091 0.09271143 0.09337737 0.09372235
 0.09357256 0.09350958 0.09388539 0.09402367 0.09395722 0.09389321
 0.09372935 0.09350097 0.09370781 0.09375577 0.09340794 0.09307982
 0.09232023 0.09118125 0.09006769 0.08980284 0.08978334 0.08965524
 0.08979145 0.09043451 0.09165234 0.09191152 0.09276922 0.0936029
 0.09326667 0.09340049 0.09403062 0.09413528 0.09416083 0.09484738
 0.09483434 0.09462003 0.09567367 0.09589621 0.09548269 0.0954529
 0.09447853 0.09346794 0.09292985 0.09331895 0.09335621 0.09354653
 0.09431281 0.09564608 0.0973897  0.09754866 0.09841034 0.0993394
 0.0991383  0.09900776 0.09906481 0.09864213 0.09884157 0.0989834
 0.09772763 0.09747915 0.09910512 0.09864698 0.09945772 0.1015807 ]
